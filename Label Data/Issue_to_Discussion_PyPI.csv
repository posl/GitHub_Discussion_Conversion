URL,Created,Comment_removeclass,Comment_time,Post_Index,Check_Point,Category
https://github.com/aio-libs/aiohttp/discussions/5096,2020/10/20 14:40,"aiohttp 3.4 is not supported anymore, you should migrate to 3.6+. Also, to prove that it's a bug you must provide a reproducer consisting solely of aiohttp-related code and being as small as possible. There's no evidence that this message is even coming from aiohttp: there's no such string in the source _ https://github.com/aio-libs/aiohttp/search?q=Signature+for+this+request+is+not+valid.
I'm converting this to a discussion because it's clearly a support request and maybe somebody from the community will be able to help you out. But you should at least show the full output of the programs you run and the way you invoke them, otherwise, nobody will be able to guess what it is you're attempting to do.",2020/10/20 14:48,1,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/17615,2021/8/14 8:38,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/8/14 8:38,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/17615,2021/8/14 8:38,"Extremely hard to say, but it looks like some kind of deployment issue. I think you should look in other logs - this might be permission/resource related, pretty much anything. Just look at all the logs - especially in your deployment logs. You wrote ""service"" which I am not sure what it really means, but I would start looking in the logs which are specific for your deployment. For sure this is not a known bug and airflow 2.1.2 works for many, so you must dig deeper and provide more information. Provisionally converting that one into discussion, as this is not a real ""issue"" of airflow.
If  you will do more investigation and get some more findings - please comment here, if it turns to be a real airflow issue, then we can re-open it.",2021/8/14 14:20,2,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/17982,2021/9/2 8:20,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/9/2 8:20,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/17982,2021/9/2 8:20,"Since there are no clear reproduction steps, this is a ""discussion"" rather than issue, I converted it so.
I believe your problems are connected with some instability or bad configuration of DNS in your k8S cluster:
There is this nice guideline on how to debug name resolution problems - you should follow it and see if it can help you to find a solution or you can gather more findings and more information which will help to diagnose the problem.
https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/",2021/9/2 8:34,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/17982,2021/9/2 8:20,"Is this problems caused by Kubernetes DNS configuration error?
I think, when we use KubernetesExecutor, airflow components(webserver, scheduler and executors) cannot share the logs of DAGs until the completion of DAG because webserver, scheduler and executors are different pods. So the web UI (on webserver) can't display the log until the dag completes.",2021/9/8 1:21,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18081,2021/9/8 3:55,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/9/8 3:55,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18081,2021/9/8 3:55,"I believe this is invalid. self.log.exception also shows exception information and you should see it in the log in this case https://docs.python.org/3/library/logging.html#logging.Logger.exception
",2021/9/8 9:02,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18081,2021/9/8 3:55,"I will close it fro now, please pas some logs if you think the message is unclear.
But If you would like to improve it and add some more information, I will invite you to open a PR directly. I will convert it into discussion now if you want to provide more information and discuss better approaches.",2021/9/8 9:04,3,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18185,2021/9/11 22:41,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/9/11 22:41,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18185,2021/9/11 22:41,Do you use constraints files to install Airflow? http://airflow.apache.org/docs/apache-airflow/stable/installation.html#constraints-files,2021/9/11 22:46,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18185,2021/9/11 22:41,"
_",2021/9/12 0:55,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18185,2021/9/11 22:41,For me it looks like if you had a  neo4j.py file in your PYTHONPATH (maybe your dag is named like that? Seems that Python interpreter sees neo4j as module rather than package.,2021/9/12 9:15,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18185,2021/9/11 22:41,I am moving that to discussion until this turns out to be a real issue with provider/airflow. But I could not reproduce it. For me all imports work fine.,2021/9/12 9:17,5,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/9/9 20:00,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,"I think it would be a better idea to prepare a sample repository, which will contain example tests, that everyone can analyze to inspire themselves in their project. I even started working on something similar, but didn't find the time to finish it. https://github.com/mik-laj/airlfow-testing-example  If you are interested, feel free to contribute.

configure all variables before import anythings from Apache Airflow

We currently have documentation that describes how to configure connections and variables in tests.  http://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#mocking-variables-and-connections",2021/9/9 20:33,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,"Most probably my explanation not clear.
I shouldn't have created it in midnight, I even wasn't  use my main account :-)
Let me explain wit some sample, probably after that my suggestion become more clear
Fist of all, I've prepared some sample Gist with simple plugin for Airflow
Main idea is just plugin which will able to install as regular package by pip - eg pip install pytest-apache-airflow
At that moment internally it work with pytest key  -p tests.plugins.airflow

I think it would be a better idea to prepare a sample repository, which will contain example tests

Yeah, It also nice. Based on to my past experience a lot of teams don't have any idea how even test integrity of dags and just try to deploy (shame, shame, shame)

We currently have documentation that describes how to configure connections and variables in tests. http://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#mocking-variables-and-connections

It also good documentation, and basically this should be used when you need redefine some variables in certain tests, not for global usage.
My suggestion more about Global Environment Variables, which defined in conftest.py:

",2021/9/10 8:48,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,Still not clear. converted it into discussion,2021/9/12 20:31,4,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,"Back to my experience test airflow DAGs and Operators not straight forward by end users.
Steps which users need to do before (correct me if I wrong):

Define AIRFLOW_HOME before import anything from airflow package, otherwise it will use default values
Define AIRFLOW__CORE__DAGS_FOLDER before import anything from airflow package, otherwise it will use default values
Set AIRFLOW__CORE__UNIT_TEST_MODE to True
Initialize DB before run tests

Everything below mostly related to collaboration with internal team rather than to develop airflow itself
Steps with this Environment Variables could be done by different ways:

Define it in .env file and use something like dotenv before run tests.
It work fine until you only one person in the team, you need to sure that everyone use this approach.
Define this variables in your IDE (like Pycharm).
Again work fine until all team member uses the same IDE, otherwise need to create documentation for all possible solution
Define this variables inside root conftest.py. That's approach seems like better choice
until you absolutely sure that one add some imports before define Environment variables

With initialize airflow DB there is no many options, probably better define in root conftest.py
session fixture with autouse=True",2021/9/14 12:05,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,"My proposal - create pytest plugin which can solve at least: define Environment Variables and initialize DB if required.
My personal Pros:

Define Project related Variables conftest.py which independent of IDE, root conftest.py and etc
Because plugin initialize before conftest.py you can put imports in top of the module, and don't worry about this
Plugins can contain additional functions and fixtures
Plugin can be installed by pip
Plugin can be set as mandatory by set in pytest.ini as required_plugins = pytest-apache-airflow-awesome

My personal Cons:

It should be somehow integrated with Apache Airflow monorepo or create separate repository. And personally a don't know best practice for this
Functions or fixtures which included in plugin should be version independent as much as possible otherwise it will add more pain rather than solve issues
",2021/9/14 12:24,6,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,"I believe those steps are already done in the conftest (including db initialization which you can run automatically by custom --with-db-init switch) - even the db will be initialized the first time you run the tests ?  Or am I missing something?
Maybe you are telling about adding a pytest plugin to let users write their own tests rather than run them in Airflow? If that's the second case - then you can easily create such plugin and publish it in https://airflow.apache.org/ecosystem/ if you think that might be useful. Happy to approve such PR.
The problem is that different users will have different ways of running tests for their own DAGs. We can expect (And keep updated) the Pytest conftest for ""Airflow testst"" but I can imagine people might want to use different approach for testing their dags (not everyone needs to set UNIT_TEST_MODE to true, not everyone needs to set the DAG folder, they might even use DagBag to load the dags directly from the folders they are located - so it's hard to suggest something to users.
But if you feel like creating such plugin as your own open-source project - feel free.",2021/9/14 12:33,7,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,"Basically I most worried that with separate project (outside of Apache Airflow) a lot of chance that it become obsolete as soon as developer/maintainer would stop use Airflow on daily basis.
Which already happen with pytest-airflow",2021/9/14 12:38,8,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,"And main motivation why I've created this ussue/feature just wanted to know is good idea to include this as part of project or not or not for now.
Probably we can return to this discussion as soon as I have create this plugin?",2021/9/14 13:03,9,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18195,2021/9/9 20:00,"Certainly - you can always contribute the code if you find it useful and you will provide some success stories/proven things and we will find a way how we can make sure it is maintained, it also needs to have proper automated testing and documentation.
The thing that it does not matter if it's in your repo or airflow - it can become stale and not maintained also in the Airflow repo. It happened with a number of things in the past. It's not ""magically' going to be maintained, there must be someone (including CI automation) to make sure that the code works, it is maintained, that the users of that code get proper support when things are not working.
We are actively pruning stuff from the repo and try to make sure what we have is actually used and useful (and is generally continuously verified this way or the other).
If you take a look there are NO end-users tools ""around"" testing and ""deploying"" airflow in Airflow repo  (except the Helm Chart  for prod  that was created precisely as a donation from Astronomer and Docker-Compose quick-start which is a bit pain to maintain to be honest). For example there are https://github.com/teamclairvoyant/airflow-maintenance-dags which is maintained by someone in the community and we have no intention to bring those or implement similar stuff in Airflow because it's not ""core"" of airflow. The only user-tool I know of are already part of the helm chart we support and release as a community (for example clean_logs.sh script, docker image entrypoint).",2021/9/14 14:21,10,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18196,2021/9/12 19:30,"Just out of the curiosity - where did you take the ""/admin/metrics"" endpoint from ? I guess it is something you have in your deployment that your admins wrote, because I cannot find such endpoint in airflow. Maybe you can point me to one and I am missing something, but i tried to find it and could not.
I am converting that into discussion until we classify it as an Airflow issue.",2021/9/12 20:56,1,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/18196,2021/9/12 19:30,"Hi @potiuk, you're right, just realized the endpoint is exposed by installing the airflow-prometheus-exporter plugin (https://github.com/robinhood/airflow-prometheus-exporter), thought it was a new feature of the 2.x version.
best regards.",2021/9/12 21:03,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/9/16 9:31,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"Not sure what changed this behaviour, but do you consider this a bug? I'd argue if you have LDAP enabled, you would want all your users to live in your LDAP directory and not be able to create any local users.",2021/9/16 9:42,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"Yeah. @pawsok  - were you atually ABLE to add new users in case of LDAP authentication before (and they Landed in LDAP)? Unless  you have custom authentication module, I believe there is no code for that functionality in Airflow so I'd also be quite surprised. Maybe that was a bug and when you added the users they landed in Airflow DB instead?
Can you please explain what was the behaviour you observed before and where the users landed (converting that into discussion)",2021/9/16 10:17,3,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"@potiuk Yes, I'm still able to add LDAP users from Airflow 2.0.1. Here is an exemplary add user view:

I have added many users in this way and they can log in Airflow, as:
username: domain/user_domain_name
password: domain_password
In Airflow webserver config file there are three variables set as the below:

EDIT:
Maybe it's something related to a newer version of Flask AppBuilder? In Airflow 2.0.1 we have Flask-AppBuilder==3.1.1, now it's Flask-AppBuilder==3.3.2.",2021/9/16 10:35,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,@pawsok Those users aren't stored in your LDAP directory are they (only in the Airflow DB)?,2021/9/16 10:50,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,That's also my thinking - that they are just in the DB and you won't find them in your LDAP - can you please double-check that?,2021/9/16 10:52,6,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"Users exist in Airlow DB and also in LDAP directory, otherwise I think that users are not able to log in to Airflow.",2021/9/16 11:12,7,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"But did you actually check if they are in LDAP directory after you add them in Airflow manually (and whethere they were not there before)? Just add a new user in airflow and see if it appears in LDAP.
As @BasPH mentioned - the LDAP syncrhronisation works in the way that LDAP users are synchronized from LDAP to Airflow DB (and this should be refreshed either periodically or at login). I highly doubt there is a way Airflow could create an LDAP user - it must have been in LDAP already  when you added it. And in this case you should not ""add"" the users manually but you should run synchronisation to bring the LDAP users to Airflow.
Eventually those users will end-up in Airflow DB but what @BasPH and myself are trying to say is that you should not need to add them manually - they should be automatically synchronized by Airflow.",2021/9/16 11:20,8,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"I do not have access to check LDAP directory, but so far it has worked this way - if the user added doest not exist in LDAP it was just impossible to log in to Airflow. So I don't expect that when adding a user in Airflow, user will also be added to LDAP automatically. That was the way it worked flawlessly.
What I did add additionally (I will remind you that I have version 2.0.1 now):

install 2.0.2 (button ""add"" is still visible)
install 2.1.0, as the next version after ""working"" one (button ""add"" is not visible)
create a fresh installation on my local laptop 2.1.3 (button ""add"" is not visible)
compare LDAP permissions between 2.0.1 and 2.1.0:

2.0.1:
can_add UserLDAPModelView
can_delete UserLDAPModelView
can_download UserLDAPModelView
can_edit UserLDAPModelView
can_list UserLDAPModelView
can_show UserLDAPModelView
can_userinfo UserLDAPModelView
userinfoedit UserLDAPModelView
2.1.0:
can_add UserLDAPModelView
can_delete CustomUserLDAPModelView
can_delete UserLDAPModelView
can_download UserLDAPModelView
can_edit CustomUserLDAPModelView
can_edit UserLDAPModelView
can_list UserLDAPModelView
can_read CustomUserLDAPModelView
can_show UserLDAPModelView
can_userinfo UserLDAPModelView
userinfoedit UserLDAPModelView
As we can see here (and also in the source code) something has changed about LDAP settings.",2021/9/20 6:58,9,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"Did some simple testing but haven't found the cause. Granted all possible permissions to the Admin role:

But no button when logged in with Admin role.
Between Airflow 2.0.1 and 2.1.3 the Flask-AppBuilder version changed from 3.1.1 to 3.3.2. There were some extensive changes to the LDAP authentication mechanism (PR), but I wasn't able to quickly pinpoint any potential changes with regards to falling back to local users.

Regardless, I'd argue that this is the intended way it should work. If there is only one authentication mechanism (LDAP), then there shouldn't be a second place for users to live in. From a security perspective, having two places for user accounts to live in, increases the attack surface.
You could ask your LDAP admin if it's possible for yourself/your team to create new accounts in your LDAP directory. Assuming you were using the local users for debugging purposes only, you might also want to look at account or password expiration options in your LDAP.",2021/9/20 8:37,10,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"Hi, it turned out that this is a problem related to missing permissions in Airflow. Class CustomUserLDAPModelView inherits from MultiResourceUserMixin and I noticed there isn't can_create action, so what I did:

Edit /www/views.py and class MultiResourceUserMixin

from:

to:


Run airflow sync-perm
Button (to add user) is visible now in Users View.
My colleague is able to log in to Airflow once I added him (he is of course in LDAP directory)

Everything was done on Airflow 2.1.3 version.
",2021/9/24 8:15,11,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,But will it work with sync without adding the user ?,2021/9/25 19:25,12,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,"@potiuk No, This works only if I add a new user directly from Airflow who is already in LDAP. Users existing in LDAP do not appear automatically in Airflow.  This approach is right for me because it gives me an extra layer of authentication that I can manage myself.
So I only specify one parameter, the path to LDAP server: AUTH_LDAP_SERVER = ""ldap://xxxx.xxxx.xxxxx"" in webserver_config.py.",2021/9/27 5:44,13,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,I see - I created the issue back then #18545  and I understand where it came from - you simply do not want automated user registration with LDAP. That's fine and there are others with similar workflow.,2021/9/27 8:57,14,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18290,2021/9/16 9:31,Is there a similar fix for OAUTH? The register user button is not visible.,2022/4/6 14:52,15,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"I just recently upgraded to 2.1.3, and I'm seeing the same thing.
on some cases, upstream tasks succeeds, but the downstream tasks are stuck in an upstream_failed state.",2021/9/3 14:31,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"Related: #16625
PR trying to address this: #17819",2021/9/3 17:27,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"Following the discussions at #17819 , you applied the fix then in your DAG above, prepare_timestamps failed and retried but create_athena_partition is stuck in up_for_retry?",2021/9/16 16:01,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,@Tonkonozhenko Do you have wait_for_downstream args set?,2021/9/16 18:17,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"We're also seeing this. Most of our DAGs have well under 100 tasks, a few just under 200 tasks, 673 active DAGs, 179 paused DAGs. We do not use wait_for_downstream anywhere.
We started seeing this after upgrading to 2.1.3 which we upgraded to specifically get the bug fix PR #16301, not sure if that bug might be related since we seem to be having weird status issues all over Airflow...
We see this in all manner of DAGs, some with a very linear path, some that branch into 100 tasks and then back to 1, others with 2 pre-requisite tasks into the final task.
Behavior:

upstream tasks all successful
downstream task(s) marked as upstream_failed
sometimes an upstream task will have a previous run marked as failed but then it retries as successful, almost as if the downstream tasks get marked as upstream_failed on that run but then don't get cleared for the subsequent retry.  But this does not always happen: we have seen multiple dagruns a night have upstream_failed tasks where all tasks prior worked on their first attempt (or at least only have logs for 1 attempt).

Please advise on what other information we can provide.",2021/9/17 1:08,5,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"@WattsInABox. If you can get scheduler logs when this happens, that would be very helpful.",2021/9/17 1:31,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"@ephraimbuddy, @WattsInABox perfectly explained what happens. We have the completely same situation.",2021/9/17 7:48,7,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"@Tonkonozhenko @WattsInABox Do you see FATAL: sorry, too many clients already. in your scheduler logs when this happens?
If there__ reproducible step please share",2021/9/17 8:48,8,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"@ephraimbuddy unfortunately, I don't have 2.1.3 logs now, but for 2.1.2 no such error and no fatal errors at all",2021/9/17 10:36,9,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"Trying to get to a reproducible step here...
Is there an existing ""unit"" test (or could you help me write a unit test) for:

A -> B dag
A set to fail with retries more than 1

And then see if the failure & retry handlers do what I think they're doing? That is:

A set to failed
B set to upstream_failed
A retries
B is untouched
A succeeds
B left in upstream_failed
",2021/9/17 13:08,10,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"Hi @ephraimbuddy - I work with @WattsInABox. We don't see FATAL: sorry, too many clients already. but we do see:

This causes the job to be SIGTERM'ed (most of the time, it seems). The tasks will now retry since we have #16301, and will eventually succeed. Sometimes it is SIGTERM'ed 5 times or more before success - which is not ideal for tasks that take an hour plus. I suspect also at times this results in the downstream tasks being set to upstream_failed when in fact the upstream is all successful - but I can't prove it.
We tried to bump the AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC to 60 to maybe ease up on hitting the database with no luck. This error also happens when only a couple DAGs are running so there is not much load on our nodes or the database. We don't think it's a networking issue.
Our pool sqlalchemy pool size is 350, this might be high - but my understanding is the pool does not create connections until they are needed, and according to AWS monitoring the max connections we ever hit at peak time is ~300-370 which should be totally manageable on our db.m6g.4xlarge instance. However, if it's a 350 pool for each worker and each worker opens tons of connections that are then alive in the pool - perhaps we are exhausting PG memory
Do you have any additional advice on things to try?",2021/9/18 22:52,11,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"
Trying to get to a reproducible step here...
Is there an existing ""unit"" test (or could you help me write a unit test) for:

A -> B dag
A set to fail with retries more than 1

And then see if the failure & retry handlers do what I think they're doing? That is:

A set to failed
B set to upstream_failed
A retries
B is untouched
A succeeds
B left in upstream_failed


It's not supposed to set B to upstream_failed if A has retries. What I believe happened is that the executor reported that A has failed but A is still queued in Scheduler. Currently, A is failed directly which we are trying to fix at #17819.
You can temporarily add a patch that removes this two lines:


and wait for #17819 to be fixed.
EDIT
Since you__e getting SIGTERM as explained by @taylorfinnel, this seems related to pending pod timeout deletion.  Increase this interval  worker_pods_pending_timeout",2021/9/19 22:15,12,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"
Hi @ephraimbuddy - I work with @WattsInABox. We don't see FATAL: sorry, too many clients already. but we do see:

This causes the job to be SIGTERM'ed (most of the time, it seems). The tasks will now retry since we have #16301, and will eventually succeed. Sometimes it is SIGTERM'ed 5 times or more before success - which is not ideal for tasks that take an hour plus. I suspect also at times this results in the downstream tasks being set to upstream_failed when in fact the upstream is all successful - but I can't prove it.
We tried to bump the AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC to 60 to maybe ease up on hitting the database with no luck. This error also happens when only a couple DAGs are running so there is not much load on our nodes or the database. We don't think it's a networking issue.
Our pool sqlalchemy pool size is 350, this might be high - but my understanding is the pool does not create connections until they are needed, and according to AWS monitoring the max connections we ever hit at peak time is ~300-370 which should be totally manageable on our db.m6g.4xlarge instance. However, if it's a 350 pool for each worker and each worker opens tons of connections that are then alive in the pool - perhaps we are exhausting PG memory
Do you have any additional advice on things to try?

In 2.1.4 we added some limits( to the number of queued dagruns the scheduler can create and I'm suspecting that the issue we have on database connections will go with it. I was having FATAL: sorry, too many clients already. db error until the queued dagruns was limited in this PR #18065.",2021/9/19 22:22,13,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"@taylorfinnell , I will suggest you to increase the value of this configuration worker_pods_pending_timeout, not sure if it__l resolve it but it__ also connected with sending SIGTERM to task runner because pods are deleted by it.",2021/9/20 0:55,14,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,Thanks! It seems to me that setting is specific to the k8s executor - but we are using the CeleryExecutor,2021/9/20 1:17,15,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"@Tonkonozhenko @taylorfinnell

That ERROR basically says it can't connect to metadata DB -- Where do you have your Metadata DB?",2021/9/20 11:06,16,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"Our metadata DB is in AWS and is a db.4xlarge that mostly looks like its chilling out doing nothing every day. The most action we see is spikes to 350 connections (there's enough RAM for 1750 connections). We're working on weeding out if the spikes are causing issues, but IMHO Airflow should not be falling over in the heartbeats b/c of a first-time missed connection. There should be some intelligent retry logic in the heartbeats...",2021/9/20 14:33,17,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"
Our metadata DB is in AWS and is a db.4xlarge that mostly looks like its chilling out doing nothing every day. The most action we see is spikes to 350 connections (there's enough RAM for 1750 connections). We're working on weeding out if the spikes are causing issues, but IMHO Airflow should not be falling over in the heartbeats b/c of a first-time missed connection. There should be some intelligent retry logic in the heartbeats...

Indeed, we do have some retries in few place, this might not be the one and needs improving. Does this error occur without those network blips / DB connectivity issues?
Can someone comments steps to reproduce please",2021/9/20 15:00,18,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18395,2021/9/3 13:01,"
IMHO Airflow should not be falling over in the heartbeats b/c of a first-time missed connection. There should be some intelligent retry logic in the heartbeats...

Actually I do not agree with that statement.
Airflow should rely on the metadata database being available at all times and loosing connectivity in the middle of operation should not be handled by Airflow. That adds terrible complexity to your code and IMHO is not needed to deal with this kind of (apparent) instabilities of connectivity. Especially that it is a timeout on trying to connect to the database. In case of SQLAlchemy and ORM database level we often do not have control on when your session and connection is going to be established and trying to handle all such failures on application level is complex
AND also it is not needed on application level in Airlfow anyway even if you want to deal with those kind of instabilities - especially in case of Postgres. For quite some time (and also in our Helm Chart - for a long time we recommend everyone using Postgres to use PGBouncer as a proxy to your Postgres database. It deals nicely also with a number of connections open (Postgres is not good in handling many parallel connections - it's connection model is process based and thus it is resource hungry when there are many connections opened)
PGBouncer does not only handle managing of connections pools shared between components, but also allows to react on similar network connection conditions - first of all, it will reuse existing connections, so there will be far less connection open/close events between PGBouncer and the Database. All the connections opened by airflow will go to locally available PGBouncer which will make them toally resilient to networking issue. Then PGBouncer will handle errors which you can fine-tune if you have connectivity problems to your database.
@WattsInABox  - can you please add PGBouncer (s) to your deployment and let us know if that improved the situation. I think this is not even a workaround - it's actually a good solution (which we generally recommend for any deployment with Postgres).
I will convert it into discussion until we hear back from you - with your experiences with PGBouncer and if those problems are still occuring after you get PGBouncer running, with some reproducible case.",2021/9/20 23:07,19,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,@thejens I have assigned you to this ticket,2021/9/17 11:39,1,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"Just one comment on that (and a bit of warning) and possibly an explanation to your @thejens surprise and disbelief (which is probably coming from not understanding the full scope of that task and impact it has on the distributed Airflow architecture).
The UI backfill requires a bit more than ""simple implementation"". This has been discussed several times at the devlist and the problem here is not the UI but ""control plane"". When you use the CLI, the admin user fully controls and manages the terminal, all the errors and potentially long running process the backfill might be., If you backfill a  lot of data, it can take a lot of time and backfill generaly works in the way that it will sequentially run historical runs.
Currently, there is no ""long running"" process in the Wab UI. all what webserver runs are gunicorn processes, that are restarted periodically and none of the worker processes survive across a page refresh.  Webserver is stateless and keep all the state in database, so it can (and will be) restarted at any time.
Backfill is entirely different thing. It has to run sometimes for hours and actively monitor the backfilled DAGs/tasks, react to failures etc. So running that from the webserver is not the best idea - ideally this should be another component in scheduler or a separate component like triggerer (coming in 2.2) to run the backfill command and the UI should at most be used to trigger it, and display the status. Taking into account that backfill is an ""afterthought"" - not something that is and should be done on a regular basis - having a separate component to serve that case (where there is a CLI for ad-hoc operations) is not the highest priority.
So in short this task is more of a backend/architecture change than UI , and it's quite a complex piece I think especially if you want to make sure that it works with multiple schedulers for example.
And yeah - I think it's a useful one, though IMHO it's not ""critical"" and weighting implementation complexity with the ""value"" of it (where you have CLI backfil) - it's not at all a surprising for me we do not have it yet.",2021/9/17 14:25,2,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"We have implemented 2 versions of backfill tools for our users. I want to share them here and suggest another version that may be more general to add to Airflow.
Backfill Queue
In this version, we were using CeleryExecutor.
User experience
Users go to a UI page to trigger a backfill. They would submit a form specifying the dag, the list of tasks, the date range, and  some flags (run backwards, mark success, ...)
After the submission is accepted, the user go to another page to watch the status of the backfill (queued, running with % of success, and list of ""aborted"" backfill dues to un-recoverable errors or backfill deadlock.) From this page, the author of the backfill can also abort the backfill that is in queued or is running or retry a backfill from where it fails.
Extra components added

Redis queues to hold the queued backfill submissions, running backfill chunks, and aborted backfills.
Backfill UI plugins which renders the form and the status page.
Backfill API which handle read and write to the Redis queue, used by the UI component.
Backfill ""worker"" which dequeue and run the backfill (via Python) with timeout and retry.

Disadvantages

Only runs 1 backfill at a time, many backfill stuck in queue
When backfill is aborted, all running DAG runs remains in running state and still require a manual action

Backfill on demand
In this version, we use the KubernetesExecutor.
User experience
User issues a backfill (in our case, it is in form of a chatops command, which is a http request to the chatops server. The chatops server have access to our K8s cluster, and bring up a pod that runs backfill.)
User can get logs and abort the backfill via different chatops command (chatops server APIs).
Extra components added

K8s Backfill template wrapped by if backfill enabled (default to False)
chatops server (which we already have to handle deploying DAG changes)

Disadvantages

When backfill is aborted, all on-going DAG runs remains in running state and still require a manual action

Proposed solution
User experience
User goes to the DAG page and trigger a backfill with data range and other backfill flags
Extra components

K8s Backfill template as described above. The UI would be using this template to bring up a new pod that runs backfill based on the user's input
UI pages that allows user to select data range and flags
UI pages that show status of the backfill or error messages from the backfill pod

I have wanted to share our ideas and hope to convert with Airflow's implementation so that we do not need to manage our own patch. We are happy to see this issue being assigned. Please let me know if there are more information that I can share to help pushing this further. Otherwise, we will be happy with whichever implementation Airflow chose to support better backfill experience.",2021/9/20 23:01,3,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"It seems backfilling comes with some baggage in terms of definitions.
What I'm after is a way to insert multiple dag-runs for historical dates in bulk from the UI, possibly with some tasks already marked as complete/skipped, as well as clearing tasks/dagruns between certain dates.
I don't see the need for a dedicated backfill process to run, the scheduler could take care of that I believe, if tasks and dags are idempotent they don't even need to care about execution order, if order matters I guess depends_on_past should be set on the tasks(?) and the scheduler should handle it(?)
I also don't consider backfill an afterthought. We often instrument new DAGs and want to execute them for historical dates to create historical data.",2021/9/21 8:31,4,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"
I don't see the need for a dedicated backfill process to run, the scheduler could take care of that I believe, if tasks and dags are idempotent they don't even need to care about execution order, if order matters I guess depends_on_past should be set on the tasks(?) and the scheduler should handle it(?)

Not currently as far as I understand how Scheduler works. The Scheduler currently is DAG based,  not individual task based. It looks at the DAGs and task dependencies for the ""future"" runs, schedules and executes them. There is no way (as I understand how scheduler runs) to get it start, monitor, send for exacution and overlook to completion selected tasks from selected dag from the past. The current architecture is that scheduler only looks ahead (possibly starting from the past if the dag has never been run) at the DAGs and determines which are the next tasks should be run for it and sends them to executors to execute - but there is no past scheduling for selected tasks). The database queries, scheduler loop, selecting which tasks to run next and when are heavily optimized for that use case and you would not be able to use it for re-running tasks without pretty much complete overhaul.
But maybe I am wrong, and do not understand well enough how scheduler works. I am happy to get corrected if I am wrong here - would love to hear from others who understand better how scheduler work.
From what I know this will likely change in the future, when Scheduler will become more ""task based"" (this is planned and will likely be implemented in 2.3 or 2.4) and once this is done, the behaviour you describe will be possible, but it's quite a big effort and changing behaviour of scheduler, as well as allowing DAG versioning, and this is yet another reason why implementing backfill now is basically a lost effort as it will have to be re-implemented. So either we implement it as a ""tactical"" solution now quicklly - with the management of backfill process separately from scheduler - with limited effort and reusing a code that others developed (see @kimyen) or we wait with that until the task-based scheduler becomes a reality an reconsider it then IMHO.
I uderstand it's important for you to run backfill, but certainly the ""afterthought""  for me is that this is something you anyway have to trigger and overlook manually, and it something that is usually managed and run by a very small number of users who have special permission and access usually and not something that is needed by all the people who write and observe the DAGS. The audience here is far smaller and this is yet another justification that CLI is ""good enough"" for now I think.",2021/9/21 15:21,5,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"
What I'm after is a way to insert multiple dag-runs for historical dates in bulk from the UI, possibly with some tasks already marked as complete/skipped, as well as clearing tasks/dagruns between certain dates.

@thejens without knowing the explicit use case, seeing all the things you listed here prompt me to this doc I wrote. I think that there are already multiple ways to ""backfill"" using the scheduler. See Manual Backfill > Airflow builtin options.
Notes that the ""backfill request view"" in the below doc is for the first backfill tool I mentioned above.
How to backfill your DAG
Background
There are multiple ways to backfill a DAG in Airflow. We will attempt to describe when to use each option.
Use case 1: New DAG
A new DAG is created on April 4th, 2020 and we want the DAG to start collecting data since March 1st, 2020.
To achieve this, while writing the DAG definition, we can set catchup=True and ""start_date"": datetime(2020, 3, 1) in the DAG's default_args.
When the code is deployed to production, the backfill (from March 1st to current time) is automatically started by the scheduler.
Use case 2: Extend DAG runs further in the past
An existing DAG has DAG runs starting from March 1st, 2020. We want to extend it to January 1st, 2020. We can achieve this by:

ensuring that the DAG has catchup=True, and
change the start date to January 1st, 2020

When the code is deployed to production, the backfill (from January 1st to March 1st) is automatically started by the scheduler.
If there are any successful DAG runs after the start date, Airflow is not going to catchup. See Start from Dag Runs view to delete the successful DAG run and trigger the catchup process.
This can also be achieved by manually backfilling the DAG from January 1st to March 1st. See manual backfill section.
Use case 3: DAG logic change
An existing DAG was used to calculated some metrics. However, the calculations need to be updated, and all past successful DAG runs need to be rerun to update the resulting data for those days.
For example, the change was deployed to production on May 1st, 2020. The May 1st DAG run is then scheduled to run and uses the new DAG logic. However, all prior DAG runs from January 1st, 2020 to April 30th, 2020 need to be re-run.
In this case, a manual backfill needs to be triggered. See manual backfill section.
Use case 4: New Task
A new task is added to an existing DAG. Regardless if your DAG has catchup=True, since the existing DAG runs have been completed, the scheduler will not automatically trigger backfill runs for the new task.
In this case, a manual backfill needs to be triggered. See manual backfill section.
Manual backfill
Understand backfill and scheduler
This page describes how the Airflow scheduler, catchup, backfill, and external triggers works. We highly recommend that you understand these concepts before performing a manual backfill.
Things to check before starting a manual backfill
Answer to these questions will also help to choose a more fit option to perform backfill.

What: Backfill the entire DAG or a specific task(s)
When: Time range you want to run the backfill
Pre-condition: Are there dependencies for your DAG/task? If so, have the dependencies been met or the pre-conditions been satisfied?
How:

Are there existing DAG runs for the period you want to backfill? If so, do you want to re-run or skip them?
If you want to backfill a specific task, can the upstream task(s) be ignored?


Impact:

What data does this backfill change?
Would it result in duplicated data?



Airflow built in options
Airflow has multiple built in options to trigger a backfill manually.
Start from Tree view
The tree view option is best fit if you only want to backfill a handful of specific tasks.
Only use this option if the task's dependencies has been met.
Go to your DAG's tree view:

click on the task you want to backfill
On the line with the ""Run"" button, click ""Ignore Task State""
Click ""Run""

Start from Task Instance view
The Task Intance view option is the best fit if you want to backfill more than a handful of specific tasks. (When clicking on each task and start running them manually is taking too much time.)
Only use this option if:

your DAG is configured with catchup=True,
the tasks' dependencies has been met,
the backfill period is covered by the DAG's start_date - end_date range.
there are existing task runs for the task(s) you want to backfill

This option is accomplished by clearing out the task instances for the task run(s) you want to backfill. The scheduler will schedule new task runs to fill in the ones that have been cleared out.
Let's imagine we want to backfill fill_meu_v2_retention from key_metrics_cube DAG between 2019-10-01 and 2019-10-10. Here are the steps to carry out this option:


Go to Browse -> Task Instances and find the tasks you want to backfill.


In this example, the target backfill task is fill_meu_v2_retention. However, we also need to clear the drop_meu_v2_retention task to make sure data is not duplicated. Select all the task runs for fill_meu_v2_retention and drop_meu_v2_retention during the backfill period; and click on With selected -> clear




Airflow is not going to catchup if there are already completed DAG Runs, we need to clear those up to trigger the catchup process. See Start from Dag Runs view.

You should start seeing tasks running shortly.
Start from Dag Runs view
The DAG Runs view is the best fit to re-run DAG(s)/task(s) that already have DAG runs.
Only use this option if:

your DAG is configured with catchup=True,
the tasks' dependencies have been met,
the backfill period is covered by the DAG's start_date - end_date range,
the task runs for the task you want to backfill do not exist, or have been deleted using the instruction here. If not, please see Start from Task Instance view

In order to do this, you need to:

pause the DAG
go to Browse -> DAG Runs
Add Filter and filter by Dag Id
Once you've found all the DAG Runs within your backfill period, select all of them and delete them.
Unpause your DAG to trigger the catchup process.

Once you've deleted the DAG runs, these DAG runs will disappear on the Tree view. However, any task instances for those DAG runs (which existed before you deleted the DAG runs) will still be there. The scheduler will automatically schedule new DAG runs and only run the tasks that have not been completed.
Backfill Request view
The backfill request view is built on top of Airflow CLI and is best fit for backfilling a long period of time.
Only use this option if:

The backfill period will not be worked on by the scheduler:

The backfill period is not within the DAG's start_date - end_date range; or
There are existing DAG runs for every DAG run in the backfill period; or
The DAG has catchup=False; or
The DAG is paused.



Use the Backfill request UI to submit a backfill request. Backfill requests are first come first served. You can watch the position of your request in the queue by looking at the Status tab.",2021/9/21 20:56,6,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"Hi, I guess the fact the manual for how to backfill being 4 pages long if printed speaks to the need for a better UX.",2021/9/22 7:01,7,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"
Hi, I guess the fact the manual for how to backfill being 4 pages long if printed speaks to the need for a better UX.

I understand your frustration with Airflow being complex system, but this is a fact of life that you have to live with and embrace  it, I am afraid.
The 4 page long manual needed speaks to the need of better UX is is quite a bold statement which completely does not take into account who are the users, what are the use cases, how complex cases are being handled.  I am not sure how you could make such a ""general"" statement because it is extremely narrow-viewed IMHO and is only valid for one small group of users (who are not even likely to be Airflow Users).
If you are developing ""non-skilled users"" tool - then this statement would be right. But If you create a tool for specialists who know what they are doing and have to be able to execute a number of complex tasks with plenty of variations then well you need to develop tool that does the job - and sometimes it means some operations there require complex UX.
Take a look at GIT. Is the UX simple? Forget it. I know by heart just a few commands (probably about few % of what's tehre) . But whenever I need something I search google, possibly even look up the documentation (sometimes many tens of pages long) and find the right thing, execute one command with ~ 30 parameters which does exactly what I want.
This is the same with Airflow. It's UX is complex because it allows for complex operations to be executed.
And since Airflow is developed by community and you have the feeling that the UX is too complex - you are absolutely free to propose changes to simplify it. If you have any concrete ideas how to do it. I would encourage you to do it. We have discussions in GitHub, Devlist discussions, we have AIPs that you can write and sent to the devlist and propose improvements.  You are absolutely free to do it (from me you just got some explanation and warnings that if you want to approach it, you will likely have to think about consequences and impact it has on the architecture).
BTW. Since this is neither a bug, nor concrete feature proposal, I will convert it into discussion.",2021/9/22 9:04,8,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"My users are novice users. They were told by someone else in their team to go backfill a DAG without knowledge of what the DAG does, nor how Airflow works. That's why I need very detailed doc to point them to.
I do agree that Airflow is a complex system and that gave it the flexibility to do all the things that it does.
I don't have a strong opinion on how backfill should change fundamentally. I think it works fine as long as you understand when and how to use it. Though there are several things I would like it to do better, including no deadlocks and cleaning up the DAG runs it left behind when it failed. The DAG runs should be in failed state (instead of in running state as of current behavior.)
Other than that, the problem I'm trying to solve is to get out of people's way. @potiuk , it was not scalable for a few of us to perform backfill for 300+ DAGs that our users wrote, and constantly develop. Performing 1 backfill for 1 DAG would take days to weeks for us. When we were the expert that do backfill for users, we would be doing it full time. Imagine you hired someone with Airflow expertise to be full time backfill executor, prepare the command, run it, watch it, clean up when backfill fail and retry on deadlock. Someone who can understand the complex of Airflow would be bored doing this job.
We proceed to find a compromise on making the tool more accessible and attempt to educate our users to self-served. It has been working quite well for us. Our users starts teaching members of their team to do backfill the right way, and only come to us when they get in a complex situation that the doc I provide doesn't cover. This effort is only as successful as it is because we also invested in providing idempotent operators. This simplifies the pre-conditions and allows the tool to automatically retry backfill that gets deadlock error.
In short, the problem I'm trying to convert with apache/airflow on is making backfill more accessible. And if this concludes in getting a different, simpler backfill experience, I would love that.",2021/9/22 16:45,9,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"Just to be clear - I am not ""against"" improving the Backfill - it's just (again repeating the comments)  - without impacting the scheduler implementation heavily the ""general"" backfill implementation is ""hard to be generic"" - i.e. work in all deployments (not only Kubernetes). And it might be WAY simpler if the DAG versioning and task-based scheduling is implemented - so improving it now as ""general"" case might simply not make sense.
However if there is a code that can be generalised and contributed fairly easily to improve it - I am all for it (but someone will have to take a lead, propose that on the devlist, discuss, arrive at consensus and implement it to completion - this is how it works here). So if you @thejens or @kimyen (especially based on your code you have already) are willing to do it - feel free to do so.. Open discussion in devlist, write an AIP, or if those are small improvements - without impacting the architecture - open PRs directly. A lot of contributors do it. Just wanted to make you aware that you have to be prepared to make it work alongside versioninig/task based scheduling which is likely something that will be primary focus for 2.3.
Some parts of what @kimyen iplemented and the docs describing scenarios are very usful to start,  and likely there are small things that can be improved incrementally (candidates for direct PRS) and some clearly have to wait for vesioning/task based scheduling


Use Case 1 is just normal behaviour of new DAG. this is not likely to need any improvement.


Some kind of action to perform combined actions described by @kimyen  as :



Use case 2: Extend DAG runs further in the past
Start from Task Instance view
Start from Dag Runs view

Those seems to rely on a series of actions that could probably be contained in one action where you a) clear all successful runs if they are there, pause the DAG and somehow inform the user that new version of DAG can be uploaded with changed start_date and catchup to get all data from past run.
Sounds like this can be done without touching the scheduler mechanism. That is a good ""tactical"" improvement before more comprehensive backfill covering variety of cases is implemented.


The cases with changing DAG structure (new tasks, changing DAG layout)  is something that is precisely the point of DAG versioning (DRAFT https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-36+DAG+Versioning) - this is likely something we will be discussing shortly to finalise the AIP, Idea. and see all consequences. This is likely not something that can be improved before that work is completed.


The Backfill Request View ""UI"" based on K8S deployment is likely to not be incorporated - unless you could work out a way to make it work without K8S. That requires a new component for Airflow (so that it can work also for Celery and Local and Sequential executor - for those users who do not use K8S. While Airlfow Loves K8S, it cannot rely on K8S for any core function - we are Python Centric, not K8S centric (We had talk about https://airflowsummit.org/sessions/2021/airflow-loves-kubernetes/) . So unless you can commit to making this work in K8S-independent way, I'd say that might be turned for example into a plugin available at https://airflow.apache.org/ecosystem/ but not as part of Airflow Core. Especially that once DAG versioning and Task-based scheduling will be implemented, we will likely be able to make in much simpler way - without utilising current CLI approach.

",2021/9/22 19:12,10,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,@potiuk what we have worked for us. I like the idea of waiting for the task based scheduler. How would backfill work with a task based scheduler? Is there a ADR (Architecture Decision Record) or discussion you can point me to?,2021/9/22 20:14,11,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,Just to add to that - a very closely releated stack overflow question from today: https://stackoverflow.com/questions/69290067/how-to-purge-historical-data-when-clearing-a-run-from-airflow-dashboard (for the need of manually messing with data if you want to catchup with longer history ) - so I think this is a very much needed feature to simplify this concrete use case @kimyen !,2021/9/23 12:22,12,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,"Maybe naive, but I see an algorithm that goes something like this:
A bulk command/UI that offers the ability to insert ""missing"" Dag-runs between 2 dates.
When doing so, specifying that all or certain tasks should be set to ""SKIPPED"" will be important to not re-start tasks that shouldn't or can't run for those dates.
If there are already Dag-runs for any dates between the 2 specified dates, instead of inserting new dagruns - taks without a previous status will/can be set to SKIPPED.
Once this command has finished running, all DagRuns required for a backfill are created, and all tasks in them are in either their previous state - if they had one - or ""skipped""
Now all you need to do is to clear the tasks that should be ""backfilled"". I'd suggest an interface where you clear taskinstance statuses between two dates.
The two above steps - insert missing dags and clearing tasks between dates - could likely be combined into a single UI where a user specifies a dag, start_date, end_date and what tasks should be cleared.",2021/9/24 9:06,13,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18428,2021/9/17 7:24,Hello everyone__ was wondering if anyone ended up implementing a UI for backfills? I'm interested in finding out whether the Airflow community has plans to support backfilling capabilities from the UI (perhaps this is something I could implement and contribute if the community supports it). Thank you for your time.,2022/6/29 3:00,14,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/18479,2021/9/22 23:02,"Hello @hpatel-higi,
This is not enough to go on to figure out the issues.
What about sharing a bit more specific information about:

How are these DAGs build up?
Do these errors only occur if they are launched at the same time?
Can we see some logs of the errors?
",2021/9/23 10:13,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18479,2021/9/22 23:02,"The DAGs are in a python file of their own.  Theres 20 DAGs in total that we have.  If create a DAG run for all of them at the same time thats when i get a DAGTimeout error and sometime AirflowTaskTimeout error.  I cant get the logs since its happening on Azure app service, but i will try and see what i can get.
Also when i was getting these errors i had my SECRET_KEY set to b'asdfasdfasdf'  not sure if that is correct or not but i changed the key to one that is generated from the following code: python -c 'import secrets; print(secrets.token_hex(16));'
I am seeing alot less errors then before.",2021/9/23 15:53,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18479,2021/9/22 23:02,"This is the error i got while just 4 DAGs were running.   I didn't have these problems in 1.10.8
",2021/9/23 17:20,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18479,2021/9/22 23:02,"This is what my airflow.cfg file looks like
",2021/9/23 17:26,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18479,2021/9/22 23:02,"I think if importing your DAG takes > 30 seconds, then you simply  try to do too much in your DAG's Top-Level code:
I just published some updates to documentation related to that - please read the docs and follow them:
https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#top-level-python-code
and:
https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#reducing-dag-complexity",2021/9/23 19:24,5,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18479,2021/9/22 23:02,"Like i mentioned in the previous comment, in version 1.10.8 we didnt have this issue.
It just started happening after we upgraded to 2.1.4.  Ours DAGs are already following the best practices.
Other thing to note is the CPU is usually around 100% usage when these errors happen.",2021/9/23 19:30,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18479,2021/9/22 23:02,"
Like i mentioned in the previous comment, in version 1.10.8 we didnt have this issue.
It just started happening after we upgraded to 2.1.4. Ours DAGs are already following the best practices.

How do you kow that? Those best practices were pubkished LITERALLY 10 minutes ago.
About 90% of the content there was not existing a week ago even in a PR and was build based on experience of people who had similar problems, so there is no way you could have followed it.
You can also increase the timeout (https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#dagbag-import-timeout)  of importing the DAGBag it after reviewing the practices you confirm that you followed them. I am converting this into a discussion, because clearly the problem is with import timeout.",2021/9/23 19:40,7,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/18511,2021/9/22 20:34,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/9/22 20:34,1,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/18511,2021/9/22 20:34,"How about using API calls to check the status of DAGs that should be enabled? This is much better than notification as it can be based on current state (and run periodically - even as a separate workflow).
Converting that into a discussion as I think this is not ready yet to be a ""feature"" and needs some discussion. We can convert it back if we find it is needed.",2021/9/24 19:43,2,1,(IV) Further Discussion
https://github.com/apache/airflow/discussions/18551,2021/9/27 13:15,"That is unlikely to happen soon IMHO.
It's a dificult one to pull if you consider how Python code is parsed, it would be next-to-impossible to find out which files to include extra and specifying them manually misses the point.
I also think Airflow is NOT good to show code other than DAG code - it's not ""code browser"" there is little value and high complexity to make a UI that would let you navigate between files etc. I think a much better approach would be to use another browser for your code and write a custom plugin in airflow to get the link to that browser.
For example if you have the code in Git and use GitSync, you could add a plugin to have a view where you redirect tto GitHub or GitLab UI. Even now GitHub (and I believe GitLab too) have automated detection of the linked code even in Python, so if you keep the code in the same repo you will be able to even navigate between DAG and imported code.
Let me convert this one to discussion , as I do not think we will ever want to make it a feature (but If others think otherwise we can always convert it back to a feature).",2021/9/27 16:04,1,1,(VII) Information Storage
https://github.com/apache/airflow/discussions/18608,2021/9/29 10:20,"I don't think this is quite a generic case for us to support it, but you are free to make any modifications to the logger configuration. Here is docs about it:
http://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-tasks.html#advanced-configuration",2021/9/29 10:52,1,,(VII) Information Storage
https://github.com/apache/airflow/discussions/18608,2021/9/29 10:20,"Yeah. I thin this is not a new feature - it is possible with advanced logging and I do not think we need to add a feature for it. Maybe  documentation update with explicit mentioning that you can enable multiple loggers this way might be useful. I am thinking about adding such documentation after several discussions I had recently.
For now I will convert it into discussion as I do not think there is a feature coming out of that (we can always move back to an issue if we get to conclusion that we should).",2021/9/29 15:20,2,1,(VII) Information Storage
https://github.com/apache/airflow/discussions/18608,2021/9/29 10:20,"Good suggestions, thank you!",2021/9/29 15:22,3,,(VII) Information Storage
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/7/21 1:48,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,Curious - does it work when you run airflow scheduler without the --daemon flag?,2021/7/21 17:13,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"
Curious - does it work when you run airflow scheduler without the --daemon flag?

I think it works as the webserver can detect the scheduler is running and I can't see the error.",2021/7/22 1:09,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"I think I know the reason. In Scheduler, the SchedulerJob is instantiated before demon context is activated. SchedulerJob is a database ORM object from SQL Alchemy and it opens the connection to Postgres:

When you activate daemon context, what happens under the hood is forking the process, and while some of the opened sockets are passed to the forks (stdin and stderr but also the opened log file handle), the established socket for DB connection is not passed:

I will add a fix for that in a moment",2021/7/22 8:52,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"@potiuk is this fix in production? I am having postgres as database.
I am using below version packages.
Airflow - 2.1.2
sqlalchemy=1.3.24
psycopg2=2.9.1
Getting below error though i started the scheduler as ""airflow scheduler -D"".
""he scheduler does not appear to be running. Last heartbeat was received 3 hours ago.
The DAGs list may not update, and new tasks will not be scheduled.""
When checked airflow.scheduler.err got below error.

I will be highly obliged if you can let me know how to fix this issue.",2021/7/29 12:49,5,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"You have to wait until it is released. The PR has just been approved few hours ago and it's going to be released with one of the next releases of airflow (depending if we manage to cherry-pick it before relase 2.1.3 or whether it comes in 2.2, this might be in a week more or less or a month. The release needs to be announced, voted and published..
You can also manually apply to your version by cherry-picking this code.",2021/7/29 17:00,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,Thank you  @potiuk for quick reply.,2021/7/30 3:24,7,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"@potiuk I have copied the code in the external Libraries's dist-packages/airflow/cli/commands folder. After that Iran below commands to start airflow again.
airflow db init
airflow webserver -D
airflow scheduler -D
but the problem still exists. Please do find the err log below.
raceback (most recent call last):
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 2336, in _wrap_pool_connect
return fn()
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/pool/base.py"", line 364, in connect
return _ConnectionFairy._checkout(self)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/pool/base.py"", line 809, in _checkout
result = pool._dialect.do_ping(fairy.connection)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py"", line 575, in do_ping
cursor.execute(self._dialect_specific_select_one)
psycopg2.OperationalError: SSL SYSCALL error: Socket operation on non-socket
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File ""/usr/local/bin/airflow"", line 8, in 
sys.exit(main())
File ""/usr/local/lib/python3.8/dist-packages/airflow/main.py"", line 40, in main
args.func(args)
File ""/usr/local/lib/python3.8/dist-packages/airflow/cli/cli_parser.py"", line 48, in command
return func(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/airflow/utils/cli.py"", line 91, in wrapper
return f(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/airflow/cli/commands/scheduler_command.py"", line 54, in scheduler
job.run()
File ""/usr/local/lib/python3.8/dist-packages/airflow/jobs/base_job.py"", line 241, in run
session.commit()
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 1046, in commit
self.transaction.commit()
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 504, in commit
self._prepare_impl()
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 483, in _prepare_impl
self.session.flush()
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 2540, in flush
self._flush(objects)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 2682, in flush
transaction.rollback(capture_exception=True)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/langhelpers.py"", line 68, in exit
compat.raise(
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py"", line 182, in raise
raise exception
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 2642, in _flush
flush_context.execute()
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute
rec.execute(self)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/unitofwork.py"", line 586, in execute
persistence.save_obj(
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/persistence.py"", line 205, in save_obj
for (
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/persistence.py"", line 373, in organize_states_for_save
for state, dict, mapper, connection in _connections_for_states(
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/persistence.py"", line 1602, in _connections_for_states
connection = uowtransaction.transaction.connection(base_mapper)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 314, in connection
return self._connection_for_bind(bind, execution_options)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 421, in _connection_for_bind
conn = self._parent._connection_for_bind(bind, execution_options)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 433, in _connection_for_bind
conn = bind._contextual_connect()
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 2302, in _contextual_connect
self._wrap_pool_connect(self.pool.connect, None),
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 2339, in _wrap_pool_connect
Connection.handle_dbapi_exception_noconnection(
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 1583, in handle_dbapi_exception_noconnection
util.raise(
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py"", line 182, in raise
raise exception
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 2336, in _wrap_pool_connect
return fn()
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/pool/base.py"", line 364, in connect
return _ConnectionFairy._checkout(self)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/pool/base.py"", line 809, in _checkout
result = pool._dialect.do_ping(fairy.connection)
File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py"", line 575, in do_ping
cursor.execute(self._dialect_specific_select_one)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) SSL SYSCALL error: Socket operation on non-socket
(Background on this error at: http://sqlalche.me/e/13/e3q8)
Also when I use SUDO airflow webserver -D, it is not starting the webserver though getting webserver start msg.
2021-07-30 10:40:45,893] {backend.py:193} INFO - Loading KWallet
[2021-07-30 10:40:45,900] {backend.py:193} INFO - Loading SecretService
[2021-07-30 10:40:45,902] {backend.py:193} INFO - Loading Windows
[2021-07-30 10:40:45,903] {backend.py:193} INFO - Loading chainer
[2021-07-30 10:40:45,903] {backend.py:193} INFO - Loading macOS
[2021-07-30 10:40:47,022] {arrow_result.pyx:0} INFO - Failed to import optional packages, pyarrow

____    |( )_______  /  /________      __
____  /| |_  /__  /  / __  /  __ _ | /| / /
___  ___ |  / _  /   _  / _  / / // / |/ |/ /
//  |//  //    //    //  _/____/|__/
[2021-07-30 10:40:47,087] {dagbag.py:496} INFO - Filling up the DagBag from /dev/null
[2021-07-30 10:40:47,155] {manager.py:788} WARNING - No user yet created, use flask fab command to do it.
Running the Gunicorn Server with:
Workers: 4 sync
Host: 0.0.0.0:8080
Timeout: 120
Logfiles: - -
Access Logformat:
On the contrary when using scheduler -D, not start the scheduler.
I am not sure if I am making any mistake.",2021/7/30 6:30,8,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"
File ""/usr/local/lib/python3.8/dist-packages/airflow/cli/cli_parser.py"", line 48, in command

There might be multiple reasons why the change might be not taken into account when you just replace the files (.pyc code not recompiled, different location of airflow installation)
You can check the actual location by running this in your python interpreter (but it can also be installed in virtualenv, so you need to make sure you have that virtualenv activated).

Usually - to be 100% sure, i change such code and  raise some hard exception there directly and see if really the modified code is used.",2021/7/30 10:23,9,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"Hi,
I also applied suggested correction and raised an Exception just after entering method  create_scheduler_job, which indeed occurred when executing airflow scheduler -D.
However, I am also getting the error whose log is added below:
",2021/8/4 22:49,10,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"@potiuk Without webserver and scheduler together, airflow is incomplete. When webserver service starts, we can start scheduler service as a integral part. We can have a separate module also to restart the scheduler.",2021/8/5 4:11,11,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"As a workaround, you do not need to run airflow in Daemon mode.
There is no need to run daemon mode in most cases. You can run airflow in a background using whatever mechanisms you have to manage your deploymens (upstart/systemd/etc. ) and there is no need to use daemon mode at all..
Usually I recommend using Airlfow via docker images - where each component is a separate container - with Ariflow Helm Chart if you run on Kubernetes https://airflow.apache.org/docs/helm-chart/stable/index.html or with Docker Compose, or other deployment mechanisms you use. I am not sure why you are getting the error if you confirmed the problem is not fixed. I could not reproduce it so maybe it's also some problem with your environment or you have some customizations that inject sql-alchemy initialization befre the fork happens.",2021/8/6 11:59,12,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,Is there an update on when this fix will be released?  I'm having a similar issue with Aiflow 2.1.4.  I'm getting an error when I run scheduler as a daemon process on my local instance,2021/9/28 20:01,13,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,,2021/9/28 20:05,14,,(III) Not a Bug
https://github.com/apache/airflow/discussions/18614,2021/7/21 1:48,"There is no fix, because we do not know what the issue is. Actually your case shows an interesting problem and maybe the root cause for the issue:
OSError: [Errno 98] Address already in use
It looks like this is a deployment issue where you try to run two airflow components on the same server - schedulers and workers for example and they are all serving the  logs on the same port. The problem is most likely with your deployment. I am converting this to a discussion as this is likely a deployment issue.
Can you please describe @agentdanger what is the deployment you are using and how it hapened that you run several airflow components on the same server?",2021/9/29 21:22,15,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/19006,2021/10/15 5:05,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/10/15 5:05,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/19006,2021/10/15 5:05,"Why and how do you run mypy?  Which version? What are the exclusions you make ? How does it relate to mypy configuration that is part of Airflow itsefl?
Just to explain: Our Mypy is working perfectly fine for airflow - It's part of the static checks for every commit. There is no ""guarantee"" that all versions of mypy will be running always with all configuration and without exclusions - as usual with any static checkers.
I would love to know what you do and what your configuration you use, because I am genuinly interested, but it is hardly an issue with Airflow - this is a discussion really - that's why I converted it.",2021/10/15 8:02,2,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/19006,2021/10/15 5:05,"We are using 0.770 
 and since we are using namespace packages we had to do some workarounds https://github.com/apache/airflow/blob/main/scripts/in_container/run_mypy.sh
Also you have to remember that you have to have alll dependende packages installed to make sure that mypy runs in a repeatable fashion. Airflow has > 500 dependent packages and you need to have them installed all via ci_all extra wtth the same versions that are ""golden"" set of dependencies (pip install apache-airflow[ci_all] --constraint ... - see https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html ). Only then you might get repeatable mypy experience.",2021/10/15 11:52,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/19006,2021/10/15 5:05,"@potiuk Please, can you reopen issue since this type comment is incorrect?
spec.setdefault('consumes', ['application/json'])  # type: List[str]
Should be:
consumes = spec.setdefault('consumes', ['application/json'])  # type: List[str]
or completely remove type comment:
spec.setdefault('consumes', ['application/json'])",2021/10/18 16:03,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/19006,2021/10/15 5:05,"@brain-buster you beat me to it __
#19065",2021/10/23 1:23,5,,(III) Not a Bug
https://github.com/apache/airflow/discussions/19006,2021/10/15 5:05,Yeah. I see the problem was with running 'mypy` when checking your own 'DAGs' wth mypy. Merged I hope we can cherry-pick it to 2.2.1.,2021/10/23 9:36,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/19140,2021/10/21 14:57,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/10/21 14:57,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19140,2021/10/21 14:57,What's your docker-compose.yaml file? What Docker image are you using?,2021/10/21 15:59,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19140,2021/10/21 14:57,"
If you are having trouble running the latest Airflow image in aws-mwaa-local-runner then you should contact AWS Technical Support.",2021/10/21 16:01,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19140,2021/10/21 14:57,I am using the latest docker-compose from the documentation page. This has nothing to do with AWS.,2021/10/21 16:25,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19140,2021/10/21 14:57,"So why mwaa-airflow-webserver-1 name ? Looks like a strange name to keep the ""plain"" airflow image in.
More quesitons:

Did you follow precistly the instructions from https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html?
Did you modify any of the  configuration/files other than the steps specified in the instructions ?
Did you use official airflow image to run it (or did you use the original image) - which version?
Did you modify image in any way or used a different one?
Could you please copy &paste the whole output showing the exact image used ?

I think there is no easy reproduction without answers and likely the answers will indicate that there were some modifcation you've done. Conveting that into discussion until more information is provided.",2021/10/21 20:08,5,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19140,2021/10/21 14:57,found a way Daniel ? Even i am facing the same issue.,2021/10/22 5:29,6,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19312,2021/10/29 13:20,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/10/29 13:20,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19312,2021/10/29 13:20,"Converted it to a discussion (it's definitely not a feature issue).
I suggest you to clarify what you are asking for if you want to get feedback/help as it's entirely unclear what you are asking for.",2021/10/29 14:03,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19451,2021/10/16 18:13,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/10/16 18:13,1,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/19451,2021/10/16 18:13,Can you elaborate?,2021/10/18 5:03,2,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/19451,2021/10/16 18:13,"in class MySQLToGCSOperator
the definition  type_map = { FIELD_TYPE.BIT: '**INTEGER**', .... is incompatible with the function def convert_type(self, value, schema_type: str): ...... if schema_type == ""INTEGER"": value = int.from_bytes(value, ""big"") else: value = base64.standard_b64encode(value).decode('ascii')
which lead us to the error 400 when we want to transfer the table from GCS to BigQuery
google.api_core.exceptions.BadRequest: 400 Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 1; errors: 1. Please look into the errors[] collection for more details.",2021/10/19 10:12,3,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/19451,2021/10/16 18:13,Could you please try to edit your message so it can be understood.,2021/10/19 10:24,4,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/19451,2021/10/16 18:13,@Walid-peach if you found the issue. Can you please open a PR to fix it?,2021/10/21 7:22,5,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/19451,2021/10/16 18:13,Until there is a proposal of improvement (ideally as PR) I move it into discussion,2021/11/7 11:19,6,1,(IV) Further Discussion
https://github.com/apache/airflow/discussions/19575,2021/11/13 0:08,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/11/13 0:08,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19575,2021/11/13 0:08,"I disagree with that assessment. This has been deliberately added there as a way to promote ASF/Downloads mechanism. It's not only a downoad page but also educational content to teach people about the ASF and the mechanism it uses. The page is ideally placed there - it explains people the purpose, of the page, explains that they also can install Airlfow using PyPI (as convenience packages) and when they are supposed to use officially downloaded sources.
So I think it serves a very good purpose - especially if you empathethically think about users who land at this page who are different - they have no context, knowledge, experience with ASF mechanism. It is - I think - an important part  of communication: to provide more complex for people who want to explore it. And HTML (H stands for Hyperlink) make such additional context to be available as hyperlinked pages to the world-wide-web as a foundational element of WWW.
I do not see it as a clear ""issue"" with the documentaiton - so @sebb If you still want to discuss (and maybe others have a different opinion there) I will convert it to discussion, and if others will express their opinions there and you would like to convince the others - feel free. I am happy to reconsider my opinion if there are others who think similarly.",2021/11/13 17:57,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/2/22 1:55,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,Is gs://airflow the correct name for your GCS bucket? GCS buckets have globally-unique names so I would be surprised / impressed if you were able to create a bucket with this name.,2021/2/22 14:52,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,@SamWheating gs://airflow is just a mock name which is not the exact name on airflow.cfg,2021/2/23 1:00,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,hey @Rukeith  did you get a solution to this?,2021/8/17 4:52,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,"@rishabh-cldcvr Sorry, I haven't fixed this. I still have no idea.",2021/8/17 5:54,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,Any updates on this?,2021/10/22 18:33,6,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,If airflow gets 404 then service account probably doesn't have adequate permission,2021/11/3 17:58,7,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,"@potatochip Well, I am sure it has enough permission, I had made sure the service account got the highest permission. And airflow did save the log to my GCS.",2021/11/8 4:48,8,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,"Hi, I have same log symptom at the Task log (I didn't check the worker log, yet).
What is interesting is that it happens only 1-3 tasks per 500 daily and always changing when and where it happens but for me it is only BigQueryOperators which are afftected.
It is Composer composer-1.17.0-preview.3-airflow-2.0.1",2021/11/16 12:42,9,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,"I have a customer with the same symptom and could the airflow team share any updates on this?
The actual message is below:
",2021/11/24 5:41,10,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/19814,2021/2/22 1:55,"@stevenzhang-support  I think every case of that might be different, the fact that you get it intermittenly (I speculate because without details you said generic ""the same"" without being specific if it is persistent or intermittent) then most likely there is a deployment issue somewhere - if Airflow can read the logs in general and only sometimes it can't then probably some proxy, firewall, rate lmitation or something comes into play and you need to investigate your deployment. If it is not working consistently - you likely has badly configured access.
But it's impossible to get ""team"" answering that differently, because we have no clearly reproducible case that could help us diagnose it. If you are on Composer, then likely the best way is to reach out to composer support.
Since there is no clear reproducibility here, I will convert it to discussion - but if someone has similar issue and will be able to provide more diagnostics and reproducibility we can always convert it back to an issue.",2021/11/24 18:52,11,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/20059,2021/12/3 5:01,"I think you need to dig deeper. The docker-compose is just a quick start and you are on your own to make any modifications.  You have your own modifications in tehre, local dags, airflow.cfg, some devops folder. There are plenty of things that could go wrong - your images might be not available to your docker-compose, you have not restarted the webserver and scheduler, you have no docker-compose files you use.  I am afraid you made some mistake that you need to debug in your deployment and you need to track it down.
Let me just comfort you that others do not see this kind of problem, so it's for sure some problem on your side, not airflow. But  from what you posted, it's not obvious what it can be so you need to dig deeper (as usual when you configure your own custom deployment). I am converting it into discussion now - I recommend you to check all the steps of rebuild, reload, restart again, make sure that all your components use the same image with the same dags (note that import errors come from scheduler, not webserver - webserver only displays them from the DB that is filled-in by the scheduler.",2021/12/5 23:18,1,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/20059,2021/12/3 5:01,It was in fact my side. I had to remove the new directory which had the new dag. Still haven't reached the point where it'll break but at least it's working again. Thank you @potiuk!,2021/12/6 15:43,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20169,2021/12/8 12:43,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/12/8 12:43,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20169,2021/12/8 12:43,"There is not much information but from your description it looks like this (or similar): https://stackoverflow.com/questions/60574601/how-can-i-fix-a-sigabrt-error-on-my-macos-hight-sierra
Simply Airflow tasks run as non-interactive process and they have no acces to your graphical card (and your UI). The opencv-headless package allows you to build the visualisation using only CPU.
You could also use your GPU likely but that requires some more complex setting and running an in-memory X Server with GPU acceleration (but you 'd need to dig much deeper to get it working).",2021/12/9 1:02,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20169,2021/12/8 12:43,"Thanks @potiuk. Installing the opencv-python-headless package did not solve it for me, unfortunately.
I'm hesitant to dig into running it on my GPU. Do you jappen to know of another simpler workaround?",2021/12/9 10:19,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20169,2021/12/8 12:43,"You should really check what's the error message there. You can try to run your stuff for example from a cron job to avoid the ""interactive"" part.",2021/12/9 13:42,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20169,2021/12/8 12:43,And I am moving that to discussion - this is not really an Airlfow bug.,2021/12/9 13:43,5,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/20169,2021/12/8 12:43,"
_",2021/12/9 18:36,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,You should make sure each airflow deployment is in a different namespace if I am not mistaken @jedcunningham @kaxil ? Can you please confirm it?,2021/12/10 19:36,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,"Is this a new requirement as all our previous deployment prior to 2.1.3 is running without this issue. Also I am seeing this fix #14795 is that related ?
Thanks for responding so quick.",2021/12/10 19:42,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,"
Is this a new requirement as all our previous deployment prior to 2.1.3 is running without this issue. Also I am seeing this fix #14795 is that related ? Thanks for responding so quick.

I am not sure - it's just seems logical to me to keep each airlfow deployment in a separate namespace (that 's what I'd do in general at least). But yeah. I might be wrong and Airlfow should work this way (but it also could be accidental that it worked)",2021/12/10 19:47,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,That's why I am asking those who likely know better :),2021/12/10 19:48,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,"Right now, the most reliable way is to run in separate namespaces.
Both running multiple instances in a single namespace and using the multi_namespace_mode option have various edge cases that just aren't handled currently (though, it is on my radar!). This hasn't changed in recent versions and it's likely you just got lucky or didn't notice issues previously.",2021/12/10 22:50,5,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,Yeah. My though exactly,2021/12/11 15:47,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,"Moving that into a discussion then. I think, if we want to make multiple airflows in one namespace that shoudl be a separate feature. So maybe @bparhy - if you really think this is needed and you cannot rearrange your airflows to multipe instances - opening a feature request might be a good thing,",2021/12/11 15:49,7,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,I am testing with 2.2.3 and it's failing in _adopt_completed_pods because I guess it's trying to set - pod.metadata.labels['airflow-worker'] = ''  for completed pods via 1.10.10 deployment in the same namespace.,2022/1/24 18:48,8,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20219,2021/12/10 18:28,"For some reason this is happening even with single airflow deployment. Tried with 2.1.3 and 2.2.2, the issue is same. My tasks are getting failed whenever scheduler get restarted. I have deployed airflow in default namespace.",2022/2/4 4:43,9,,(III) Not a Bug
https://github.com/apache/airflow/discussions/20410,2021/11/8 15:18,"It seems to me that retries and rescheduling are not compatible. How would we implement a logic that Reschedules 5 times? The specific use case is that a database we are using has spurious problems with contention that cannot be disambiguated from actual problems with our query from the error message. (sum of all queries > capacity vs single query > capacity) and backing of should ameliorate the first while the latter is an error on our side, which should fail the task
We also thought about failing over into a pool of size one, but the DAG and logic would be quite odd (ie twice the same nodes with an intentional bottleneck in-between, but A' only runs when A failed with a specific error _ )",2021/11/10 11:29,1,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/20410,2021/11/8 15:18,"
It seems to me that retries and rescheduling are not compatible.

It's not the same at all. retry has nothing to do with AirflowRescheduleException
This seems more of a support question rather than a bug.
I'm converting to a GitHub discussions as Q/A",2021/12/19 20:43,2,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/20410,2021/11/8 15:18,AirflowRescheduleException is for sensors. Don't mix it in operators.,2021/12/19 20:45,3,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/20516,2021/12/25 19:17,"That is unlikely to happen because of security.
Callbacks are executed in the context of Worker or DagFileProcessor, Scheuler is not supposed to execute any code provided by the user in the DAG. IT's the scheduler that dermines which executor can be used, and it sends prepared task to the executor (sometimes based on the ""queue"") parameter. And as you mentioned - the celery workers pick the tasks from the queu that they are configured with, so by the time the task start, their queue already pre-determined where they should be run.
The only real place where you can change queue for the tasks is at teh DAG parsing time - which effectively means that once the task has been plced in the DAG structure it's queue has to be determined. You canot dynamically change it in scheduler. Schedulers just schedules whatever is declared in the code that comes ""pre-installed"" with airflow. - for example custom triggers, or custom timetables have to be pre-installed and DAGs cannot define their logic - they can at most declare and configure which timetable/trigger will be used.
So the only way it could be implemented is by defining some ""customizable"" mechanism of queue selection - rather than allow DAG writer to define it in the way that callbacks are defined.
I will convert it into discussion - maybe it will be picked by someone who would like to have similar mechanism, but at the very least it would require extensive discussion in devlist and AIP (Airflow Improvement Proposal).",2021/12/27 13:07,1,1,(IV) Further Discussion
https://github.com/apache/airflow/discussions/20750,2022/1/7 5:59,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/7 5:59,1,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/20750,2022/1/7 5:59,Converted it to discussion - this is not an issue of airflow. if you would like to get help with it - I think you must post a bit more details - like your code that generates the import (but I would guess it's just your sequence of imports that is wrong).,2022/1/7 14:34,2,1,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/21012,2022/1/21 9:04,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/21 9:04,1,,(VII) Information Storage
https://github.com/apache/airflow/discussions/21012,2022/1/21 9:04,"The SystemsManagerParameterStoreBackend is already available in Airflow as custom Secrets manager:
https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/aws/secrets/systems_manager/index.html#airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend",2022/1/21 9:54,2,,(VII) Information Storage
https://github.com/apache/airflow/discussions/21012,2022/1/21 9:04,"
The SystemsManagerParameterStoreBackend is already available in Airflow as custom Secrets manager:
https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/aws/secrets/systems_manager/index.html#airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend

The existing operator only provides a read for specific actions i.e. connection URI, Config, and custom variable pertaining to airflow information. I am talking about using SSM parameter store as metadata storage to share configuration/metadata across different subsystems, as well as airflow. This means all actions including setting and getting parameters.",2022/1/21 10:53,3,,(VII) Information Storage
https://github.com/apache/airflow/discussions/21012,2022/1/21 9:04,"I don't thik Airlflow will never have ""AWS"" or ""Google"" or any other ""specific"" configuration built in in other way that current ""customization options"".  Airflow is ""Cloud Agnostic"".
It provides features to ""talk"" to external systems (via Providers: Hoops, Operators, Sensors) and airflow configuration capabilities can be extended precisely via Custom Secrets Backend.
If you also want to use SystemsManagerParameter for sharing Inter-tasks metadata - feel free to add and contribute custom XCom Backend using  https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html#custom-backends in the Amazon Provider. But that does not require a separate issue or agreement, just contribute it to the AWS provider.",2022/1/21 11:13,4,,(VII) Information Storage
https://github.com/apache/airflow/discussions/21012,2022/1/21 9:04,"BTW. I converted it to discussion. If you feel the answer you got is not enough and you want to have something else - feel free to elaborate here. If you think Custom Secret Backend, Custom XCom is not enough - please describe what you mean by

I am talking about using SSM parameter store as metadata storage to share configuration/metadata across different subsystems, as well as airflow. This means all actions including setting and getting parameters

Ideally showing examples of how you think the configuration and DAGs should be affected by the proposal and why exactly current customization options are not good enough (ideally a specific example where you show where the current customization options will break).",2022/1/21 11:17,5,1,(VII) Information Storage
https://github.com/apache/airflow/discussions/21012,2022/1/21 9:04,"Hello @potiuk,
Customers often use AWS Systems Manager Parameter Store to retain parameters that are not necessarily required only for Airflow DAG configurations.
We have use cases where we create a baseline configuration for VMs (such as required packages, OS type, vCPU and memory configurations) in an Airflow DAG and then store this baseline VM configuration information in AWS parameter store to be retrieved by our centrally managed deployment system (AWS CloudFormation based).
This allows VM config info to be available to users/entities that do not have access to our Airflow UI.
While we do not directly use AWS Systems Manager Parameter Store to ""configure"" our DAG runs, it is being used to communicate/share DAG outputs to other entities within our larger system. I believe this does not break the ""Cloud Agnostic"" tenet. Rather, it provides another way for Airflow to interact/talk to Cloud providers (in this case AWS) via operators, hooks, sensors, etc.
Would this serve as a viable use case to adding this feature into the AWS Provider package?",2022/1/23 15:05,6,,(VII) Information Storage
https://github.com/apache/airflow/discussions/21039,2022/1/14 7:39,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/14 7:39,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21039,2022/1/14 7:39,Not sure what you try to do and the problem you have. I will convert into discussion or maybe you can open a PR showing what you want to dol,2022/1/22 23:14,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21047,2022/1/19 9:12,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/19 9:12,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21047,2022/1/19 9:12,"I do not think there is a clear reproduction path, and this is not a problem that we woudl be aware of. Maybe someone will be able to help with that - but maybe try to provide more information and try to see if there are no more logs/errors in your kubernetes logs? Check for any ""resources"" (memory) related problems and similar. I heartily recommend using k9s tool for that - it is really helpful to anylyse logs and see problems with your pods/configuration/resources..
If this is something fully reproducible, please specify how exactly you run your backfill - including the commands you issue and what the backfill command will print you. Ideally in the way that will be possible by anyone to reproduce it.",2022/1/23 17:26,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21047,2022/1/19 9:12,Converting it into discussion until more information is available,2022/1/23 17:26,3,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21047,2022/1/19 9:12,"Thank you for your kind reply.
The more information attached as below.
Start backfilling job with command airflow dags backfill echo -s 20220110 -e 20220110 and the logs:

scheduler logs:

scheduler pod config

backfill task pod config

Please let me know if anything is unclear.
Thank you again for you help.",2022/1/24 3:27,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21047,2022/1/19 9:12,"Really helpful. I think this is duplicate of #20982
",2022/1/24 13:26,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21047,2022/1/19 9:12,"I have met the same issue with Airflow 2.2.4. I was running the airflow backfill cmd airflow dags backfill test_dag_2 -s 2022-02-25 -e 2022-03-01 -v from the schduler pod in our k8s cluster. And it finished the first dagrun2022-02-25 and stuck in the second dagrun whose task pods actually haven been completed in the k8s cluster but still showing 'scheduled' in the airflow GUI and the backfill was stuck from this point.

The logs:

I tried to run the backfill command using subprocesshook in a dag and it has no this issue but after a few successful backfill dagruns, some tasks got stuck forever to 'scheduled' state and not run anymore. I created an issue about this in #23145",2022/4/21 12:49,6,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21047,2022/1/19 9:12,this issue seems to be similar to the issues mentioned in the PR: #23720,2022/5/17 21:14,7,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21049,2022/1/20 13:09,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/20 13:09,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21049,2022/1/20 13:09,"Converted it into discussion. There is no information on reproduction, I am not even sure what your problem is. If you want to continue discussing it with others, then I suggest you to add more details and explain precisely what your problem is. Look at the logs, try to find similar issues first and possibly even discuss it in slack in #troubleshooting but you need to provde more information - maybe someone will be able to help you. Specifically you need to provide stack traces and logs generated by the crash loopbacking pods.
Also I'd suggest you to use the official Helm Chart to install Airflow, rather than try with your own resources https://airflow.apache.org/docs/helm-chart/stable/index.html  - You will have better starting point and be sure that you are not fighting with your own deployment issues.",2022/1/23 20:13,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21049,2022/1/20 13:09,"Yes. I tried a lot of time to resolve but not achieved any success in this. Now, I am trying to create custom configuration in airflow application but somehow few of variables are not able to enable.",2022/1/25 11:59,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21050,2022/1/20 18:31,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/20 18:32,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21050,2022/1/20 18:31,I believe your installation has gone somewhat wrong - almost looks like you have mulltple versions of airflow installed at the same time. I suggest you reinstall you Airlfow installation from scratch (alsol you have not really posted how you installed airflow). Converting it into discussion.,2022/1/23 20:41,2,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/21050,2022/1/20 18:31,"@neonimp what task runner do you use?
the screenshot is based on the 2.2.3 version and i also confirmed that in the main branch, it is same.

you can find it in your airflow.cfg, task_runner = 
@potiuk it looks like it is a bug, the def return_code interface is different in those classes.",2022/5/31 21:58,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21130,2022/1/26 5:08,"I converted it into discussion. The execution_date is already deprecated. The logical_date have been proposed and voted on devlist:
Here: https://lists.apache.org/thread/16t6mo5r8oj9gyo5zt5rg0pphgjdzd0f
This is such an important decision that it must be discussed on devlist and require voting IMHO. Feel free to either continue the discusion thread I linked to or start a new thread on the devlist, but I see no way any discussion here might help
Apache's ""What did not happen on the devlist, did not happen"" - is aplicable in this case",2022/1/26 16:35,1,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21130,2022/1/26 5:08,"
_",2022/1/26 17:29,2,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21212,2022/1/19 15:00,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/19 15:00,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21212,2022/1/19 15:00,"It's impossible to get any reprosucibility here. You likely have some duplication in your dag_ids - same dag_ids coming from multiple files. But it's hard to say what the problem is . Converting it into discussion. If you provide more information on your dag structure,  logical dags you have and dynamic dags generation schemes you have maybe we can somehow help",2022/1/29 19:59,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21254,2022/1/25 13:26,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/25 13:26,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21254,2022/1/25 13:26,I tihnk you need to explain a bit better what you mean. I read the description and looked at the screenshots few times and I do not understand what the problem is. I converted it into a discussion to clarify it.,2022/2/1 17:42,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21254,2022/1/25 13:26,"Sorry. I'll try to describe it better.
If TimeDeltaSensorAsync(delta=datetime.timedelta(hours=1), task_id='job1') starts at 8:00 1.2.2022 and finishes at 9:00 1.2.2022. It will have start_date and end_date equal to 9:00 1.2.2022. However, I would expect the start_date to be 8:00 1.2.2022 and end_date to be 9:00 1.2.2022. And this happens for all deferrable operators.
Does this make more sense?",2022/2/1 20:09,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21254,2022/1/25 13:26,"Actually my colleague found this and it seems it should be fixing the issue.
But tanks a lot :)",2022/2/2 17:07,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21278,2022/2/2 17:11,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/2/2 17:11,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21278,2022/2/2 17:11,"This is not an issue - this is a discussion. I believe it is pretty expectedw way how decorators work, but I wonder what others have to say on thtat one.",2022/2/2 20:39,2,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/21278,2022/2/2 17:11,"This isn't anything to do with the TaskFlow directly, it's how python scoping works with for-loops, coupled with now DAGs are parsed.

When parsing the DAG file (which happens up front) test_task(dag_id) will be called with 1..3 as the values
At this point, the value of the dag_id variable is left at test_dag_3
Then Airflow says ""I want to run dag X"", pulls the dag out of the module.

So when the task function is executed, the current/latest value of dag_id variable is the one you get.
The way to do this and get the behaviour you want is to use dag from the context:
",2022/2/3 11:35,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21368,2022/2/3 17:34,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/2/3 17:34,1,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21368,2022/2/3 17:34,"I turned it into discussion because this won't be handled as a ""feature"" I am afraid.
I think we will not apply such change to implement the specific case of yours? If you want to propose a bigger change in processing of Airflow DAG - discussion on devlist and AIP (maybe even follow-up on https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-5+Remote+DAG+Fetcher)
How about using modified time? https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#file-parsing-sort-mode ? Did it work for you? I think you can also follow https://airflow.apache.org/docs/apache-airflow/stable/concepts/scheduler.html?highlight=fine%20tuning#fine-tuning-your-scheduler-performance - to see if you can improve the perfomance of scheduler.",2022/2/6 20:03,2,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21368,2022/2/3 17:34,"Update on this, I went with @potiuk suggestion of implementing a local dag cache using dill and it's working nicely
We now get the benefit that dags deleted upstream are also deleted in Airflow, and no performance downsides
In case it's of use, here's an idea of the code we're running:

And then in the loop where I get dags from an API we do something like this:
",2022/2/25 11:16,3,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21369,2022/2/2 9:13,__  seeing this as well,2022/2/4 15:01,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21369,2022/2/2 9:13,"We are seeing similar issues after an upgrade from 2.1.4 to 2.2.3.  On our scheduler, this has been somewhat mitigated as we increased AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL and AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL both to 10 minutes from 5 seconds.
However I am still seeing it in our Celery Worker and it effectively blocks tasks from running for 80+ seconds, before it times out.
I suspect our error is related to Initializing Providers Manager[import_all_hooks] which is something new with 2.2.X and it largely seems to always fail with Exception when importing 'airflow.providers.docker.hooks.docker.DockerHook' from 'apache-airflow-providers-docker' package.  Our log is below.
I have downgraded to 2.1.4 to see if that resolves our issue.
@dcardinha @WattsInABox , do either of you have the option to turn on DEBUG logging, possibly to see something similar?
",2022/2/4 18:38,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21369,2022/2/2 9:13,"The problem is that your dag parsing takes too much time. You apparently missed this advice from the log:
Please take a look at these docs to improve your DAG import time:
2022-02-04T08:16:06.683052855Z * https://airflow.apache.org/docs/apache-airflow/2.2.3/best-practices.html#top-level-python-code
2022-02-04T08:16:06.683058750Z * https://airflow.apache.org/docs/apache-airflow/2.2.3/best-practices.html#reducing-dag-complexity
Can you please follow up ALL the best practices there and report your findings after addressing them?
Converting it into a discussion, awaiting the results of review of your DAGS (parsing of which takes far too much time).",2022/2/6 20:32,3,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/21384,2022/2/7 10:46,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/2/7 10:46,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21384,2022/2/7 10:46,"Until we know it's an issue (and have a reproduction path) - it should be a discussion. I converted it so. Maybe someone will be able to help you but it looks like a race condition somewhat generated by either your DAG or composer environment and your task apparently fails immediately for some reason (but what is the reason is hard to say).
Not likely to be an Airflow Issue. I suggest to add more debugging to the task that fails and try to figure out why and if you cannot - raise an issue with the composer support.
But maybe other people who had similar issue will be able to say - I am just guessing as the information you gave is not nearly enough to reason and reproduce the error.",2022/2/7 10:56,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21426,2022/2/8 15:00,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/2/8 15:00,1,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21426,2022/2/8 15:00,"PLease avoid creating github issue if you have trouble with making your custom things to work - Github Issues is not to help you debug your problems. Use discussions for this where you might get help from others.
But in order to do that, you need to share logs and errors and tell what you tried to do to investigate the issue. Then you MIGHT get help if you show that you tried. There is no chance somoene here will debug your problems here for you before you've shown you made your effort and give enough information that might give others a chance to help you.",2022/2/8 15:06,2,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21426,2022/2/8 15:00,Solved my issue by adding volumes in airflow-init section in docker-compose.,2022/2/10 8:15,3,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21474,2022/2/9 17:10,"I turned it into a discussion because I guess you have not added the Custom Timetable via plugin interface: https://airflow.apache.org/docs/apache-airflow/stable/plugins.html?highlight=plugins
You must add Custom Timetable through plugin (and register it via timetables)",2022/2/9 17:53,1,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/21492,2022/2/10 6:33,"Please in the future do not open Issues in case you are not 100% sure the issue is in airflow and not your deployment.
Open https://github.com/apache/airflow/discussions/ instead. When  you attempted to open this issues you were guided to do so, please read the guidance and follow.
This is not reproducible. It works as expected with default airlfow configuration.
I think your changes were not propagated to the right worker or it has not been restarted or you have your own custom logging configuration that overrides the airflow one (in which case you should fix the configuration of yours). If you have problem with tracking it down start with default airflow configuration https://airflow.apache.org/docs/apache-airflow/stable/start/index.html and observe that it works as expected, then you can work out which of your customizations (if you have them) triggers it by modifying the default configuration.",2022/2/10 12:24,1,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21492,2022/2/10 6:33,"I am actually experiencing the same issue:

give the following output:
",2022/3/30 10:23,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,"Not sure if this is related, but I am also seeing an ""Import error"" every now and then, apparently appearing out of thin air

This disappears when I refresh the page",2022/2/9 20:28,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,related: #20648,2022/2/15 7:26,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,Some logs should be present - can you also check if you have enough resources / kubernetes logs ? It is lilkely your tasks are killed due to lack of resources (memory most likely).,2022/2/16 21:38,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,Converting to discussion until more information provided.,2022/2/16 21:40,4,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,"I am also seeing an increase in the scheduler memory usage over time
",2022/2/17 13:09,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,"Which version of Airlfow do you use @zorzigio ?
From your observation about  memory I assume it is before 2.1.4?
Is it ""active"" or ""cache"" memory ? If it's cache, this is harmless and expected (In 2.1.4 #18054 we added kerne advisory to not cache log file memory). The cache growth was from log writing, but cache memory growth is entirely harmless and expected in earlier versions - the memory will be freed as needed and it's normal behaviour of Unix.
Before we dive deeper I strongly advise (if my guess is right) to upgrade to latest release of Airlfow. There were already a number of fixes since then so this is very likely.
See this thread: #21499 (reply in thread) if you want to see testimony of others who did.",2022/2/17 14:50,6,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,"@potiuk, thanks for looking into this.
I am using the latest version of Airflow (which of today is 2.2.3)
Here is the output from  airflow info
",2022/2/17 15:46,7,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,"I think the issue you are having is about DAG serialization. You can try clearing the serialized DAGs. There's an airflow db reserialize command but it's not released yet. Try this:

To have airflow reserialize the dags again
When there is an issue running a task 
 due to serialization, airflow may not have logs at that time. Here's where it's handled by the scheduler: https://github.com/apache/airflow/blob/5a38d15e539ca3bb16a86d14a6da2939f1ebb257/airflow/jobs/scheduler_job.py#L613-L63",2022/2/17 20:01,8,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,This issue seems similar to #21082,2022/2/18 9:04,9,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21624,2022/2/9 12:36,Any update on this?,2022/12/30 13:32,10,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21633,2022/1/13 10:38,"Hi,
are you aware the the timestamps displayed show the start of the current interval?
E.g. if it shows now (on 2021-01-17) that the next dag run is 2022-01-13 09:40:00+00:00, it means your dag will be executed at the end of the interval which is at 2022-01-20 09:40:00+00:00.
In the list of runs it will also display the scheduled runs only by the start of the interval and not the actual time of the dag execution. This can cause confusion, especially when you also run your dag manually. Check out the logs of the runs to see when your dag ran.
Maybe this helps",2022/1/17 9:04,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21633,2022/1/13 10:38,"
This can cause confusion, especially when you also run your dag manually. Check out the logs of the runs to see when your dag ran.

Manual dag runs interfere with the schedule of scheduled dag-runs?",2022/1/17 13:05,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21633,2022/1/13 10:38,"
Manual dag runs interfere with the schedule of scheduled dag-runs?

No, it does not interfere with the runs itself, but with the order they are displayed on the airflow-webserver when you click on the dag. The displayed time is always the start of the interval (not the end at which the dag is executed).
Example: Assuming you have a dag that runs daily at 8am (0 8 * * *) and you execute the same dag manually at 7.45am. If you check the dag at 9am today (2022-01-17), one assumes when looking at the tree that you see execution times at 2022-01-16 08:00 (scheduled), 2022-01-17 07:45 (manually), and 2022-01-17 08:00 (scheduled). However, that is not the case, even though the dag was exactly at those times executed. The displayed times of those three executions are:  2022-01-15 08:00 (scheduled), 2022-01-16 08:00 (scheduled), and 2022-01-17 07:45 (manually), because the intervals started at these times.
Ergo, when you check if your dag was successfully executed at 22-01-13 09:40:00+00:00, you have to look for the timestamp 2022-01-06 09:40:00+00:00.
When the next dag run states 22-01-13 09:40:00+00:00, the dag will be executed at the end of the 7-day interval that started at the given timestamp.
It is also possible that there is another  bug or error. I had a similar issue than you and was looking for the reason only to find out that I interpreted the timestamps wrong",2022/1/17 13:41,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21633,2022/1/13 10:38,"I'm not able to reproduce.
Converting to discussion",2022/2/17 6:57,4,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21802,2022/2/21 9:44,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/2/21 9:44,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21802,2022/2/21 9:44,"Would it be possible to to wrap it with py-spy, so we can determine what's being slow?
e.g.
",2022/2/23 1:03,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21802,2022/2/21 9:44,"Python is inherently a bit slow to start up so don't expect any magic. Airflow though is a bit of an extra bad case though since it imports so many other modules.
If you use the Docker image it is even slower (assume you do since using K8), because

The entrypoint performs some airflow db check before starting any tasks.  Not sure why. This takes 5 seconds some times.
You loose the .pyc-caching since it starts a fresh container each time. I did some test long ago by pre-baking the .pyc files by simply ending the Dockerfile with RUN airflow --help and it shaved off almost a complete second on subsequent docker runs. Maybe i should upstream this fix to the official image?
",2022/2/24 17:02,3,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21802,2022/2/21 9:44,"
Python is inherently a bit slow to start up so don't expect any magic. Airflow though is a bit of an extra bad case though since it imports so many other modules.
If you use the Docker image it is even slower (assume you do since using K8), because

The entrypoint performs some airflow db check before starting any tasks.  Not sure why. This takes 5 seconds some times.


This is explained in the docs:
https://airflow.apache.org/docs/docker-stack/entrypoint.html#waits-for-airflow-db-connection

The entrypoint is waiting for a connection to the database independent of the database engine. This allows us to increase the stability of the environment.

Alongside the documentation how to disable this check:

But you gave me thought that we can only run it for specific commands - so if you run airflow commands as ""separate container"" commands, this might help a bit.


You loose the .pyc-caching since it starts a fresh container each time. I did some test long ago by pre-baking the .pyc files by simply ending the Dockerfile with RUN airflow --help and it shaved off almost a complete second on subsequent docker runs. Maybe i should upstream this fix to the official image?


This is deliberate decision and baking in .pyc files is bad idea as it increases the size of the image significantly (you are basically trading of the size of the image, network, and storage with first time start for some commands.
If you want to run airflow commands repetitively, rather than running new container every time, run a single comtainer and exec command in the running container.
@Wats0ns : Airlfow version SHOULD be fast (also in terms of .pyc) because it imports very little, so I also second @jedcunningham here - py-spy would be useful. I just checked my ""airflow version"" and I looked where the slowness might come from.
Almost for sure those are your local_settings or your log configuration. Parsing settings.py (and local settings) and establishing logging configuration is the one thing that happens in airflow version.
So please - run tha py-spy and let us know here by posting it (or mostl likely you will find where it comes from in your configuration).
I am converting this one into a discussion, until we hear more about the py-spy results as this is likely not an airflow issue/",2022/2/24 19:09,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21837,2022/2/16 21:16,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/2/16 21:16,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21837,2022/2/16 21:16,"I think you are mixing logs vs. remote logs here. When stackdriver is enabled, your task logs should be sent to stackdriver and what you are showing as ""Here's the StackDriver output (unfiltered) from executing a sample DAG"" is the file log of task instance execution in the log file, not stackdriver.  You likely will find the missing logs in ... stackdriver. Or maybe your explanation was not clear enough.
Just one comment: You need ot make sure that you configure your docker-compose properly (and we do not guarantee the docker compose we have is good for this kind of modifications. Our docker compose is ""quick-start"" only and you have to make sure you know what you are doing there if you want to productionize it and warn against it:
https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html

DO NOT expect the Docker Compose below will be enough to run production-ready Docker Compose Airflow installation using it. This is truly quick-start docker-compose for you to get Airflow up and running locally and get your hands dirty with Airflow. Configuring a Docker-Compose installation that is ready for production requires an intrinsic knowledge of Docker Compose, a lot of customization and possibly even writing the Docker Compose file that will suit your needs from the scratch. It__ probably OK if you want to run Docker Compose-based deployment, but short of becoming a Docker Compose expert, it__ highly unlikely you will get robust deployment with it.


If you want to get an easy to configure Docker-based deployment that Airflow Community develops, supports and can provide support with deployment, you should consider using Kubernetes and deploying Airflow using Official Airflow Community Helm Chart.

It is likely that you have misconfigured your docker compose. It's very easy to overlook something and make mistakes when you configure the docker compose, so that's why we do not recommend to use it and it will be hard for you to get any help here even if you show your full docker compose (As it is likely to be too complex to analyse and look for bugs and typos).
I stronly recommend to switch to K8S and Helm chart as there are ready-to-use recipes on how to reconfigure it and much easier to get help. https://airflow.apache.org/docs/helm-chart/stable/index.html
I converted it into discussion, as this is likely misconfiguration rather than Airlfow problem and needs further clarifications.",2022/2/26 18:47,2,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/21842,2022/2/20 19:29,Can you upload the scheduler logs when this happens,2022/2/20 19:31,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21842,2022/2/20 19:29,Converted it to discussion until more information is provided that might enable to asses if this is an Airflow issue or not.,2022/2/26 21:48,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21842,2022/2/20 19:29,"@ephraimbuddy Somehow I missed this.
Somehow this issue happen today, again. The tasks are scheduled in pods that belong to a Ec2 spot instance node.
The spot node goes down, and the tasks pods are marked as failed, without retrying. If would have retry, then the new task will be run a pod in a node that is up, and this issue won't happen.
Here the logs.
Date,Message
""2022-03-16T00:03:26.443Z"",""[2022-03-16 00:03:26,146] {scheduler_job.py:570} INFO - TaskInstance Finished: dag_id=continuous_load, task_id=load_table, run_id=scheduled__2022-03-15T23:45:00+00:00, run_start_date=2022-03-16 00:01:26.128956+00:00, run_end_date=None, run_duration=None, state=running, executor_state=failed, try_number=1, max_tries=1, job_id=403468, pool=default_pool, queue=default, priority_weight=1, operator=ExtractLoadOperator""
""2022-03-16T00:03:26.442Z"",""[2022-03-16 00:03:26,069] {kubernetes_executor.py:575} INFO - Changing state of (TaskInstanceKey(dag_id='continuous_load', task_id='load_table', run_id='scheduled__2022-03-15T23:45:00+00:00', try_number=1), , 'load_table.b45adf3ca7ef4f5482639bfedbc4c340', 'airflow-data-eng', '109859634') to failed""
""2022-03-16T00:03:26.442Z"",""[2022-03-16 00:03:26,064] {kubernetes_executor.py:374} INFO - Attempting to finish pod; pod_id: load_table.b45adf3ca7ef4f5482639bfedbc4c340; state: failed; annotations: {'dag_id': 'continuous_load', 'task_id': 'load_table', 'execution_date': None, 'run_id': 'scheduled__2022-03-15T23:45:00+00:00', 'try_number': '1'}""
""2022-03-16T00:00:21.204Z"",""[2022-03-16 00:00:20,563] {kubernetes_executor.py:297} INFO - Kubernetes job is (TaskInstanceKey(dag_id='continuous_load', task_id='load_table', run_id='scheduled__2022-03-15T23:45:00+00:00', try_number=1), ['airflow', 'tasks', 'run', 'continuous_load', 'load_table', 'scheduled__2022-03-15T23:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/continuous_load.py'], {'api_version': 'v1', 'kind': 'Pod', 'metadata': {'annotations': None, 'cluster_name': None, 'creation_timestamp': None, 'deletion_grace_period_seconds': None, 'deletion_timestamp': None, 'finalizers': None, 'generate_name': None, 'generation': None, 'initializers': None, 'labels': None, 'managed_fields': None, 'name': None, 'namespace': None, 'owner_references': None, 'resource_version': None, 'self_link': None, 'uid': None}, 'spec': {'active_deadline_seconds': None, 'affinity': None, 'automount_service_account_token': None, 'containers': [{'args': [], 'command': [], 'env': [], 'env_from': [], 'image': None, 'image_pull_policy': None, 'lifecycle': None, 'liveness_probe': None, 'name': 'base', 'ports': [], 'readiness_probe': None, 'resources': {'limits': {'cpu': '3000m', 'memory': '6G'}, 'requests': {'cpu': '1000m', 'memory': '2G'}}, 'security_context': None, 'stdin': None, 'stdin_once': None, 'termination_message_path': None, 'termination_message_policy': None, 'tty': None, 'volume_devices': None, 'volume_mounts': [], 'working_dir': None}], 'dns_config': None, 'dns_policy': None, 'enable_service_links': None, 'host_aliases': None, 'host_ipc': None, 'host_network': False, 'host_pid': None, 'hostname': None, 'image_pull_secrets': [], 'init_containers': None, 'node_name': None, 'node_selector': None, 'preemption_policy': None, 'priority': None, 'priority_class_name': None, 'readiness_gates': None, 'restart_policy': None, 'runtime_class_name': None, 'scheduler_name': None, 'security_context': None, 'service_account': None, 'service_account_name': None, 'share_process_namespace': None, 'subdomain': None, 'termination_grace_period_seconds': None, 'tolerations': None, 'volumes': []}, 'status': None}, None)""
""2022-03-16T00:00:01.934Z"",""[2022-03-16 00:00:00,968] {kubernetes_executor.py:530} INFO - Add task TaskInstanceKey(dag_id='continuous_load', task_id='load_table', run_id='scheduled__2022-03-15T23:45:00+00:00', try_number=1) with command ['airflow', 'tasks', 'run', 'continuous_load', 'load_table', 'scheduled__2022-03-15T23:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/continuous_load.py'] with executor_config {'KubernetesExecutor': {'request_memory': '2G', 'request_cpu': '1000m', 'limit_memory': '6G', 'limit_cpu': '3000m'}}""
""2022-03-16T00:00:01.933Z"",""[2022-03-16 00:00:00,953] {base_executor.py:82} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'continuous_load', 'load_table', 'scheduled__2022-03-15T23:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/continuous_load.py']""
""2022-03-16T00:00:01.933Z"",""[2022-03-16 00:00:00,952] {scheduler_job.py:473} INFO - Sending TaskInstanceKey(dag_id='continuous_load', task_id='load_table', run_id='scheduled__2022-03-15T23:45:00+00:00', try_number=1) to executor with priority 1 and queue default""
",2022/3/16 9:12,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21842,2022/2/20 19:29,"We are facing the issue with our configuration as well, seems like a bug when we lost a machine by spot instances our task fails without any retry. I will try to gather the logs to share here but I can say that seems the case in our side. We were expecting to the task retry the other two times, but that is not being the case.",2022/7/25 22:30,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"We also have this issue, but with the KubernetesExecutor. Airflow version is 2.1.4. Seems to happen whenever the scheduler is being restarted by the deployment.
Here is the stack trace, but it's likely a little jumbled as I had to pull it from DataDog and the lines don't always arrive in order there:
",2021/11/18 21:20,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"We are seeing it on apache/airflow:2.2.3-python3.8. Any advice on how to troubleshoot it?
",2022/1/6 15:14,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,Facing the similar issue.,2022/1/14 10:01,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"Having the same issue. Seeing the problem after we pulled the latest version of the image(apache/airflow:2.2.2-python3.9) which was pushed a month ago.
previous to that we were using the image with the same tag and it was fine. something has been introduced in the new image I think. we are using kubernetes executor btw.",2022/1/14 13:55,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,We are going to switch to latest Celery 5.2.3 in 2.2.4 - can you please take a look (it solves a number of stabilty problems in Celery). RC should be out some time next week most likely.,2022/1/22 23:44,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"Same issue here after upgrading to apache/airflow:2.2.3. We are also using Kubernetes Executor.
Any idea of what is causing the problem?",2022/2/16 22:20,6,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"Facing same issue
helm chart 1.3.0
image version apache/airflow:2.2.2",2022/2/17 0:27,7,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"@nanaones @rmakoto-ze @hamedhsn @rtudo
Can you please provide the exact steps to reproduce the problem you have? What exactly you do, what logs and behaviours you see? I would like to take a close look at that but I need a little bit more information to start looking at it.",2022/2/17 12:10,8,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"This might be some ""side-effect"" of fixing MySQL changing the key a month ago (this is when we pushed the new images) and maybe the reason is some change that sneaked in by mistake (which we might be able to fix).",2022/2/17 12:11,9,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,I dont know if that is related but I am seeing this behavior only on the airflow that runs on Postgres in kubernetes. we have other instances of airflow that use RDS as backend and they are fine.,2022/2/17 16:25,10,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"some logs from the scheduler below. I dont see much more info anywhere.
The setup is simple, we are using kubernetes executor with image apache/airflow:2.2.2-python3.9
",2022/2/17 16:28,11,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"@hamedhsn
I've ""fixed"" it by increasing the liveliness health check timeout as it seems that it was taking more than 20 seconds and this is what caused it to be killed and hence the logs what you see about ""Sending Signals.SIGTERM to GPID 55""
Relevant content (I think): #20698 & #19001
and the fix is

May want to bump it even higher if scheduler keeps restarting.",2022/2/17 17:09,12,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"
@nanaones @rmakoto-ze @hamedhsn @rtudo
Can you please provide the exact steps to reproduce the problem you have? What exactly you do, what logs and behaviours you see? I would like to take a close look at that but I need a little bit more information to start looking at it.

@potiuk
ENV:
Airflow on EKS  1.21
Helm 1.3.0
Airflow 2.2.3
This is the screen seen from the Lens. It's constantly restarting.
I have no problems using it now, but I think it could be a problem.

Airflow scheduler keeps restarting with the following error:
",2022/2/22 7:36,13,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21844,2021/10/14 16:28,"This is the same as you already provided.  I think you need too look at the POD and Kubernetes logs as you likely have some resource problems (not enough memory ?) triggering it. If things are continuously restarting, this is the most likely case.
I am converting this into discussion untils some more details are provided.",2022/2/26 23:19,14,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21846,2022/1/20 17:21,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/20 17:21,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21846,2022/1/20 17:21,Could you provide a simple DAG to reproduce this behaviour? I have tried setting tags=None but I couldn't get the error,2022/2/24 16:58,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21846,2022/1/20 17:21,Converting it to a discussion until more info is provided allowing to reproduce it.,2022/2/27 0:56,3,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21846,2022/1/20 17:21,"This happened to our 2.2.2 deployment. We've had some DAGs with tags=[...] and when we completely removed that line, those DAGs caused this error at the scheduler.
The issue is that for some reason this did not result in import error that was visible anywhere, we had to be alerted by a partner that data was no longer being processed.
Another quick fix is to manually remove those entries from dag_tag in the matadb.",2022/3/15 14:04,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21846,2022/1/20 17:21,Created an issue #22289 - thanks for providing reproducible steps.,2022/3/15 14:47,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/21854,2022/2/25 5:24,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/2/25 5:24,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21854,2022/2/25 5:24,"Converted it into discussion. I think you simply should make a. non-self-signed certificate for your webhook. The error is - I believe - not raised by Airlfow but by Slack trying to connect to your server (but I am guessing as you only provided partial stacktrace and it's not clear where it comes from).
My recommendation is to use https://letsencrypt.org/ to get the proper non-self-signed certificate so that Slack can verify it.",2022/2/27 19:08,2,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/21854,2022/2/25 5:24,"I encountered the same issue today with airflow 2.2.3. I tried adding custom CA to ubuntu trust store and adding REQUESTS_CA_BUNDLE=/path/to/your/certificate.pem environment variable but still the error 'ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1131)'
apache-airflow                  2.2.3
apache-airflow-providers-celery 2.1.0
apache-airflow-providers-ftp    2.0.1
apache-airflow-providers-http   2.0.1
apache-airflow-providers-imap   2.0.1
apache-airflow-providers-sqlite 2.0.1
Flask                           1.1.2
Flask-AppBuilder                3.4.1
Flask-Babel                     2.0.0
Flask-Caching                   1.10.1
Flask-JWT-Extended              3.25.1
Flask-Login                     0.4.1
Flask-OpenID                    1.3.0
Flask-SQLAlchemy                2.5.1
Flask-WTF                       0.14.3",2022/3/1 3:42,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21854,2022/2/25 5:24,"I get a similar error only when using the new @task decorator.

(Caused by SSLError(SSLError(""bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])"")))
[2022-03-15 20:59:52,292] {{local_task_job.py:264}} INFO - 0 downstream tasks scheduled from follow-on schedule check

We have legacy DAGs which are running fine, but the one DAG using the decorator silent errors (no error is sent to slack when calling the on_failure_callback).
Is there some conflict going on here?
In my airflow_slack_failed_task_webhook, the failing line is the urllib3 line as follows:
urllib3.PoolManager().request('POST', 'https://hooks.slack.com/services/' + token , body=slack_msg, headers={'Content-Type': 'application/json'})
Do I need to manually specify the proxy parameters? I have tried manually specifying the certifi certificate location and updating that to the latest available version.
Example below:
Silent error:
`@task(task_id=f'run_schedule_plan_{item_name}', on_failure_callback=airflow_slack_failed_task_webhook)
def run_scheduled_plan(schedule_item):
dashboard_id = schedule_item.get('dashboard_id')
item_name = schedule_item.get('name')

and what runs fine as usual:
    t_update_tables = PythonOperator(task_id             = 'update_tables', python_callable     = update_tables, op_kwargs           = {'tables'         : tables, 'our_database' : our_database, 'etl_database'   : etl_database, 'warehouse'      : warehouse}, provide_context     = True, dag                 = dag, on_failure_callback = airflow_slack_failed_task_webhook)",2022/3/16 20:19,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21854,2022/2/25 5:24,"We have tried different options for SSL but nothing seems worked. We are continuing to work on different options. I don't have a way to install self certificate as it is a managed service from AWS.
As a work around for this problem, I used the below code to suppress SSL verification and it worked.
requests.post('https://slack.com/api/chat.postMessage', {
'token': slack_token,
'channel': slack_channel,
'text': text,
'icon_emoji': slack_icon_emoji,
'username': ""gopi"",
'blocks': json.dumps(blocks) if blocks else None
},verify=False).json()
@potiuk  I am trying to suppress the SSL using SlackWebHookOperator but it is not working.  I tried the below code.
SlackWebhookOperator(
task_id='slack_webhook',
http_conn_id=SLACK_CONN_ID,
webhook_token=SLACK_WEBHOOK_TOKEN,
message=slack_msg,
username='airflow',
extra_options={'verify': False}
)
Any thoughts ?",2022/3/18 19:15,5,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21854,2022/2/25 5:24,"@Kannedhara
I managed to fix this by changing what we currently had (which SSL errored):
 response = (urllib3.PoolManager() .request('POST', 'https://hooks.slack.com/services/' + token, body=slack_msg, headers={'Content-Type': 'application/json'}))
to (which now works)
 response = (urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())  .request('POST', 'https://hooks.slack.com/services/' + token, body=slack_msg, headers={'Content-Type': 'application/json'}))
I'm not sure why, but I think at a certain point in time the requests package demanded this explicit certification stuff.",2022/3/18 19:23,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/21856,2022/2/25 18:02,This is at most a discussion no issue. @rdeteix  - please avoid creating issues when you are explicitly warned during Issue creation that you should open discussion in such cases.,2022/2/27 23:46,1,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21857,2022/2/25 15:38,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/2/25 15:38,1,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21857,2022/2/25 15:38,This can get especially misleading in the Task Instance details page where there priority_weight is listed twice (potentially with different values if there are downstream tasks),2022/2/25 18:29,2,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/21857,2022/2/25 15:38,"The priority_weight_total includes downstream or upstream tasks prioroties or both -depending on the weight rule. This is very clear to me. TaskInstance when executing is considered to have priority based on total weight (including upstram or downstram or both priorities).
This is natural consequences that Task is a definition (static) and TaskInstance is ""runtime"" (i.e. it contains acual status used for scheduling). I see completely no problem with it. Not sure what the proposal is here.
@michaelmicheal  - if you have a concrete proposal please open a PR and explain what you want to improve here.
I am also converting this into discussion.",2022/2/27 23:53,3,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/22028,2021/11/22 18:08,Maybe you would like to take a stab on fixing both in a PR?,2021/12/4 16:23,1,,(V) Already Fixed
https://github.com/apache/airflow/discussions/22028,2021/11/22 18:08,"@potiuk I'll give it a try, though I am also wondering about other parts of the SparkSubmitHook code, e.g. this part in def on_kill(self)


renew_from_kt will fail if there is no ""airflow kerberos"" ticket renewer or if the user hasn't somehow initialized a credentials cache at the ccache path from the config
Since this is using that one ccache from config, what happens if two or more SparkSubmitOperators with different keytabs timeout/get killed at the exact same time. Would the following be possible or would the scheduler not run the two kill commands in parallel?


SparkSubmitOperator A with keytab/principal for dev_user and SparkSubmitOperator B with keytab/principal for ops_user are killed/timeout at the same time
renew_from_kt for A is called
renew_from_kt for B is called (at the same time or shortly after 2. but before 4.)
subprocess with kill_cmd for A is opened (fails cause ccache contains ticket for ops_user, who is not allowed to modify YARN jobs of dev_user)

Whats the design decision here to use renew_from_kt instead of creating/using a temporary ccache location for each YARN kill?",2021/12/6 18:43,2,,(V) Already Fixed
https://github.com/apache/airflow/discussions/22028,2021/11/22 18:08,"
Whats the design decision here to use renew_from_kt instead of creating/using a temporary ccache location for each YARN kill?

No idea. I've not been here when the decision was made :)",2021/12/6 19:09,3,,(V) Already Fixed
https://github.com/apache/airflow/discussions/22028,2021/11/22 18:08,@cb149 you can check the PR that added the functionality #9044,2022/2/19 9:11,4,,(V) Already Fixed
https://github.com/apache/airflow/discussions/22028,2021/11/22 18:08,Converted it into discussion in case more discussion is needed.,2022/3/6 22:06,5,1,(V) Already Fixed
https://github.com/apache/airflow/discussions/22166,2022/3/10 19:45,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/3/10 19:45,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/22166,2022/3/10 19:45,"I think you are confusing constraints with requirements.
constraints doesn't install anything.",2022/3/10 19:53,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/22166,2022/3/10 19:45,"@eladkal  Understood but I can't find a minimal requirements file either. Constraints because I wanted the right versions of the libraries in place and referenced in install. I know req or constraints - the install will blow up if I am referencing azure or some of the apache providers. Would you know if there is a req file which would get me a working version of Airflow 2.2.3 with just Dummy, Bash and Python operators? I can then validate those libraries are available on my artifactory before running the install",2022/3/10 20:04,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/22166,2022/3/10 19:45,"This is a discussion, not issue.
@sandeepmohan . There is no ""requirements.txt"" in  Airflow (and will never be). It is not needed, because we have setup.py where are all the dependencies are specified. You should follow setup.py semantics not requirements.txt  - those are mutually exclusive (and actually you can easily get requirements.txt by using our constraints - see below).
When you install airflow you install main airflow and specify which ""optional"" extras you want.
So you can install core apache airflow like that (and THIS is your minimal installation):

If you want to get requirements.txt from that you can create an empty virtualenv, install airflow the way described above and run  pip freeze and you will get your requirements.txt - exactly the same way as for any other package out there. This is standard pip and packages behaviour.
Also you can add optional features you might need. For example you can add google provider and async capabilities of airflow like that:

And again you can pip freeze if you want fixed set of requirements for this combination. This is where setup.py approach is superior vs. requirements.txt because you can generate such requirements.txt for any combination of:

Python version
Airflow
Any set of extras you need.

Detailed list of of all ""extras"" you can use this way is here: https://airflow.apache.org/docs/apache-airflow/stable/extra-packages-ref.html
Without any extra, you only install core airflow and all the necessary dependencies. You DO NOT blow the installation in any way by installing unnecessary stuff you install everything that is necessary. And it is much better than ""requiremetnts.txt"" for example because the installed package sets might be different for different python version (and setup.py takes care about it as well as about the extras).
And to add to what Elad wrote - the constraints file is just list of ""golden versions"" of dependencies that we know work well with given version of airlfow. And this is the only ""valid"" way of installing airflow. It will pull the right version of dependencies. As @eladkal mentioned - you are NOT installing everything that is specified in the constraints - you are limiting the packages that you are installing to the version specified there.",2022/3/10 20:21,4,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/10/13 15:47,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,SGTM. Let's talk about it in PR/PRs.,2021/10/13 16:09,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"PS. If you want to pull-request, you don't have to create an issue first, but if you want, you can do it. Creating an issue will allow you to collect feedback or share plans with other people.",2021/10/13 16:10,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"
Creating an issue will allow you to collect feedback or share plans with other people.

That was my thinking. For example, this may be too advanced of an example for the basic tutorial anyway, as it brings in database connections, etc. It might be simpler to just work with files directly to start.",2021/10/13 16:11,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"
That was my thinking. For example, this may be too advanced of an example for the basic tutorial anyway, as it brings in database connections, etc. It might be simpler to just work with files directly to start.

I think this will be difficult to be discussed as a 'whole' here - if you - as new contributor - find that the examples are confusing for you (a little) but you seem to know how to fix them (you seem to), the best approach is to create small PRs fixing the problems one -by one. Start small and just fix one thing at a time. Then the potential dicussion will be much more focused (we can and we usually do discuss directly in PRs), and everyone will loose much less time on it.",2021/10/13 17:08,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"Feel free to create PRs for individual items here @drobert  - I also marked it as ""good first issue"" for others to contribute. I also converted your bullets into task lists so that you can tick-off any of the changes you've made already and close the issue when all is either done or addressed by discussion.",2021/10/13 17:11,6,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"Also I think (side comment) - since this is pretty natural that some code might be a little out-dated or buggy, I think changing the title to ""Basic tutorial examples could be improved"". reflects a bit better the actual state of it. A bit ""less aggressive"" and ""more empathetic"" for all those 1700+ contributors who created the software and docs that you can use for free :D.",2021/10/13 17:15,7,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,I would like to work on this issue.,2021/10/13 18:27,8,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,Feel free,2021/10/13 19:15,9,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"
I think changing the title to ""Basic tutorial examples could be improved"". reflects a bit better the actual state of it

@potiuk I did not intend to make this sound aggressive, but I will admit I don't think ""might be a little out-dated or buggy"" is accurate either. It is not possible this code or this data set was ever attempted, and it is admittedly a very frustrating experience for a new developer to fail at the given ""hello world"" example.
There is no disrespect intended at all on the Airflow product or the countless hours dedicated engineers have given to its release. But in the context of the getting started tutorial, I felt this title was accurate without being accusatory. (It's not ""horrible documentation"" or ""clearly the doc writers are lazy"", but the tutorial is significantly full of bugs at just about every step.)
I could see ""Basic tutorial examples do not function"", but ""improvement"" does not seem accurate. I was attempting to write this from the context of someone looking for whether there are open issues with the documentation (as I was).",2021/10/13 20:52,10,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,I thin the importnt thing is to make it better. Which i heartily invite you to help with,2021/10/14 7:39,11,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"I got an error:
Traceback (most recent call last):
File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1334, in _run_raw_task
self._execute_task_with_callbacks(context)
File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1460, in _execute_task_with_callbacks
result = self._execute_task(context, self.task)
File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1516, in _execute_task
result = execute_callable(context=context)
File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py"", line 134, in execute
return_value = super().execute(context)
File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py"", line 174, in execute
return_value = self.execute_callable()
File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py"", line 188, in execute_callable
return self.python_callable(*self.op_args, **self.op_kwargs)
File ""/opt/airflow/dags/first_pipeline.py"", line 25, in get_data
with open(data_path, ""w"") as file:
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/dags/files/employees.csv'
So I solve it by creating the path but it is not mentioned in doc.",2022/3/11 21:09,12,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"
So I solve it by creating the path but it is not mentioned in doc.

Can you please make a PR correcting it? It's as easy as clicking ""Suggest a change on this page"" and correcting it.  You can then become one of ~2000 contributors and give back for the free software you use.",2022/3/11 21:41,13,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,@likecodingloveproblems ^^,2022/3/11 21:41,14,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22233,2021/10/13 15:47,"Converting it to discussion, as there is no clear ""criteria"" when this issue is fixed.",2022/3/14 1:32,15,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22283,2022/3/15 7:04,"I don__ see how it__ unexpected, running a task instance requires some information on the DAG run containing it, and not locking the run would lead to potential inconsistencies.",2022/3/15 8:10,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22283,2022/3/15 7:04,"This is at most a discussion.
Can you please @ghostbody explain what is your reasoning, why do you think it's wrong to have a lock (including explanation on how two threads could get into the situation you described.
Unless I understand it wrongly (you have not explained it) you can get into lock timeout when some custom query of yours keeps the lock - yeah that can happen in any database where locks are used. And you are not supposed to manually run the sessions and queries you wrote (database is an internal thing in Airflow and you should not manually run any queries).
If you find how Airflow itself (without your custom code accessing the DB) can get into the situation wher those locks are problematic - by all means please try to find it and describe the reproducible scenario (but not the scenario where you run custom code, but scenario that causes Airflow to experience that issue without some custom queries from your side).",2022/3/15 12:10,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22283,2022/3/15 7:04,"We tried and it's really hard to reproduce in simple sinario. You need a lot of dags and big concurrucy.
We got a lot of lock wait timeout with about 1000 dag runs which is exactly what I describe with innodb_lock_wait_timeout = 30.
When I do ti: Optional[TaskInstance] = with_row_locks(qry, of=cls).first(), this problem is fixed.
The DagRun needs no locks here.
",2022/3/21 2:02,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22308,2022/3/16 11:28,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/3/16 11:28,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22308,2022/3/16 11:28,"You need to provide logs (and possibly analyse) of the webserver and likely kubernetes from around the time, the webserver is killed.  Ideally you should try to analyze it before and see if you can identify the reason yourself. Also you should see how the log files differ from ""normal"" situation.
There is no way we can act on it without seeing this information. Converting it into discussion until more information is available.",2022/3/16 11:59,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22308,2022/3/16 11:28,@potiuk  I compared the logs and we did not see any unusual in the logs. The only thing we noticed was high cpu spike. However I will try to provide you logs the next time we see this issue.,2022/3/16 12:06,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22308,2022/3/16 11:28,"Hi @potiuk - We faced this issue again on our production environment. I have attached the logs for webserver, cpu, memory metrics of webserver pod. During the issue I saw spike in both cpu and memory metrics.
Also, it took almost 2 min for health check to give a response

Logs-
airflow-webserver.log
CPU metrics for past 1 week

CPU at the time of issue

Memory metrics at the time of issue

CPU and memory request and limits for webserver pod on prod

Please let me know in case you need any other information.
Thanks,
Hussain",2022/3/28 8:59,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22309,2022/3/13 18:58,"Converted to discussion.
Those are just informational messages and they do not seem alarming.
It looks like it is pretty ""normal"" to happen occassionally and the log indicates that scheduler attempted to schedule a task (mistakenly) for which not all dependencies are met (this might be due to various race conditions) - but it correctly refused to start it in this case and apparenlty correctly run it at next scheduling loop when dependencies were already met.
If you have no problems resulting from that, then I'd say there is nothing to worry about.",2022/3/16 12:14,1,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/22361,2022/3/18 11:15,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/3/18 11:15,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/22361,2022/3/18 11:15,"If you want to render Native Python object then you need to follow instructions on the docs and use render_template_as_native_obj=True:
https://airflow.apache.org/docs/apache-airflow/stable/concepts/operators.html#rendering-fields-as-native-python-objects
converting issue for github discussion as for the moment this doesn't appear to be a bug",2022/3/18 11:50,2,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/22361,2022/3/18 11:15,I am experiencing the exact same issue with the DataprocCreateBatchOperator. Have you found a solution to your problem @MaksYermak ?,2022/7/28 14:29,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/22376,2022/3/19 12:57,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/3/19 12:57,1,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/22376,2022/3/19 12:57,Please DO NOT open issue when you want to ask questions. You were very clearly guided you should open discussion instead.,2022/3/19 13:04,2,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/22407,2021/8/15 23:03,"The only thing remotely related to errors I found:
",2021/8/15 23:14,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22407,2021/8/15 23:03,"Seems like its wrong resource_version processing
https://v1-20.docs.kubernetes.io/docs/reference/using-api/api-concepts/#410-gone-responses
https://v1-20.docs.kubernetes.io/docs/reference/using-api/api-concepts/#unavailable-resource-versions",2021/8/16 17:15,2,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22407,2021/8/15 23:03,"I redeployed configmap, used with the git-sync container, and this problem above (Kubernetes resource version is too old) is gone.
But the problem with 1.20 Kubernetes remains.
Airflow successfully creates Executor Pods, but it can't create any Kubernetes Pod Operators with no specific errors logged.",2021/8/18 2:05,3,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22407,2021/8/15 23:03,"Same situation with airflow 2.2.4 deployed from helm chart 1.5.0
openshift 3.11
kubernetes v.1.11.0
upd
Same result for k8s 1.21 and airflow 2.1.4 , no error message
Have no idea where I go wrong
upd2
worked on 1.21 with in_cluster=True",2022/3/19 22:24,4,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22407,2021/8/15 23:03,I think without further logs (including k8s logs) it's next to impossible to diagnose it.,2022/3/21 17:40,5,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22407,2021/8/15 23:03,I converted it into discussion - maybe someone above will be able to provide more information/logs/,2022/3/21 17:41,6,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22407,2021/8/15 23:03,"we had the same issue, after pinning

KubernetesExecutor can spawn KubernetesPodOperator pods immediately
Our setup:
",2022/4/5 17:39,7,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22407,2021/8/15 23:03,"We've faced with the same issue
Config:
Kubernetes 1.21
Airflow 2.2.5
apache-airflow-providers-cncf-kubernetes==2.1.0
kubernetes==11.0.0
Also thre're no logs or smth to post here. I've tried different versions of py packages (apache-airflow-providers-cncf.kubernetes,
kubernetes) but no luck",2022/5/18 7:44,8,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22553,2022/2/25 7:14,Without full stack trace it is impossible to determine the root cause. Please submit full stack trace information (can be a link to gist) when it happens again. Converting it to a discussion until we know more.,2022/3/27 15:42,1,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"I can't reproduce with LocalExecutor and KPO run containers in my local K8S-KIND in the default namespace (where airflow is not installed):
airflow 2.2.2 - apache-airflow-providers-cncf-kubernetes==3.1.1
or
airflow 2.2.4 - apache-airflow-providers-cncf-kubernetes==3.1.1
the operator can re-attach the running pod if I restart the sceduler

could you give us more context on the KPO ( what kind of K8S it use, in_cluster ? ... ) and the FULL airflow logs of the error",2022/3/23 11:24,1,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"

It is on EKS


in_cluster=True


Using the airflow  namespace. Same as the Airflow deployment.


No special annotations. Using the default template.
kpo_error.log",2022/3/23 12:32,2,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"
it look like your have a 1 hour timeout, can you check",2022/3/23 12:38,3,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"I do have a one-hour timeout for the task. This task takes 30s~1:30s when I downgrade to apache-airflow-providers-cncf-kubernetes=3.0.2. When I upgraded, it timed out.
It is supposed to create a new pod called engines-distant-sharer-{randomString} but instead it matches on the pod tagengineuserenginesdistantsharer which is the airflow worker that is running the task.",2022/3/23 12:59,4,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"you had this problem just after updating apache-airflow-providers-cncf-kubernetes ?
scenario :


KPO start a pod with apache-airflow-providers-cncf-kubernetes=3.0.2


restart airlfow with apache-airflow-providers-cncf-kubernetes=3.1.1


KPO do not re-attach correctly the pod and timeout


?",2022/3/23 16:06,5,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"
_",2022/3/23 16:35,6,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,the scheduler is able to start a worker pod ?,2022/3/23 16:42,7,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,didnt see the edit. the scenario you laid out is correct.,2022/3/23 16:46,8,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"so your issue is that
3.1.1 is not backward compatible with 3.0.2
so the provider should had bump to v4 because it's a breaking change ? (@potiuk )",2022/3/23 16:51,9,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"I dont think is related to backward compatibility. Maybe the correction in (3) is that the KPO is failing to start the pod. Not reattach,.",2022/3/23 16:53,10,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,Hmm. My question Is it always the case with 3.1.1 or is it something specific for @sushi30 setup?,2022/3/23 19:49,11,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,I cannot identify anything unique in my setup. The tasks in this setup have been working without fault for the past year or so. This broke with the change to 3.1.1.,2022/3/24 10:29,12,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"I am able to reproduce this with a minimal configuration helm chart on minikube. The latest apache/airflow image is using version 3.0.2 so you need to create a custom image with the new provider version. This DAG works fine with 3.0.2 and hangs in 3.1.1:


Screenshots
Task ran on 3.0.2

Then hangs on 3.1.1:
",2022/3/24 11:15,13,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"If 3.1.1 is not managing correctly the POD's started in 3.0.2 then can you delete the pod and clear the airflow TASK and the XCOM
so the task ( with 3.1.1) can start a real new POD and not try to re attach a running POD",2022/3/24 11:17,14,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"
If 3.1.1 is not managing correctly the POD's started in 3.0.2 then can you delete the pod and clear the airflow TASK and the XCOM so the task ( with 3.1.1) can start a real new POD and not try to re attach a running POD

This has nothing to do with pods started in 3.0.2. The KPO is not able to start new pods in 3.1.1.",2022/3/24 11:18,15,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"
This has nothing to do with pods started in 3.0.2. The KPO is not able to start new pods in 3.1.1.

@sushi30 - it is very likely something environmental for you. People often argue that ""it worked before so it must be backwards compatibiliity problem"" where in fact there might be other - environmental factors where misconfiguration or wrong deployment caused things to ""work"" (or rather mask problem) before, only to be revealed when for example new library provides more thoropugh check. Or maybe a library change causes some more resource usage and you simply need to increase resources (memory/disk/the like). There can be many things that could go wrong. I would not jump into conclusion this is backwards compatibility. It might be, but does not have to   - and it is not at all obvious.
That would be rather surprising to have some general problem -  we do not see other people reporting problems like that one.
Do you have any logs telling more what's happening? Maybe you can take a look at the logs of K8S creating PODs and maybe they will tell you what's wrong. The informaton that PODs are ""hanging"" makes it impossible to diagnose what's wrong - without more details we have even less information than you have.
And looking at logs of what's happening when it fails is something that only you can do, I am afraid. Also it would be great to get some more information  - which K8S version you have for example.
What I Can you also try the 2.2.5rc1 release of Airflow  (We just put it up for voting). The images we have in dockerhub contain both - latest Airflow and latest cncf.kubernetes so if you could try it and see if the problem persists there would be helpfi.",2022/3/27 16:16,16,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,Converting it into a discussion until we have more information.,2022/3/27 16:16,17,1,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"BTW. Yes the label has been changed to ""run_id"" but I do not thinkj it has anything to do with ti (however maybe @dstandish could tell)",2022/3/27 16:21,18,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"Another question @sushi30  - does your deployment rely in any way on the ""execution_date"" label?",2022/3/27 16:22,19,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"@sushi30 run the KPO the same namespace as the airflow deployments
could you try to run the pod in another namespace to see if it work ? (to isolate the problem)",2022/3/27 18:17,20,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"I can confirm this also occurs with apache/airflow:2.2.5rc2.
When trying this on another namespace the pod fails. This is expected because out-of-box helm chart does not support starting pods in different namespaces. This requires enabling multiNamespaceMode.",2022/3/29 14:56,21,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22555,2022/3/23 10:39,"I've faced with the same issue on Airflow 2.3.4. I've deployed Airflow on Kubernetes and used Kubernetes executor. Any dag with KubernetesPodOperator launched task pod and couldn't create a desired pod because it matched the task pod, so it was deadlocked.
I've found this PR #23371, which fixes this issue - but unfortunately it is not included in the latest Airflow release at this moment (2.3.4). So I've cherry-picked this commit 8e3abe4 on top of the v2-3-stable branch and built prod docker image (with option install providers from sources). I've tested such approach and it works fine.",2022/9/2 10:24,22,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22558,2022/3/25 14:29,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/3/25 14:29,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/22558,2022/3/25 14:29,"Converted it into discussion.
Why do you think there is anything wrong there? It's perfectly normal for K8S to kill stuff while running, this is what can happe because of resource management, hardware failures and other reasons.
Is there anything wrong that happened here that you wanted to point out? what was wrong and what did you expect to happen?",2022/3/27 19:18,2,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/22559,2022/3/25 14:45,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/3/25 14:45,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/22559,2022/3/25 14:45,This is very unlikely to happen. There is no distinction vs. commands in Airflow. You must have a different configuration of your deployment tooling.,2022/3/27 19:52,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/22559,2022/3/25 14:45,"Converted it into a discussion, and I encourage you to  check the internal tooling of yours/the way workers are deployed. There must be a difference vs other components that manifests this way.
Please also double check if the ""vanilla"" airflow commands behave as you expec them (without your company tooling). I am pretty sure they do. That will help you to diagnose the problem on your side.",2022/3/27 19:55,3,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/22673,2022/3/31 21:31,Yeah. We know about MRO. Simply don't use Mixins. The solution is simply to not use them. The best soluiton is to add docstring in the BaseOperator to not use Mixins in operators and warn the users. Mixins and mutliple inheritance are hard and it's completely counter-productive to try to support it. Would you like to make a PR for that?,2022/3/31 21:53,1,,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22673,2022/3/31 21:31,"Converting it into discussion, if more discusssion is needed.",2022/3/31 21:57,2,1,(IV) Further Discussion
https://github.com/apache/airflow/discussions/22874,2022/4/8 7:27,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/4/8 7:27,1,,(VII) Information Storage
https://github.com/apache/airflow/discussions/22874,2022/4/8 7:27,This could be a problem with WSL only : https://camcops.readthedocs.io/en/latest/administrator/server_troubleshooting.html#posix-spec-does-not-match-last-transition,2022/4/9 4:02,2,,(VII) Information Storage
https://github.com/apache/airflow/discussions/22874,2022/4/8 7:27,"Yep. Very much so. We are not going to address as WSL is not a ""production"" environment for us - at most it can be development environment and even there we recommend using Docker that should not have this problem.
It's better that WSL fixes it (or maybe it's already. Cionverting it into a discussion in case someone wants to discuss and propose potential workarounds.",2022/4/9 9:05,3,1,(VII) Information Storage
https://github.com/apache/airflow/discussions/23363,2022/4/29 12:06,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/4/29 12:06,1,,(II) Invalid Issues
https://github.com/apache/airflow/discussions/23363,2022/4/29 12:06,"Converting that to a discussion. You have not posted what kind of logs you are seing. Please add information, names, which are the big ones and copy parts of those so that it can be reasoned about.  We do not know how large is large and what is causing that - it migth be simply bugs in your DAGs.
Getting rid of the logs  might or might not be possible -depending on the kind of logs you have.  And there are also  other ways that can actually help you.
Generally all DAGs are parsed by scheduler regardless if they are paused - this is by design and is not going to change. If parsing of the dags produces a lot of logs - you shoudl make sure they are not produced. Tthis is entirely expected to get the DAGs parsed continuously even if parsed.
We want to have DAGs refreshed with the latest structure regardless if they are paused or not so that you can unpause them quickly.
Normally DAGs should not generate a lot of logs unless your example dags are doing that on purpose during parsing. so if they are generating a lot of logs, this likely the problem of your DAGs that need fixing.
And as a last resort you can always add .airflowignore if you want to disable the dags from being parsed.",2022/4/29 16:00,2,1,(II) Invalid Issues
https://github.com/apache/airflow/discussions/23365,2022/4/8 17:41,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/4/8 17:41,1,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/23365,2022/4/8 17:41,You have not written which component is producing this error.?,2022/4/13 15:13,2,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/23365,2022/4/8 17:41,BTW. Quick solution to that will be to set CONNECTION_CHECK_MAX_COUNT variable to 0 for the failing POD but it;'s not going to solve the problem becaue apparently for some reason the pod (I assume from the title) doing migration got a wrong configuration for SQL_ALCHEMY connection.,2022/4/13 15:21,3,1,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/23365,2022/4/8 17:41,"Is it possible that you have AIRFLOW__CORE__SQL_ALCHEMY_CONN  defined separately for your appliation?
Could you export your failing job defintion from kubernetes using kubeectl ?",2022/4/13 15:31,4,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/23365,2022/4/8 17:41,"Hello,
I will try your suggestion and get the information you asked.
Thanks a lot.",2022/4/20 19:20,5,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/23365,2022/4/8 17:41,"Hello,
We are having this issue in the pod run-migration-db,
CONNECTION_CHECK_MAX_COUNT is set as 0 (default value)
AIRFLOW__CORE__SQL_ALCHEMY_CONN: false
and in the section webserverConfig we import flask_appbuilder.security.manager import AUTH_OAUTH in order to configure Google Oath",2022/4/29 12:53,6,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/23365,2022/4/8 17:41,"
AIRFLOW__CORE__SQL_ALCHEMY_CONN: false

What do you mean by ""false""? This is a real connection you should have here?",2022/4/29 16:09,7,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/23365,2022/4/8 17:41,BTW. I will convert it into discussion - until it's vleared - this is likely your problem with configuration you need to debug./,2022/4/29 16:09,8,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,"Sorry for miss-clicked the close button.
Just pop up a thought for the issue this morning. Is any chances that the termination was triggered by MWAA (as it detect the process of Shard acting like a zombie process) instead of Airflow itself? Though, not so sure how could I verify if this is the case...",2021/10/25 4:00,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,"It's a possibility, but we have no way to verify. Maybe you could contact MWAA?",2021/10/25 5:26,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,"After dig logs a bit deeper, I have found the patterns (and issues?). And seems the issue should have nothing to do with MWAA environment.

The smart_sensor_task in all Shard DAGs create a new attempt when it pick up SIs/TIs. While new attempt means retry as failed for other (non-SmartSensor) tasks
It seems like smart_sensor_task is set to have maximum 101 attempts, does this means the task will eventually failed and create a new DAG run after 101 times it execute some sensors to poke due to # 1.



The smart_sensor_task missed logs after it pick up SIs/TIs. To be more specific, it missed logs after ""_load_sensor_works"" function executed, like ""Loaded %s sensor_works"" and ""Taking %s to execute %s tasks"".
All these Shard DAG runs were terminated 1 day after it started. I am wondering if it's because the timeout condition, but hard to tell since logs after _load_sensor_works"" are missed due to # 3.


Here are 2 cases for # 4:

Case 1, DAG run start at 2021-10-24T07:28:55 and terminated at 2021-10-25 07:34:01 on attempt 61



Case 2, DAG run start at 2021-10-16T07:30:04 and terminated at 2021-10-17 07:35:13 on attempt 17

",2021/10/25 9:46,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,"Below is the code for one of Shard DAG that generated by Airflow itself automatically after SmartSensor enabled. Which match my previous comment that retries is limited to 100 times and smart_sensor_timeout is overwritten to 24 hour, instead of the default 1 week in init.
",2021/10/25 10:04,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,"Just to summarize what I found, not sure if these are bugs or some of them are as it was designed.

SmartSensor timeout and terminate itself, which raise TI/DAG fail and false alarm when we monitor for TI/DAG fail.


It should not timeout as long as the smart_sensor_task is still running the loop and poking around.
Even we terminate smart_sensor_task for other reasons, it should be terminated in state success instead of fail.


smart_sensor_task should not create a new attempt every time it pick up SIs/TIs. This probably will cause DAG fail when the times of attempts reach retries.
Logs missing after smart_sensor_task picked up SIs/TIs. To be more specific, logs within function ""_load_sensor_works"" after line 358 (v2.0.2), Performance query %s tis, time: %.3f, and after the function are not record.
",2021/10/26 1:41,5,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,"Here one of the log for smart_sensor_task I downloaded from MWAA for reference. As you can see, it logged 4 lines as expected when 0 task is picked

but only logged 1 line where following information are missed when tasks are picked
[2021-10-24 06:56:36,180] {{smart_sensor.py:358}} INFO - Performance query 7 tis, time: 0.008
smart_sensor_group_shard_0_smart_sensor_task_2021-10-23T07_31_48.529736+00_00_60.log",2021/10/26 1:54,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,Hi @MartinKChen did you find a way to fix the issue? I'm on airflow 2.0.2 and having the same issue. I wonder if there is a way to at least reset the timeout for smart_sensor_group_shard.,2022/1/6 0:53,7,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,"@xiaochong2dai I did not fix the issue by modifying Airflow's source code as we are using MWAA (Amazon Managed Workflow for Apache Airflow), which makes it impossible to reach the code.
Alternatively, a workaround for the issue is to create our own operator and DAGs for shard group.

Create an operator that inherit from SmartSensorOperator, which used by Shards, and overwrite the ""execute"" function by comment out the code causes Shard fail. Following the sample code:



Create Shard DAGs that uses the operator above. Which copied from ""airflow-2.0.2\airflow\smart_sensor_dags\smart_sensor_group.py"" with different Operator.


There is a downside for the solution that smart_sensor_group_shard_* created by Airflow sill been created automatically as they are generated by Airflow itself, we just keep it suspended, and enable our own DAGs instead, which may cause confusion when operating.",2022/1/6 5:27,8,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,Thank you so much @MartinKChen for the detailed explanation!,2022/1/6 5:44,9,,(III) Not a Bug
https://github.com/apache/airflow/discussions/24011,2021/10/18 10:02,"It's unlikely that this bug/problem will be explored as SmarSensor is a deprecated feature
I suggest that you will move to deferrable operators.
https://airflow.apache.org/docs/apache-airflow/stable/concepts/deferring.html#smart-sensors
I'm moving this issue to Github Discussions",2022/5/29 18:24,10,1,(III) Not a Bug
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"I find no value with the current SLA feature. I don't know many people who are using it due to it's limitations.
I would say that a better approch is to deprecate the current SLA feature and replace it with a new one. I'm willing to pick up some tasks on that project if we decide to move forawrd with it.",2021/9/10 8:26,1,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,Yeah. I think we should have a more comprehensive/better/complete/usable SLA implementation replacing the current one.,2021/9/10 10:20,2,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"Seems like this could use the new triggerer service. Essentially, it is like branching out and having a suspended task with a trigger that activates at the deadline.",2021/9/30 8:38,3,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"
Seems like this could use the new triggerer service. Essentially, it is like branching out and having a suspended task with a trigger that activates at the deadline.

Hi @malthe, could you elaborate a bit more? What's ""triggerer service"" and how can it be used to warn about the deadline?",2021/10/4 5:09,4,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"@yuqian90 what I'm referring to are the new deferrable operators.
There's a framework in there which allows us to set up future actions such as reacting to a ""missed deadline"". It might need a little reworking in order to implement SLAs but I think it's pretty close since you could also just branch out and use the new DateTimeSensorAsync.
Note that this framework is available only from Airflow 2.2 onwards.",2021/10/4 5:58,5,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"Yeah, this feels like a good use for async sensors indeed. I think it's even possible to do it with the old sensors (with obvious resource issues, but possible if you have practically unlimited worker resource).",2021/10/4 6:44,6,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"Maybe I am not understanding the ticket, but I don't find this to be true:


sla_miss_callback only fires after the task finishes. That means if the task is never finished in the first place due to it being blocked, or is still running, sla_miss_callback is not fired.


if I have a DAG that should be hourly and for example I set a 1-hour SLA, I will get an SLA miss email if a single DAG run is still running past 1 hour. If a DAG finished and it failed it will use on_failure_callback, and if a DAG finished and it succeeded it will use on_success_callback. So that covers both cases of a DAG being complete (which is only possible if no tasks are running or blocked) so obviously the sla_miss_callback is if the DAG is missing its SLA, even if it is running.
I do agree that there should be better control for task-specific deadlines, but also this can be accomplished partially today by putting the part of the DAG that needs a deadline should be by itself in a separate DAG with an SLA in place, and then the remainder of a DAG will be in a 2nd task and use ExternalTaskSensor.",2021/11/10 1:13,7,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"I tried to use ExternalTaskSensor to check other tasks deadline
but because the try_number is not increased when the sensor mode is reschedule
so it will run forever until the conditions are met and marked as SUCCESS (never be marked as FAILED and call the on_failure_callback function)
The related issue is here: #18080",2021/11/12 2:45,8,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"
I tried to use ExternalTaskSensor to check other tasks deadline
but because the try_number is not increased when the sensor mode is reschedule
so it will run forever until the conditions are met and marked as SUCCESS (never be marked as FAILED and call the on_failure_callback function)
The related issue is here: #18080

I believe the intention of using ExternalTaskSensor with a deadline planned is to set a timeout and have no retries so if it doesn't complete by the timeout it fails, not just run forever.",2021/11/12 2:55,9,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,Also as of Airflow 2.2.0 I think sensors will no longer retry if the sensor times out: #12058,2021/11/12 3:02,10,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"
I believe the intention of using ExternalTaskSensor with a deadline planned is to set a timeout and have no retries so if it doesn't complete by the timeout it fails, not just run forever.

Thank you for the information.
(I am quite new to Airflow so please excuse my lack of knowledge __ )
Actually I tried adding a timeout to my ExternalTaskSensor before but its behavior seemed not change. Maybe I should retry with a shorter one.
And by the way, with soft_fail=True, my on_failure_callback will not be called, I believe.
(on_retry_callback won't be called either with mode : ""reschedule"".)
Therefore, I have to create a new sensor based on the ExternalTaskSensor (the logic is quite the same but there is no inheritance relationship between them) to check deadlines of other tasks in the same DAG, to send a notification when the deadline missed while make it possible to let Airflow mark the DAG as SUCCESS when it tasks finished after deadline.",2021/11/12 3:32,11,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24189,2021/9/4 14:14,"I think this is a wider effort - improving SLAs. We know current SLAs are not really useful and there should be an effort to redefine SLA behaviour basically from scratch.
However it requires devlist discussion and AIP added here as this is important, big fetature to add to Airflow. https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals
Converting it to discussion, but would be great if somoene starts an effort around creating ""New SLA AIP"".",2022/6/4 19:17,12,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"Workaround
As the task only look into parents for the state, we can create a DummyOperator which has dependency overall

",2021/4/21 8:26,1,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"I'm sorry but I don't understand what is the feature request here?
TriggerRule.ALL_DONE works fine with Success, Failed and Skipped.

",2021/6/11 19:26,2,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"
I'm sorry but I don't understand what is the feature request here?
TriggerRule.ALL_DONE works fine with Success, Failed and Skipped.



sure @eladkal , In your example
the DAG state will be success even though one of the intermediate tasks has been failed,
because the DAG state only depends on the state of the last task.
which may not be a requirement in many of the cases and should be configurable.",2021/6/13 13:28,3,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"So remove end_op from the code and it will work as you expect, isn't it?",2021/6/13 13:55,4,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"
So remove end_op from the code and it will work as you expect, isn't it?

yeah, but what if you wanted end_op to run.
ex.
you are created a GCP Dataproc or AWS EMR Cluster and running bunch of tasks, your last task is Delete cluster.
if any of the tasks failed, you still wanted to delete your cluster but the status of dag must be failed.",2021/6/13 17:38,5,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"I have similar use case but I don't cause the DAG itself to fail if EMR failed. Like you I run a task task to terminate the machine.
Am I correct that what you are asking is to change the way that Airflow consider DAG runs statuses (success/failure) by giving the user the option to set it according to his own logic?",2021/6/14 5:34,6,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"
I have similar use case but I don't cause the DAG itself to fail if EMR failed. Like you I run a task task to terminate the machine.
Am I correct that what you are asking is to change the way that Airflow consider DAG runs statuses (success/failure) by giving the user the option to set it according to his own logic?

yeah, that's correct.",2021/6/14 5:58,7,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"@eladkal @potiuk @ashb @kaxil @mik-laj
Does this requirement looks legit?
I would like to work on it, if this looks ok.",2021/10/27 5:24,8,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"
I have similar use case but I don't cause the DAG itself to fail if EMR failed. Like you I run a task task to terminate the machine. Am I correct that what you are asking is to change the way that Airflow consider DAG runs statuses (success/failure) by giving the user the option to set it according to his own logic?

@eladkal  In your case, if you consider dag to be successful even though intermediate jobs have failed,
How are you monitoring your jobs and Isn't the dag should fail ideally in your case?",2021/10/27 5:26,9,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"Like @eladkal mentioned - I have had similar requirements like him and I would have on_failure_callbacks notifying that my task failed.
For your case, you can write a CustomOperator which you have at the end that checks if your EMR task failed or not and fail this task itself, thereby the DagRun will also be failed as it is the last task.

Your feature request while legit need more thinking through:

What happens when there are more than one terminal tasks
Do we want to allow users to set which task_id need to succeed for DagRun status
",2021/11/2 23:03,10,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24190,2021/4/21 8:13,"this is as expected We implemented some solutions for that for AIP-47 system tests. There is a simple pattern of adding a ""watcher"" task to monitor tasks that should fail whole dag in case any of upstream tasks failed and I think this answers all the questions. See https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-47+New+design+of+Airflow+System+Tests for details.
Converting it into discussion in case more discussion is needed.",2022/6/4 19:21,11,1,(VII) Information Storage
https://github.com/apache/airflow/discussions/24193,2020/11/27 12:30,Thanks for opening your first issue here! Be sure to follow the issue template!,2020/11/27 12:30,1,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24193,2020/11/27 12:30,"Yes, this would be a good feature!
What most people seem to do right now as a work around is to have a special ""backfill dag"" that does the batching.
We (collectively) will need to spend some time designing an interface for this, and then likely raise it as an Airflow Improvment Proposal https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals
I'll happily help you with this process.",2020/11/27 12:40,2,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24193,2020/11/27 12:30,This sounds good! I wonder if we could mix it with triggering backfill externally #11302,2020/11/27 12:48,3,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24193,2020/11/27 12:30,"
This sounds good! I wonder if we could mix it with triggering backfill externally #11302

Yeah absolutely. I'll raise an AIP for backfill improvements when I get the chance and we can discuss what the scope of that should be there.",2020/11/27 15:05,4,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24193,2020/11/27 12:30,"@jward-bw I did a small draft last week so feel free to use it:
https://docs.google.com/document/d/1q138mGBfr9uEJbe43sTPobj_30vo2g2pIel1rBt9fLg/edit?usp=sharing",2020/11/27 16:24,5,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24193,2020/11/27 12:30,"Very old. If this is still an issue, let's move it into a discussion.",2022/6/4 19:30,6,1,(VII) Information Storage
https://github.com/apache/airflow/discussions/24195,2020/6/15 6:53,Thanks for opening your first issue here! Be sure to follow the issue template!,2020/6/15 6:53,1,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24195,2020/6/15 6:53,Is this feature available now?,2021/4/19 10:41,2,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24195,2020/6/15 6:53,And perhaps add optional operator option email_on_hold to be able to send an e-mail when a task is put on-hold.,2021/12/6 8:52,3,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24195,2020/6/15 6:53,"To my understanding a possible solution is to change the On/Off from DAG level to Task level.
So a DAG would be considered ON if at least one of it__ task is ON. Even assuming this is feasible it brings many challenges for example how would dag timeout behave if some tasks are on hold?",2021/12/6 8:59,4,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24195,2020/6/15 6:53,"I think that's probably the best you can do yes. After each DAG run, use the on_failure_callback and on_success_callback methods to disable the DAG model (i.e., put it into ""off"" state).",2021/12/6 9:06,5,,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24195,2020/6/15 6:53,"This is a nice feature- some kind of ""manual approval"" for a task. I think this is a good idea, but needs an AIP and devlist discussion. Hopefully someeone will pick it up. In the meantime I convert it to a discussion.",2022/6/4 19:35,6,1,(I) Non Actionable Topic
https://github.com/apache/airflow/discussions/24199,2022/1/10 5:45,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/10 5:45,1,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24199,2022/1/10 5:45,"I see a good working solution of this with https://szeevs.medium.com/handling-airflow-logs-with-kubernetes-executor-25c11ea831e4
All we need to do is make this k8s_task_handler file available under utils/log/ and configure log_config.py file to link airflow.tasks to this new handler as well.",2022/1/13 4:25,2,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24199,2022/1/10 5:45,"k8s task handler.py which we can use https://gist.github.com/szeevs/938ad3cf96e732d4b1b55a74015aed5b
log_config.py will look like

Make sure while deploying, configure this env

name: AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS
value: ""log_config.LOGGING_CONFIG""

This will do the job. :)",2022/1/13 4:28,3,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24199,2022/1/10 5:45,@nirutgupta Where should the file k8s task handler.py  be located in the airflow path?,2022/5/31 18:47,4,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24199,2022/1/10 5:45,"I created a new directory custom_log and made that file available under it.
",2022/6/1 4:41,5,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24199,2022/1/10 5:45,"Converted it into discussion - I think it is solved by the deployment, but the discussion is open if it is not solved yet.",2022/6/4 21:54,6,1,(VII) Information Storage
https://github.com/apache/airflow/discussions/24199,2022/1/10 5:45,"It looks there is no need for including custom k8s_task_handler.KubernetesTaskHandler, a standard logging.StreamHandler from python's logging does the job.
My configuration is copied under ${AIRFLOW_USER_HOME}/config as in @nirutgupta's solution, but the content is as simple as:
",2022/6/9 6:52,7,,(VII) Information Storage
https://github.com/apache/airflow/discussions/24798,2022/2/8 22:19,Just a proposa - feel free to attempt the fix. There is an open issue #18371 to upgrade all google clients to >= 2.0 - maybe this is a good chance to become one of the > 1900 contrubutors like you to upgrade it ?,2022/2/8 23:24,1,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/24798,2022/2/8 22:19,"Is it worth looking into why the discovery document is throwing a 403 with the current authentication logic? It may be impacting google API calls other than just the dataflow discovery document. Or is it possible it's an issue on the dataflow discovery url's side?
Regarding bumping the package to 2.x: It looks like there some comments about changing version requirements of other google clients, but none of them seem to impact just allowing the google-api-python-client to use 2.x (as long as its not changed to require 2+). There doesn't appear to be anything that would impact code using the client either. That wouldn't necessarily solve the issue though, since I would be locking it to google-api-python-client>=1.6.0,<3.0.0 so users may still end up with version 1.x if they have other packages with more restrictive dependencies.",2022/2/10 16:07,2,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/24798,2022/2/8 22:19,"
Is it worth looking into why the discovery document is throwing a 403 with the current authentication logic? It may be impacting google API calls other than just the dataflow discovery document. Or is it possible it's an issue on the dataflow discovery url's side?

Sure. Feel free to look at it!",2022/2/15 17:10,3,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/24798,2022/2/8 22:19,"https://stackoverflow.com/questions/59858003/using-airflow-with-bigquery-and-cloud-sdk-gives-error-user-must-be-authenticate
This might be related. People noticed that removing the quota_project_id from the credentials file appeared to resolve the issue.",2022/3/30 18:34,4,,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/24798,2022/2/8 22:19,Converting to discussion due to the note from pbhuss which suggests this is not an Airflow issue.,2022/7/2 9:55,5,1,(VI) Unrelated Repository
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,Thanks for opening your first issue here! Be sure to follow the issue template!,2021/9/27 15:09,1,,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,"@potiuk  I think I can try this one.
My current understanding of the code + changes required:

CloudwatchTaskHandler delegates to watchtower.CloudWatchLogHandler which provides the option to create a log group if it doesn't exist (true by default) https://github.com/kislyuk/watchtower/blob/develop/watchtower/__init__.py#L84
Do we intend to continue with the existing behavior but allow users to override based on config values defined ? e.g.  conf.get('logging', 'CLOUDWATCH_CREATE_LOG_GROUP', fallback=True) similar to https://github.com/apache/airflow/blob/main/airflow/config_templates/airflow_local_settings.py#L209 ?

Let me know if thats the approach. Can do the changes accordingly.",2021/10/7 14:32,2,,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,Hard to say. I am not expert on cloud-watch - but maybe others can help :),2021/10/10 21:31,3,,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,"Was curious more from a config standpoint. I believe the way to specify/override the default config is from airflow_local_settings.py. Will create a PR. Will make it easier to iterate on it.
If anyone else gets to reviewing the approach, please let me know :)",2021/10/12 5:29,4,,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,"Hey folks,
watchtower does an idempotent create for the log group as you can see here.. If the log group already exists it will catch that error and ignore it silently.
The issue you actually hit is an authentication issue, as seen in your exception message:

botocore.exceptions.ClientError: An error occurred (AccessDeniedException) when calling the CreateLogGroup operation: User: arn:aws:sts:}:assumed-role/airflow-service-account-role/botocore-session-1632755037 is not authorized to perform: logs:CreateLogGroup on resource: arn:aws:logs:eu-central-1::log-group:airflow:log-stream:

The role you're using doesn't have permission for the log group creation. It's worth double checking that role has all the required permissions for cloudwatch logging (creating groups, streams and uploading records).",2021/10/28 18:38,5,,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,"Given the discussion in 19022, I think this issue can be resolved/closed?",2021/11/3 15:46,6,1,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,"Hey folks,
sorry that I didn't respond for that long.
I also figured out that this looks like an authentication problem but even with all privileges given to airflow the error still occurred. I was wondering if airflow maybe tries to create the log group inside the log stream. The error arn:aws:logs:eu-central-1:<redacted>:log-group:airflow:log-stream: looks like the correct path, but I don't see a reason why AWS would require the sub-resouce :log-stream: when calling to create a log-group.",2021/11/3 19:54,7,,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,"What I'm quite sure of is that the issue is constrained to your configuration. Otherwise we'd have this exception for all users with logging to cloudwatch.
I don't think the boto3 api would return an authentication issue if the log group name was formatted incorrectly. The airflow-service-account-role you're using likely is still missing the correct policies.
As for the strangely formatted loggroup name, check the config you're using to specify it. You can even paste that here to get a second pair of eyes on it.",2021/11/3 20:48,8,,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,"I am facing the very same issue on Airflow 2.2.2

The Cloudwatch log group is already created using Cloudformation and the policies accordingly:

The Airflow cofiguration variables are the following:

Adding logs:CreateLogGroup to the task role seems to resolve the issue. But why is that necessary?",2022/5/17 12:45,9,,(V) Already Fixed
https://github.com/apache/airflow/discussions/24892,2021/9/27 15:09,Based on discussion in #19022 i'm converting this to github discussion,2022/7/7 6:47,10,,(V) Already Fixed
https://github.com/apache/airflow/discussions/26012,2022/1/19 21:03,Thanks for opening your first issue here! Be sure to follow the issue template!,2022/1/19 21:03,1,,(III) Not a Bug
https://github.com/apache/airflow/discussions/26012,2022/1/19 21:03,Please update the title to reflect the nature of your issue,2022/1/19 22:05,2,,(III) Not a Bug
https://github.com/apache/airflow/discussions/26012,2022/1/19 21:03,Feel free to submit a PR,2022/1/20 8:07,3,,(III) Not a Bug
https://github.com/apache/airflow/discussions/26012,2022/1/19 21:03,"
Feel free to submit a PR

How do i test changes?",2022/1/20 19:51,4,,(III) Not a Bug
https://github.com/apache/airflow/discussions/26012,2022/1/19 21:03,"
How do i test changes?

https://github.com/apache/airflow/blob/main/CONTRIBUTORS_QUICK_START.rst",2022/1/23 20:52,5,,(III) Not a Bug
https://github.com/apache/airflow/discussions/26012,2022/1/19 21:03,@alexandermalyga since you work with Trino maybe you can assist on this issue?,2022/8/27 6:29,6,,(III) Not a Bug
https://github.com/apache/airflow/discussions/26012,2022/1/19 21:03,"
@alexandermalyga since you work with Trino maybe you can assist on this issue?

I haven't been able to reproduce the issue (or I'm not understanding it correctly).
When running Trino without authentication, it's enough to just provide some username without a password.
When running with authentication, TrinoHook picks up the provided connection (or the default trino_default) just fine.
Tried both options using Airflow 2.3.4 and apache-airflow-providers-trino==4.0.0 without any issues.",2022/8/27 15:52,7,,(III) Not a Bug
https://github.com/apache/airflow/discussions/26012,2022/1/19 21:03,Thank you for checking. Converting this issue to discussion as it seems to be something local,2022/8/27 16:17,8,1,(III) Not a Bug
https://github.com/apache/superset/discussions/17857,2021/12/22 13:19,We would appreciate it if you could provide us with more info about this issue/pr! Please do not leave the title or description empty.,2021/12/22 13:19,1,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/17857,2021/12/22 13:19,Is there possible to schedule reports on different time with different user,2021/12/22 13:21,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/17857,2021/12/22 13:19,Basically I want scheduling email using metadata,2021/12/22 13:22,3,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/17857,2021/12/22 13:19,"@betodealmeida @eschutho may have some insight here, but for the time being, since this is a how-to question rather than an Issue, I'm converting it to a discussion, and folks can hopefully dive into it there.",2021/12/22 18:39,4,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/17857,2021/12/22 13:19,You may also want to reach out to the Slack community here as well... let me know if you need any pointers on connecting there (it's in the docs),2021/12/22 18:40,5,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/17857,2021/12/22 13:19,Hi @VivekGupta97 You can schedule each report separately for a different time. Are you looking to have one report send out different times based upon who the email is sent to?,2021/12/22 18:42,6,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/17857,2021/12/22 13:19,Actually I want to Send Report on Different Time . If there are 500 reports and I want to send them to the user on different time so will it be possible or just I need to create different report schedule for that manually ?,2021/12/23 4:05,7,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18640,2022/2/8 2:08,Hello @ad-m I think this is a good idea! Thanks for sharing. I am moving this issue to discussions as we are looking to use the Github issues for true bugs only.,2022/2/9 11:37,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18640,2022/2/8 2:08,"The question that we have to answer is whether we forbid anything else to be referenced by ""React"".
It is worth taking a look at the imports:

And references too:

It looks ReactNode (34+ imports) vs. React.ReactNode (60 references) and PureComponent (8+ imports) vs. React.PureComponent (30 references) is another common inconsistency.",2022/2/10 4:47,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18673,2022/2/2 10:32,"Hello @davidjez thanks so much for this idea. Just want to inform you that from now on we would like to use the issues only for true bugs. For that reason, I am moving this one to Discussions. Thanks!",2022/2/11 8:23,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18700,2022/2/14 2:00,"Hi @LuPan2015, GitHub issue list is used to track bugs now, so I transfer this topic to the discussion board.",2022/2/14 3:56,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18700,2022/2/14 2:00,"thanks, I have learned",2022/3/21 2:15,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18742,2022/2/8 17:02,Hi there @xjobex we're trying to keep Issues focused on bugs! I've moved this to a Discussion instead :],2022/2/15 13:39,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18742,2022/2/8 17:02,I asked on Flask-AppBuilder and they answered me that it is not supported: dpgaspar/Flask-AppBuilder#1803 (comment),2022/2/20 21:44,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18744,2022/2/15 15:06,Hey @bluepython508 I'm moving this over to Github discussions because we'd like to keep Issues focused on true bugs / bugs to be validated or fixed! Thank you :),2022/2/15 15:33,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18806,2022/2/17 18:02,Hi @Narendra678 Github issues is for reporting of bugs or occasional feature requests / design conversations. I'm moving this over to Github Discussions where someone can try to help (or you can also join the Superset Slack),2022/2/18 0:34,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18806,2022/2/17 18:02,"Tutorial: How to enable Superset language selection
Background Info: as of 2022-Feb-17, Superset version 1.4.1
Language Selection are disable by default, you can see that in file superset/config.py

So, How to enable that?
Step 1: Create a superset_config.py file
in my case I need to use Chinese:

in your case you need Spanish

Important: You have to config AT LEAST 2 Language to enable language selection
Step 2: add superset_config.py to proper PYTHONPATH location
Like this:

This line put superset_config.py into proper location.

Why /app/pythonpath/?
because in Docker Image apache/superset, it set default PYTHONPATH to /app/pythonpath/

The result look like this
",2022/2/18 7:48,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18806,2022/2/17 18:02,"Hi Sir,
I did not understand this ""You can consider contributing some Spanish translation into Superset source code""
you mean we can not do this?
Regards,
Naren",2022/2/24 14:16,3,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18806,2022/2/17 18:02,"Do i need to modify below code?
LANGUAGES = {
'en': {'flag': 'us', 'name': 'English'},
""es"": {""flag"": ""es"", ""name"": ""Spanish""},
}",2022/2/24 14:21,4,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18806,2022/2/17 18:02,"@1c7 after adding superset_config.py to /app/pythonpath/ by docker exec ...
I can't login with default admin admin , do you know why this?",2022/5/28 10:57,5,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18807,2022/2/17 17:57,"Hi Naren, moving to Discussions as this isn't a bug or feature request!",2022/2/18 0:34,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18807,2022/2/17 17:57,"Hi @Narendra678, were you able to find a solution for this?",2022/8/8 10:41,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18807,2022/2/17 17:57,"I think users without Explore Chart permission, will not see this option in dashboard",2022/8/9 4:13,3,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18807,2022/2/17 17:57,"Hi @kamalkeshavani-aiinside, in my case, we would like the users to be able to 'Export CSV' / 'Download -> CSV' but to restrict them from seeing / viewing the query. Are you aware of any permissions that enable us to achieve this?",2022/8/9 4:15,4,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18807,2022/2/17 17:57,"Remove the explore on Superset privilege and that will remove the View Query option.  A side effect of that will be that the role can no longer view embedded standalone charts.  That same permission controls both.
My workaround for this blocking standalone charts in an iframe is to make a single-chart dashboard and then embed that.",2022/8/10 18:19,5,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18844,2022/2/21 10:14,"Converting to discussion, as this is an edge case.",2022/2/22 10:02,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18889,2022/2/23 15:07,"Hey @rahularram1999 since this isn't a bug, I'm moving this over to Discussions :)",2022/2/23 18:42,1,1,(III) Not a Bug
https://github.com/apache/superset/discussions/18927,2022/2/24 9:45,Hi @aakanshajecrc we're reserving Github issues for discussion around bugs so I'm moving this over to Discussions!,2022/2/24 13:29,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18928,2022/2/24 9:28,"Hey there! We use Github issues in this community to discuss bugs, so I'm moving this over to Discussions :]",2022/2/24 13:30,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18941,2022/2/23 10:45,"You need to add some more context, like logs, error messages etc, otherwise it will be very difficult for others to help. In addition, this appears to be a question on how to enable LDAP - @zeromsi have to checked if the proposed solution in the FAB docs works? https://flask-appbuilder.readthedocs.io/en/latest/security.html#authentication-ldap",2022/2/23 11:50,1,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18941,2022/2/23 10:45,"Converting this to Discussions, as this is not reporting a specific bug, but is rather a request for configuration help.",2022/2/25 9:37,2,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18941,2022/2/23 10:45,"I have also been struggling for weeks to get this to work, with no success. Now I have picked it up again, and started with the basics, which means that in my container I have a small python script using python-ldap that actually works by itself!
Question is, how should one configure the options in AppBuilder to mimic the working python setup?
The script:
`import ldap
ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_NEVER)
l = ldap.initialize(""ldaps://myldapserver:636"")
l.set_option(ldap.OPT_REFERRALS, 0)
l.set_option(ldap.OPT_PROTOCOL_VERSION, 3)
l.set_option(ldap.OPT_X_TLS,ldap.OPT_X_TLS_DEMAND)
l.set_option( ldap.OPT_DEBUG_LEVEL, 255 )
l.simple_bind_s(""myuser@domain.com"",""mypassword"")
base = ""DC=domain,DC=com""
criteria = ""(&(objectClass=user)(sAMAccountName=johndoe))""
attributes = ['displayName']
result = l.search_s(base, ldap.SCOPE_SUBTREE, criteria, attributes)
print(result)`",2022/3/16 10:25,3,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18944,2022/2/25 11:35,"Hi @pm20202 we're reserving Github issues primarily for bugs and occasional enhancement requests. I'm converting this to a Githhub Discussion, which is better suited for debugging. I'd also encourage you to join the Superset Slack community!
superset.apache.org/community",2022/2/25 12:18,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18969,2022/2/28 15:03,Hi @maya-harel I'm moving this over to Discussions since this isn't a bug :],2022/2/28 18:47,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/18969,2022/2/28 15:03,"@maya-harel Hi, do you have any ideas?",2022/9/13 14:06,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19087,2022/3/8 8:44,Moved to Discussions since this is a feature request and not a bug!,2022/3/9 19:50,1,1,(III) Not a Bug
https://github.com/apache/superset/discussions/19087,2022/3/8 8:44,"There's a legacy chart type called, Bubble Chart - I think that's what you're looking for.",2022/3/10 9:37,2,,(III) Not a Bug
https://github.com/apache/superset/discussions/19185,2021/7/17 6:08,@rubenssoto thanks for submitting! Moving this feature request to Github Discussions so we can keep Issues focused on bugs!,2022/3/16 16:27,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19190,2021/7/16 3:58,Hey @Riskatri converting to a Discussion so hopefully someone in the community can help you debug this!,2022/3/16 16:35,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19254,2022/3/18 3:13,"
Currently, we can only create a chart from scratch, instead of modifying it based on an existing chart_

I don't understand this issue, Could you detail it?",2022/3/18 4:52,1,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19254,2022/3/18 3:13,"

Currently, we can only create a chart from scratch, instead of modifying it based on an existing chart_

I don't understand this issue, Could you detail it?

Similar to copy action in role. As shown below

This way I can quickly create a new chart based on an existing chart_",2022/3/18 5:15,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19254,2022/3/18 3:13,"Hey @LuPan2015 you're correct that Duplicating / Copying charts doesn't exist at the moment as a feature.
Two workarounds:


Export and Import the chart and give it a new name.
OR


Open the chart in Explore and when you click Save, save it as a new chart with a new name (don't overwrite).


I'm converting this thread to a Github Discussion, since we use Github Issues to focus on bugs in existing functionality :)",2022/3/18 13:35,3,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19282,2022/3/21 10:42,"Hi there, I think this may be an issue with your setup / config not a bug in Superset per say! I'm converting this to a discussion so someone can try to help you out!",2022/3/21 12:40,1,1,(III) Not a Bug
https://github.com/apache/superset/discussions/19304,2022/3/21 16:02,Converting this into a discussion,2022/3/22 11:31,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19304,2022/3/21 16:02,"Thanks for putting this together!
Some immediate thoughts:

For code style choices, one should always just follow Prettier and ESLint. If something is not banned by ESLint, then it's allowed. If something gives warning in ESLint, then please fix it.
If something will be captured by ESLint, then we don't need to include it in this guide. One example would be discouraging using loop indexes as keys. Ideally we'd like this style to be short and sweet and that people can remember and learn something new after reading it.
We should guide users to use existing hooks in /src/hooks and add new ones in this centralized place.
I'd recommend changing ""We prefer the usage of functional components"" to ""we prefer functional components over class components"" just for clarity.


Re: code readability, allow me to rewrite your code to demonstrate a couple of points:

can be rewritten to


To maximum readability and reduce footprint, return value types should be avoided unless it adds clarify.
Arrow functions are preferred for one-liners---ESLint will warn you about this.
Optional parameters with default values do not need ? and type annotation. TypeScript is smart enough to infer discount is number.
I personally prefer to write React component as named functions and utility functions with arrow functions as it makes React components looks more ""special"" and code is more concise this way when you export default.
",2022/3/22 17:49,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19304,2022/3/21 16:02,"@geido This is great. Should we make a distinction between reusable components and larger components that are made up of smaller components? For example, creating a Story would apply to a reusable component like a dropdown component but a Cypress test would be applicable for a larger component that makes up a larger portion of the page as in the SqlLab left toolbar component. I think focusing this document on one or the other could make it simpler.",2022/3/29 15:48,3,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19304,2022/3/21 16:02,Awesome stuff! The one nit I might want to change is the bit about Default Exports. I kind of prefer Named Exports. Even if you import { SomeComponent as WhateverYouWantToCallIt } from './SomeFile' it's very easy to search globally in your IDE for instances of SomeComponent whereas that's more difficult when if someone does import YourThingWasRenamedHere from  './SomeFile'.,2022/4/14 7:18,4,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19335,2022/3/21 8:11,"Please use
'from superset.typing import CacheConfig' instead of 'from superset.superset_typing import CacheConfig'
and do  ""superset db upgrade""
It will work.",2022/3/21 8:13,1,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19335,2022/3/21 8:11,superset.typing was recently renamed to superset.superset_typing. If you're running docker-compose you may need to do docker-compose pull for it to work.,2022/3/21 8:17,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19335,2022/3/21 8:11,"Moving to Discussion so the debugging help can continue __ If there's a clear bug you can help the community reproduce, then definitely open a Github Issue!",2022/3/23 14:38,3,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19353,2022/3/24 8:38,"Hey there, moving to discussion since this is a question :) We're hoping to keep Issues focused on bugs to be reproduced and fixed!",2022/3/24 15:27,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19378,2022/3/25 18:10,Hey Zenan! Moving to discussions since issues are for bugs primarily :),2022/3/26 12:46,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19589,2022/4/7 10:58,"Superset uses SQLAlchemy as an abstraction layer to talk to databases and MongoDB doesn't have a reliable SQLAlchemy dialect.
https://preset.io/blog/building-database-connector/
You can connect Superset to Presto or Trino instead and have either of those engines query MongoDB!
https://preset.io/blog/2021-6-22-trino-superset/
Moving to discussions as this isn't a bug :)",2022/4/7 14:19,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19590,2022/4/7 10:07,I could be wrong but I believe this functionality was removed entirely from Superset. Moving to Discussions so Issues can be focused on bugs!,2022/4/7 14:22,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19590,2022/4/7 10:07,"Ok, since url still can be accessed directly, my assumption was that it's display is hidden under some permission.",2022/4/8 6:35,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19652,2022/4/11 2:07,"Hey Aswar! Moving to Discussions since this isn't a bug (we're trying to keep Issues focused on Bugs and SIPs).
You may also find this introductory event useful around the Superset API: https://www.youtube.com/watch?v=vFTI087HYNE",2022/4/11 18:58,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19704,2022/4/13 20:23,"Since this isn't a bug, moving to Discussions! Note for the future, this has been discussed / answered in the Superset Slack!",2022/4/13 23:18,1,1,(III) Not a Bug
https://github.com/apache/superset/discussions/19705,2022/4/13 6:19,"Hey @SakshiTharkude the CSV export actually exports the data underlying a chart. It's basically what's returned from the database :)
This is consistent and makes sense for all charts but I could see why it's confusing for Table and Pivot Table charts, where the ""chart"" closely resembles a tabular dataset (in which case, you may expect CSV to export that instead).
I'm moving this to discussions as this is a cool feature request but not a bug per say, as this is working as intended!",2022/4/13 23:21,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19705,2022/4/13 6:19,"When exporting to CSV with the 'Pivoted' option, all row fields are concatenated into one column in Excel. Is this a known issue?
",2022/10/6 18:17,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19706,2022/4/13 6:01,Hey @SakshiTharkude moving to Github Discussions so we can keep Issues focused on bugs!,2022/4/13 23:34,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19762,2021/10/19 1:43,"If there are multiple time shifts for a certain series, should we cycle through different dashing options or even revert back to changing colors at some point?",2021/10/19 3:28,1,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19762,2021/10/19 1:43,"The nvd3 charts handle it by cycling through multiple dashing options.

I haven't seen users using more than 2 time shifts in a single chart, but it could certainly happen - the setup allows that.
This would be the best solutions in order:

User can chose whether to use dashed line or different color (even if they can't chose what color / what type of dash)
Current experience - dashed line
(not preferred) Current e-chart solution - lines are different colors

cc @graceguo-supercat",2021/10/19 16:36,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19762,2021/10/19 1:43,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue .pinned to prevent stale bot from closing the issue.",2022/4/17 18:56,3,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19762,2021/10/19 1:43,"I think this issue is still relevant for when we evaluate feature parity with echarts before deprecating nvd3; Maybe it's a feature as opposed to bug, so I will move it to dicsussions.",2022/4/18 16:38,4,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19762,2021/10/19 1:43,"Having the time shift in a different 'solid' color makes it extremely difficult for the human eye to focus on a metric and its corresponding time shifts. This is especially true when you have multiple metrics. I tried using a color schema like ""D3 Category 20c"" to navigate this issue temporarily.

Another variation of this issue is when you add a Group By and a Time Shift. You get a bunch of colorful lines and it becomes impossible to identify a metric and time-shift pair (without looking at the legend carefully). e-charts assigns colors to the Group By first then to the Time shifts.


Can this bug be made into a request that can be prioritized for Superset?",2022/6/9 22:56,5,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/19762,2021/10/19 1:43,"@Mayakpwiki these Github discussions in the Ideas section are indeed the way to make a feature request. However, prioritization is another issue entirely, and it's up to the devs (or the orgs paying them to work on Superset) to make those priority calls :D
That said, it's your lucky day! @zhaoyongjie merged a PR here that solves this.",2022/6/10 4:39,6,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22484,2020/12/23 12:30,"This is something we should improve because the messaging is not clear (cc: @srinify).
These are different things: inside the database we have two types of tables, physical tables and (native) views. In SQL Lab these are represented by the table icon and the eye icon, respectively.
Once you're outside SQL Lab, Superset doesn't really care if the database entity is a physical table or a native view _ it treats them the same, because the DB abstract these two entities through a single API: SQL.
Superset has a concept that lives atop the physical tables and native views: the dataset. The dataset can be physical or virtual. A physical dataset is one that points to a ""table name"" on the DB, that could be physical or a view. A virtual dataset is one that has SQL associated with it, pretty much like a native DB view. The main difference is that the SQL defining a virtual dataset lives in the Superset main database.
So you could have a physical dataset in Superset that points to a native DB view. Or a virtual dataset that is defined by SQL stored in Superset, pointing to a physical dataset that points to a DB native view, defined by SQL stored in the DB.
In summary, at the DB level we have 2 things: physical tables and views.
One level above, we have Superset datasets, that can be: physical datasets or virtual datasets.
This is super confusing, and we should figure out a better way of communicating this. Maybe we can use native views whenever possible, so that when a user clicks ""visualize"" in SQL Lab we run CVAS if the DB supports it? And then we provide ways of fetching the view definition stored in the DB so that the user can edit it.",2020/12/23 22:53,1,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22484,2020/12/23 12:30,"Somewhat related, I find myself having to hover over the icon frequently because the icons themselves don't clearly scream ""physical"" or ""virtual"" (in the way that the eye / eyeball icon actually does imply virtual).",2020/12/24 1:22,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22484,2020/12/23 12:30,"This thread hasn't been touched in a while, but it's still a pertinent discussion! Pinging @jess-dillard here to see if there's a good way to re-route this. That said, I'm also going to close the GH Issue (it's not a bug) and move it to an Ideas thread on Discussions, so we can take up the topic there! Thanks!",2022/12/20 23:33,3,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22760,2021/6/4 14:29,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue .pinned to prevent stale bot from closing the issue.",2022/4/16 13:57,1,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22760,2021/6/4 14:29,"Appreciate these suggestions, particularly supporting penguins and/or penguin data. That said, since this Issue lands more in feature request territory than bug report territory (both of which are difficult territories to map), I've migrated it to an Ideas discussion where I hope we can gather more sentiment around it (and not gather sediment around it, which would be easier to map).",2023/1/17 21:57,2,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22877,2022/2/9 9:19,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue .pinned to prevent stale bot from closing the issue.",2022/4/16 14:57,1,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22877,2022/2/9 9:19,"@MBRGA I see this feature request hasn't gotten much attention in upwards of a year. I'm going to close it as stale, but if you're still pursuing this, I urge you to ping me here and/or propose it in a Issues thread on Github Discussions, which is where we're directing feature requests these days. Thank you!",2023/1/26 7:45,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22877,2022/2/9 9:19,@rusackas well I definitely still think it's something that many users would find valuable.,2023/1/26 13:02,3,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22877,2022/2/9 9:19,Fair enough... I'll convert this to an Ideas discussion.,2023/1/26 21:19,4,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22948,2022/2/14 22:24,"Yeah this is a less than ideal experience. We've hit it a few times internally and have informally discussed what to do about it. But for now -- Jesse, how do you feel about moving this over to Ideas https://github.com/apache/superset/discussions/categories/ideas
@geido @villebro and I have been trying to keep Github issues curated & focused on bugs or bugs to be replicated, etc. __  If you agree, you can click ""Convert to Discussion"" button in the right hand side
Tagging @jess-dillard @rusackas @yousoph who may know more about this",2022/2/15 13:37,1,1,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22948,2022/2/14 22:24,"I think it's okay to keep tasks and feature requests in issues. If we want a clean view of bugs, we can use the label filters. At any time, Engineers should always be working on a healthy mix of bugs and new features, instead of just bugs. We have tons of TODOs from the Operational Model project, which may eventually be converted into issues, too.
To me, discussions are more for user questions and big proposals that do not have a clear path forward. If you believe this issue belongs to that category, then feel free to move it.",2022/2/15 15:36,2,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22948,2022/2/14 22:24,@ktmud that makes sense. If we get good over time at identifying what's a bug vs a feature request and encouraging tagging then Issues will be an organized list of things,2022/2/16 21:45,3,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22948,2022/2/14 22:24,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue .pinned to prevent stale bot from closing the issue.",2022/4/17 21:56,4,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22948,2022/2/14 22:24,"Heya. After much discussion, we're trying to move feature requests, design discussions, and config Q&A topics over to Discussions, and put them in the right buckets. This is in hopes that we can use Issues more for bugs, and whittle things down to an actionable backlog. The alternative is to dial up stalebot and whittle that way, which would have closed this by now. I think this is still a valid discussion, and don't want to close it, so I'm tempted to move it to an Ideas thread.
I'm not sure if @kasiazjc and/or @jess-dillard have an action item on this but I'll ping them here for good measure.
For where we're at, I think this (and other items) should be moved to discussions to stay alive there and get more feedback. When we get the Issues backlog pared down, hopefully, we can revisit how we use Issues and keep things manageable.",2023/2/1 21:21,5,,(I) Non Actionable Topic
https://github.com/apache/superset/discussions/22948,2022/2/14 22:24,"This is definitely on our radar, and our current effort involves getting the ant-d table integrated into Superset as a component so we can utilize its built-in column filtering functionality so that most filtering happens inside the table itself.",2023/2/1 22:00,6,,(I) Non Actionable Topic
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"Hey @entest-hai,
You'll want to pass in a custom DefaultStackSynthesizer to your stack and tell it what deploy role you're using. Here's an example
",2022/2/8 22:22,1,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"@peterwoodworth I tried  your suggestion but still get an error

Then in addition, I pass env

still get another error
",2022/2/11 1:26,2,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,Is this possible to run cdk deploy by providing an assumed role in CDK stack rather ran configuring AWS CLI with credentials?,2022/2/11 1:30,3,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"
Hey @entest-hai,
You'll want to pass in a custom DefaultStackSynthesizer to your stack and tell it what deploy role you're using. Here's an example


Hi, I ran into this same problem and your solution doesn't quite make sense to me. Why would I want to or need to hardcode my role to use for deployment? What is the point of the --role-arn command line parameter then?
I'm doing all this is C# and downgraded to the CDK V1 Nuget libraries and using the exact same command line specifying the role-arn to use for CloudFormation and it worked 100%. I can also see in CloudFormation that the correct role was used to execute the CloudFormation template, which leads me to believe there is something wrong with the V2 implementation of --role-arn.
A side note: a policy containing my execution roles needs to be specified in when bootstrapping the CDK using the --cloudformation-execution-policies parameter. This policy is added to the cdk-hnb659fds-cfn-exec-role..... role and not the deploy role. the cdk-hnb659fds-cfn-deploy-role which is what is causing the above error. Is the deploy-role maybe used instead of the exec-role where executing CDK?",2022/2/11 9:55,4,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"So, since this BUG now turned into a discussion, can we please discuss what the purpose of the --role-arn command line parameter is and why we need to hardcode the deployment role ARN into our CDK's?",2022/4/1 6:54,5,1,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"I am also extremely confused by this. What does --role-arn do, what does the synthesizer.deployRoleArn property do, and how are they different?",2022/4/4 14:17,6,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,Also interested. I am trying to specify a different deploy role in GHA cdk action to deploy non-developer stacks.,2022/4/15 16:17,7,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"So interesting and will wait solutions from team, but found that when I user cluster.connections.allow_from(***) for Kafka I have this issue but when I do my cluster without cluster.connections.allow_from it works fine. Maybe it can help.",2022/4/25 17:21,8,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"@peterwoodworth can you please respond to these questions. The original bug was just closed and moved to this discussion after you provided a solution that does not work and it also doesn't answer any of the questions.
I'm currently faced with the issue where I have a lot of stacks that are working 100% using CDK V1, but I'm now getting messages stating that it is soon going into maintenance and I should upgrade to V2, except that converting these CDK's to V2 does not work because --role-arn is no longer working. Apart from it being completely counter intuitive to code the execution ARN into the CDK , it also doesn't doesn't work. Not even the sample application.",2022/4/26 11:45,9,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"Seems like I found temporary solution, to use --profile with role configuration in a profile instead of --role-arn . Of course it is inconvenient that it will be necessary to generate a aws profile with role before launch, but still a working option.",2022/5/5 20:07,10,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/19672,2022/2/4 13:47,"Hi there @entest-hai - I was able to get this working. Your error is that arn:aws:sts::123456:assumed-role/cdk-hnb659fds-deploy-role-123456-ap-southeast-1/aws-cdk-haitran is not authorized to perform iam:PassRole - so you need to add those permissions to the deploy role, not the CloudFormation execution role. Here's what I was getting when I tried this:

In my case, it was the cdk-hnb659fds-deploy-role-570774169190-us-east-1 role that needed modified, not arn:aws:iam::570774169190:role/test1234. This role did have a iam:PassRole action, but the Resource tag was set to the default CDK CloudFormation execution role, so that's why it was getting permission denied. If I modified the deploy role and set it like this:

it happily deployed. So I think what you'd need to do is to modify your deploy role to allow it to PassRole on your CF execution role. Now, this value is set when you bootstrap, but it looks like rerunning cdk bootstrap with a role parameter doesn't actually change the bootstrap template, so to me that seems like a bug.
In summary, I think I have a working workaround for you - and we'll confirm/research/prioritize/resolve the bug too.",2022/5/18 21:18,11,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/20049,2022/4/22 7:36,"Hey @tom10271, going to migrate this to a discussion.
Adding encryption was part of our graduating this module to stable. This was back a year ago, so before we released v2. What we did to prevent breaking existing users was to hide this behind a feature flag.
However, CDK v2 removed most feature flags. These are the only ones we still support in v2. I believe that your best path forward here is to remove the encryption property with an escape hatch
",2022/4/22 21:02,1,1,(III) Not a Bug
https://github.com/aws/aws-cdk/discussions/20049,2022/4/22 7:36,Why not just pass the encrypted property as false?,2022/4/22 22:36,2,,(III) Not a Bug
https://github.com/aws/aws-cdk/discussions/23860,2020/6/25 19:23,"This difference in behaviour is not coming from the CDK.
It's coming from either CloudFormation or the Lambda backend. In both cases, layers or functions, we treat them the exact same way by not passing the name property when not specified.
Perhaps this is something we can hide under the CDK and make it a little better for the user.",2020/6/26 13:57,1,,(VII) Information Storage
https://github.com/aws/aws-cdk/discussions/23860,2020/6/25 19:23,Internal ref: tt/0503389452,2020/6/26 14:05,2,,(VII) Information Storage
https://github.com/aws/aws-cdk/discussions/23860,2020/6/25 19:23,"This issue has not received any attention in 1 year. If you want to keep this issue open, please leave a comment below and auto-close will be canceled.",2022/6/17 16:08,3,,(VII) Information Storage
https://github.com/aws/aws-cdk/discussions/23860,2020/6/25 19:23,"Commenting; this is still a nasty little issue. Potentially it should be solved at the Cloudformation level, as this is also a problem for people using Cloudformation directly.",2022/6/17 19:02,4,,(VII) Information Storage
https://github.com/aws/aws-cdk/discussions/23860,2020/6/25 19:23,"I agree with some prior comments in this thread, it should be solved at the Cloudformation level. It's not ideal, but you can avoid this problem by specifying the layerVersionName property, or by specifying different construct ids (which will result in different logical ids).
I am going to convert this to a discussion so that people can find this info if they hit this behavior.",2023/1/26 22:41,5,1,(VII) Information Storage
https://github.com/aws/aws-cdk/discussions/23884,2022/1/11 8:35,"Try using something like:

Credit to @robertd #13310 (comment)
//EDIT: Have you tried using iam.PermissionsBoundary.of(this).apply(boundary);?",2022/1/11 14:37,1,,(V) Already Fixed
https://github.com/aws/aws-cdk/discussions/23884,2022/1/11 8:35,"
Try using something like:

Credit to @robertd #13310 (comment)
//EDIT: Have you tried using iam.PermissionsBoundary.of(this).apply(boundary);?

Thanks for sending the link, I was able to fix it! Would be great if we can find some exceptions to overrides somewhere in the docs.",2022/1/13 14:35,2,,(V) Already Fixed
https://github.com/aws/aws-cdk/discussions/23884,2022/1/11 8:35,"Hi @dannysteenman im trying to use this in my permission boundary (CDK V2 Python) where I have custom resources auto generated by cdk I need to override IAM permission boundary and prefix, but im not able to rename iam role. Have you used the above code in version2?",2022/5/3 14:31,3,,(V) Already Fixed
https://github.com/aws/aws-cdk/discussions/23884,2022/1/11 8:35,"Looks like there's room for discussion here, but that the issue itself has been resolved. Converting to discussion.",2023/1/27 22:01,4,1,(V) Already Fixed
https://github.com/aws/aws-cdk/discussions/23887,2021/4/13 23:32,"I just want to +1 this and say I tried pipelines for a day and gave up because it is so slow.  For the last year I have run cdk deploy either from my local instance or from jenkins and it is 100x faster than the pipelines.  It uses caching (for docker and npm), and is done in seconds, with the only wait being cloudformation.
I don't need a self mutating pipeline especially if it slows things down all the time.
I've seen a custom built solution (lambci) with github webhook -> lambda -> eventbridge -> lambda that run in seconds.  The codebuild provisioning time, and lack of caching by default in simpleSynth is really bad.  One of our toy projects had 14 buildAsset actions each using codebuild, just to build a few lambdas in pipelines.  That is insanely expensive.",2021/4/14 15:21,1,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/23887,2021/4/13 23:32,"Re: CodeBuild provision time - I've been annoyed by this issue as well, and was able to reduce it significantly (~250s -> 30s) by using a different buildImage than the default LinuxBuildImage.STANDARD_1_0 (not sure if this applies to any CDK pipelines jobs, but for custom build jobs this was great help).
Also - it would be amazing to merge all asset deployments into the same job if possible. Running a separate one for each zip/image you want to upload is a bit overkill IMO.",2021/4/17 12:29,2,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/23887,2021/4/13 23:32,I already created the PR to merge all asset deployments into one job. It awaits review.,2021/4/17 13:26,3,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/23887,2021/4/13 23:32,"For the reference - that's #13803
Regarding the CodeBuild image in CDK Pipelines: The image aws/codebuild/standard:4.0 is used. According to AWS docs this should be cached. We would probably see a lot higher provisioning time if it was not, since the image itself is almost 9 GiB. I checked one of my recent assets jobs and it took 19 sec for provisioning - still high in my opinion though. A custom image with FROM alpine still takes 17-18 sec to provision in CodeBuild.

Examle used to test provisioning time of CodeBuild

Dockerfile


",2021/4/17 18:04,4,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/23887,2021/4/13 23:32,"I love and support this discussion, and I think it's a good one to have. For the record, I also don't know what the best solution would be, but if you come up with tangible solutions that fit into the Pipelines framework, then I'd love to hear them.
If your conclusion is that CDK Pipelines needs to work drastically differently, I think a better solution would probably be to start a competing construct library and publish it. If yours gets more popular than ours, we can always re-asses to make that one the default :).
In any case, an open discussion ticket is not what our issue tracker is for. This is not a forum. Try the Slack channel instead, or GitHub Discussions, or the AWS Forums.  This issue is not immediately actionable, so it doesn't belong here.
I will keep it open to encourage the discussion, but we're not likely to move on this ourselves soon.",2021/4/21 10:25,5,1,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/23887,2021/4/13 23:32,"Hi, do we any update on this issue, can we make our cdk pipeline fast by avoiding assets build steps for those assets which have no change?",2022/7/18 10:54,6,,(IV) Further Discussion
https://github.com/aws/aws-cdk/discussions/23887,2021/4/13 23:32,"This issue doesn't seem to have a viable set of action items associated with it so I'm going to convert it to a discussion to retain the good tips in here. If you believe this change was made in error, please feel free to open a new issue.",2023/1/28 0:55,7,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"The main challenge with something like this is that it is necessarily glyph-specific and requires different work for different glyphs, or may not be applicable to many glyphs at all. This is unlike jitter which is completely generic and can be used to jitter any coordinate or number spec value at all without having to know anything about what it is jittering.
This specificity is not, in and of itself, ""bad"" . But it does make the task of figuring how to fit something in to the existing API somewhat harder. I.e. we can't just have a transform like Jitter because transforms can apply to any numeric property, and this new things definitely can not. It could only apply to coordinates circles and any other subset of glyphs that are taught how to behave this way. That restriction needs to be obvious and/or enforced in the API to keep users in the pit of success.
@allefeld Do you have any concrete thoughts on how this might be ""spelled"" in the API? It would be really helpful if you could provide proposed code samples that you would like to be able to write.",2021/3/26 17:39,1,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"I guess my offhand initial thought is that things like this might be better suited for higher level tools on top of Bokeh eg. Holoviews or Pandas-Bokeh or Chartify, because they operate at a better level of context to coordinate things like this.
cc @bokeh/dev",2021/3/26 17:43,2,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"I first posted an incomplete version, sorry, see completed version now.
I agree, this only really makes sense for circle glyphs, and only for the combination of a quantitative axis (DataRange1D) and a categorical axis, or no other axis (a single 'category').
I'm only getting into the Bokeh API right now, so I don't have a deep understanding and can't really comment, especially not on the JS side.
How I could imagine it, on the Python side: It could be a method on GlyphRenderer, which throws an exception if the conditions are not met (see the first few lines in my Python version). Assuming the RangeEvent we discussed is implemented, this method could be used in a callback, so that the stacking is automatically updated if the ranges are changed, and therefore the relation between circle size and axis scales.",2021/3/26 17:49,3,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,I agree with Bryan here. The amount of such tweaks is practically infinite and often only a handful of people needs a particular one of them. Bokeh already has everything a user needs to implement it by themselves via a custom Transform subclass.,2021/3/26 17:51,4,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"Something that might be more reasonable at the Bokeh level: A Python-only function that takes the concrete coordinates for a bunch of circles, and returns ""splutzed coordinates""  [1]

I'd be OK with that. But it would have limitations to aware of, namely that his is a one-time, up-front operation. If it is necessary to accommodate dynamic, changing data (or sizes), then that would only be an option in Bokeh server apps were splutz could just be called again.
[1] need a name for this, obviously, but I am not sure what it would be",2021/3/26 18:03,5,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"FWIW, I don't think it's worth it to introduce new things that alienate Bokeh and BokehJS even more.
Plus, as soon as you create such a function and people start using it, you will undoubtedly see new feature requests targeted at Bokeh to somehow expand this API or invent a similar function or make it available on the JS side. All while they would have always been able to solve all such problems with a custom Transform and Expression sub-classes.
I think the effort would be better spent in creating a small and well documented library of such sub-classes that would also be used in the gallery, just to show what's possible. Another step would be to make the NPM dependency for such sub-classes optional when the JS code is written if a form that's usable as-is. Then it would be perfect, IMO.",2021/3/26 18:13,6,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"
FWIW, I don't think it's worth it to introduce new things that alienate Bokeh and BokehJS even more.

I don't have especially strong feelings about this particular ask, but I will say in general that I think the best lens to view things through is that of ""BokehJS is ground truth and everything else is merely a language binding"". For any language (Python, R, Scala, whatever) there should be close parity at the models level. But I also think any language should absolutely build more idiomatic, convenience or sophisticated API on top of that as appropriate. We abandoned bokeh.charts because it was over-ambitious and we didn't have resources, not because it was a bad idea, per se.
I'm generally sympathetic to the idea of offering convenience free-functions for purely data preparation when the data preparation is 1000x simpler to do in Pandas, e.g.  These are just tools in an adjacent toolbox, to reach for if they happen to be useful, isolated back-box functions that are simple to test (because they have no interaction with models or BokehJS). What I am __ is new models that don't / can't play nice with existing patterns, adds to the existing web of model testing, or model-manipulating API that is easy to accidentally or unintentionally misuse.
A good example of this is hexbin. I don't think HexTile would get much or any use except that we provide hexbin as a convenience. Similarly all the CDS data-prep conveniences around Pandas and Groupbys. Neither of those has direct BokehJS analogues but it is good that they exist.",2021/3/26 18:52,7,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,I should also say: I think the proposal to better demonstrate Transform and Expression is very worthy (but should go in a separate issue),2021/3/26 19:20,8,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"Sounds useful! I don't have much opinion about whether this functionality belongs in BokehJS, Bokeh Python, or outside of it, but would like to challenge the idea that it's restricted to circle glyphs. I'd say that instead it's only likely to be useful for statially compact glyphs, i.e. shapes for which a circular bounding region is a good approximation to the shape itself.  For circles that criterion is trivially true, but I think it's also true for all of the typical markers in Bokeh:

If I fit a bounding circle to each of these and ensure there is no overlap between those bounding circles, I think that's a useful transformation regardless of marker shape, at least for the typical shapes of markers as above. With that in mind, even for circle glyphs one might want the bounding (keep-away) region not necessarily to be identical to the circle radius itself, to enforce a small whitespace border even between circles, at which point there's even less reason to distinguish between circles and other marker shapes.",2021/3/26 19:34,9,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,@jbednar my example code above refers to markers in general :),2021/3/26 19:39,10,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"Thanks everybody for considering my proposal. I can see that you don't consider this a good fit for the core of Bokeh, and I understand.


@bryevdv: splutz

Maybe I misunderstand, but the problem with that one-time approach is that the scales are needed. I can always preprocess the data I pass to Bokeh, but in order to achieve the effect of not-overlapping-but-touching I need both the scale ranges and the size of the plot box, and they are not available before the plot has been displayed.

@jbednar: to enforce a small whitespace border even between circles

Yes, I have done that in my Python example implementation. It's also true that this can be extended to every marker that is roughly spherical, not just circles, but I think the effect is less convincing.

Regarding Transform sub-classes, that sounds to me like the right approach. But I have to say, it is very hard to figure out how to do things, because the API is extremely complex, and the reference documentation tends to be cryptic if one doesn't already know one's way around:

That leaves me completely clueless on how to implement and use a Transform subclass. A demonstration would definitely be helpful.
What would also be helpful would be more documentation of the architecture, how the different pieces fit together. https://docs.bokeh.org/en/latest/docs/reference/models.html goes into that direction, though it is very short. _ But I realize that's a lot of work.
Easier to realize would be improved navigation. For example, if I look at the Reference page for renderers, https://docs.bokeh.org/en/latest/docs/reference/models/renderers.html, it is quite long, and there is no in-page navigation. If I scroll down, I get lost in a long list of methods and properties, many of which repeat. Every class has js_event_callbacks, js_property_callbacks, subscribed_events, etc., and the information that name does not come with uniqueness guarantees is repeated 12 times on the page, every time with a big colored box and the same example code. On the other hand, finding the actual classes is hard when scrolling though.
Some ideas to improve that:
_ An in-page navigation panel on the right side, like it already exists in the User Guide.
_ Colored boxes for the class signatures, not (or less prominently) for notes.
_ Class details are collapsed by default and can be expanded if needed, like in the holoviews API documentation.",2021/3/26 20:13,11,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"
On the other hand, finding the actual classes is hard when scrolling though.

@allefeld I tend to agree with you but please know that other users have complained loudly (vociferously, even) in the past that it's not possibly a real reference guide, that it is worthless in fact, unless it includes absolutely every minute detail up to any point of repetition. So we no longer ""factor out"" base class properties or methods. I guess there is no possible pleasing everyone. The next release uses an updated Sphinx theme that affords a better right hand page navigation menu, though the reference guide may need some active editing to make the best use of it.

js_event_callbacks, js_property_callbacks, subscribed_events

Just FYI these should really all be hidden/private but could not be made so, for uninteresting technical reasons.
cc @tcmetzger re: docs ideas (separate issues should be branched off as appropriate)",2021/3/27 23:01,12,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"@allefeld Information about creating custom extensions (subclasses) is in the ""Extending Bokeh"" chapter of a the user's guide:
https://docs.bokeh.org/en/latest/docs/user_guide/extensions.html
I have to be frank and state that creating custom extensions is an advanced topic to begin with, and I think even amount that, this would be on the more difficult end of the spectrum.
To be honest, I don't actually see an especially clean path to this with an expression or transform. The cleanest, most isolated way to implement this would probably be a new scatter marker type. In the past with ~20 individual marker models that would have been a nightmare. But now that all markers are really just one Scatter model, it would not be quite so bad to add a single DodgedScatter or whatever. I think that would address @p-himik concern about python/JS parity as well.",2021/3/27 23:19,13,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"@bryevdv, it goes without saying, but maybe should be said anyway, that this criticism doesn't take away from the amazing work you guys have done, and compared to many other complex projects, your documentation is extensive. Would it be helpful if I create another issue for the documentation-related proposals?
I like the idea of a DodgedScatter, but I'm not sure I'm up to the task to implement this on the JS side. I've worked a bit with JS, but I'm not by far as comfortable with it as I am with Python.",2021/3/28 16:28,14,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"@allefeld please don't read too much into my previous comment :) I think your docs ideas are good, but yes, for better task management I think they ought to go into dedicated issues.",2021/3/28 17:06,15,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"@p-himik do you have any comments about adding a new specialized scatter marker?
BTW I also wanted to mention another motivating case for my position above: contour plots. I would very, very much like it to be possible to create real contour plots with Bokeh. But this will always only be a ""python side data prep"" kind of thing, because of the extensive code and compute required. But even though it is ""technically possible"" now for anyone to create contour plots using MultiPolygon, realistically absolutely no one ever will without some helper API to make it much, much easier. I don't think adding compute-only helpers alienates Bokeh and BokehJS (as long as the helpers do not deal with models directly)",2021/3/29 17:38,16,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"I am very predisposed against piling up features. Features are endless. And Bokeh has a scope. Things in this area aren't invented that fast - how often do you see a new plot type being created, or a new web standard being widely accepted that would be greatly beneficial for Bokeh?
Sometimes libraries become complete, and that's a great thing and a great goal to aim for. It's not an unattainable ideal, it's an achievable goal.
This is a nice and a very short read on the topic: https://drewdevault.com/2021/01/04/A-culture-of-stability-and-reliability.html
As far as I can tell, Bokeh gives everything someone needs to create such a marker themselves. I haven't checked in ages, but perhaps the documentation in that department can be improved, but that's about it. Creating a new marker is a trivial task.
The counter-argument to that might be that there are Bokeh users that don't know JS and that would still like to have a custom marker or any other piece of functionality that requires custom JS code. To that I can only offer using support forums or hiring someone with enough skill to do it or to figure out how to do it.
And if someone creates such a marker/expression/transform/whatever, and publishes it with a nice permissive license, and tests it, and shows that there's at least some interest from users other than the maintainer itself - only then I would considered adding such an extension to the core library.",2021/3/29 18:35,17,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"On contour plots - I would add this feature to the core library. Only because contour plots have been around virtually forever and are still actively used. Whereas this particular feature request is just three days old. And I don't recall seeing such plots all that often (I might be completely wrong, but then someone needs to show it).
But before adding contour plots, I would make sure that I've completely exhausted all viable possibilities of making most of the stuff happen on the BokehJS side. As an example, if I'm not mistaken, a similar argument has been made about {v,h}stack quite some time ago now - the code ended up being only on the Python side. Well, about a year ago I found out that it's not that hard to implement it on the JS side as well.",2021/3/29 18:43,18,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"
And I don't recall seeing such plots all that often (I might be completely wrong, but then someone needs to show it).

This is one variation of a dot plot, and this kind dodging is described explicitly in The Grammar of Graphics (1999) and probably earlier. I've definitely encountered this sort of plot in more statistically-inclined settings over the years with some regularity (e.g. it's built into ggpot geom_dot afaik).

I am very predisposed against piling up features. Features are endless.

I am also similarly predisposed, by-and-large, trust me :)  But I am also not opposed to applying experience and judgment to ascertain the amount of risk and effort involved and input those factors in the the calculus as well. In this case, for instance, all changes would be isolated to a new code in single new class that does not need to interact with anything else or (most importantly) require changes elsewhere. That observation goes a good distance for me in this specific case.

The counter-argument to that might be that there are Bokeh users that don't know JS and that would still like to have a custom marker or any other piece of functionality that requires custom JS code. To that I can only offer using support forums or hiring someone with enough skill to do it or to figure out how to do it.

That is very specifically the argument, and I would claim that if the burden is ""users have to write custom JS extensions"" then 99% of the time that means Bokeh just won't get used. But also getting back to scope: the scope of Bokeh includes explicitly affording browser plotting to Python folks without having to know JS (at least to rough order).

Well, about a year ago I found out that it's not that hard to implement it on the JS side as well.

I'd love for you to elaborate on this (in a new separate issue). AFAIK the ""issue"" with vbar_stack is not so much that it is python-only, i.e. it would be trivial to implement vbar_stack in BokehJS. But rather that the stacking relations, once set up, are a pain to reconfigure (from either Python or JS).
Regarding contours, we are already looking at optional C or C++ packages to support it. The best I could imagine doing in JS is implementing a Marching Squares implementation. But it would probably be too slow, and worse, Marching Squares is a purely local algo that generates a pile of disconnected segments rather then actual iso-lines that could drive MultiPolygons. But this discussion should go in #8360 probably",2021/3/29 19:09,19,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"
this kind dodging is described explicitly in The Grammar of Graphics. I've definitely encountered this sort of plot in more statistically-inclined settings over the years with some regularity.

Then I yield to your expertise. :)

In this case, for instance, all changes would be isolated to a new code in single new class that does not need to interact with anything else or (most importantly) require changes elsewhere

Sure, that's a good thing. But it still results in extra code that maintainers need to support. Markers have been rewritten before. There more markers there are, the more time each such global change will require, the more there's a chance to introduce new bugs.

99% of the time that means Bokeh just won't get used

And that's perfectly fine! If there's a better tool for the job, it should be chosen instead of Bokeh.
To be fair, this very same argument is true for every single library out there. Every single one of them lacks a feature that someone needs. The difference that really matters is that some libraries offer good extendability mechanisms, and some don't.

the scope of Bokeh includes explicitly affording browser plotting to Python folks without having to know JS

If it involves a widely used feature that's somehow not yet in Bokeh then I agree, because the situation is exactly the same as I've described above with the contour plots.
My whole point is that if a FR is a random ""I want X and Bokeh doesn't have X but plotting library Y has X"", then it should be left for an extension rather than the core. This particular FR seemed to me that way - after all, a bit strange to see such thing not having been implemented after so many years, given how simple it is. But your experience shows otherwise, so I concede. In this case it's not ""an extra feature"" then but rather ""a missing feature"".

I'd love for you to elaborate on this (in a new separate issue). AFAIK the ""issue"" with vbar_stack is not so much that it is python-only, i.e. it would be trivial to implement vbar_stack in BokehJS. But rather that the stacking relations, once set up, are a pain to reconfigure (from either Python or JS).

When and if I'm involved with that code again that uses vbar_stack, I'll definitely create an issue and a PR. IIRC it would have to be a new marker type exactly so changes of the CDS can be propagated properly.",2021/3/29 19:29,20,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"The question whether this has a place in Bokeh is obviously up to you guys. But, rejecting it because you ""don't recall seeing such plots all that often"" is a bad argument. People necessarily stick to what plotting packages offer them, however good or bad that is. Random jitter is much easier to implement, therefore it's everywhere _ I'd say it's a poor man's dodging. Both are intended to solve the problem of overplotting, but jitter doesn't do it properly.
By contrast, while contour plots in Bokeh would be nice, I don't think it's that important. One can use measure.find_contours from scikit-image, and then simply plot the lines with Bokeh. Similarly, I don't think histogram computation or kernel density estimation belong into a plotting library, because they are statistical data analysis procedures which can be separated from the plotting. Stacking circles cannot be separated from the plotting.",2021/3/29 19:38,21,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"
rejecting it because you ""don't recall seeing such plots all that often"" is a bad argument

Of course, that's why I'm not rejecting it and keeping on discussing it.
If I were the only dev member here, I would simply expect or ask for a justification that's not just ""this is something that I need"" but rather a proper description of the method, how widespread it is, what value it has. All proper enhancement proposal stuff.",2021/3/29 19:44,22,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"
By contrast, while contour plots in Bokeh would be nice, I don't think it's that important. One can use measure.find_contours from scikit-image, and then simply plot the lines with Bokeh.

Sure, you can get contour lines like that, but often what people are after for contour plots are labeled contour lines (holoviz/holoviews#4494, https://gitter.im/pyviz/pyviz?at=605cdd99563232374c43874e), which do require JS-level support or some serious hacking.",2021/3/29 20:16,23,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"@allefeld

One can use measure.find_contours from scikit-image, and then simply plot the lines with Bokeh.

I am afraid it is not quite so simple:

Output contours are not guaranteed to be closed:

MultiPolygons expects close polygons, and you want to use MultiPolygons rather than lines:

in order to be able to fill iso-band with colors or hatching
to be able treat entire iso-band as a logical unit, e.g. for selection/highlighting (even if disconnected)

Regarding:

But, rejecting it because you ""don't recall seeing such plots all that often"" is a bad argument.

I mean, it sort of is. There is always more work than people to do it, which mean prioritization is impossible to avoid. A reasonable prioritization that many projects employ is ""what will have the most impact per effort for users?"" [1] In this case, I think it is reasonably common and impactful but was avoided until now because the existence of ~20 individual marker glyphs made the effort too high. Now that there is just one Scatter I think that calculus has changed.
[1] devs are also human beings, so absent dedicated funding, ""This is just something I would enjoy spending my limited personal free time on"" is another perfectly valid criteria.",2021/3/29 20:17,24,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"
another perfectly valid criteria.

Of course. I was only protesting the suggestion that this is a weird thing that nobody ever uses. I think it would be appreciated by people with a stats background like myself.",2021/3/30 13:22,25,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"I've updated the title to reflect the recent thoughts. I'm leaning towards marking as feature but we need a a little more information to flesh out a proposal. First: What is the full range of applicability for this glyph? The use case above of distributions on on categorical axes is certainly a common one. But cases like this also seem applicable (these are also explicitly described in GoG):

Can one glyph cover both these cases? (I think so, but what are any specific considerations?)
Second: what is the actual API / set of properties that a dodged scatter has? e.g. I have to presume there are at least a few tunable parameters to control how the dodging is done, what are these? What does proposed usage look like
@allefeld This is where you can help greatly by working up a proposal for the code you would like to be able to write.",2021/3/30 21:53,26,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"Range of applicability
The idea behind my proposal is that we have a scatter plot (to see the actual data, unlike a histogram or a KDE), but we want to avoid overplotting. If the data to be scatter-plotted consist of two numerical values (standard 2d scatter plot), then avoiding overplotting by dodging means one has to modify the data to be displayed, which goes contrary to seeing the actual data. (In that case I would rather go for the Datashader approach.)
If one dimension is categorical, then adding offsets to the categorical data does not change the category. That's why I restricted my implementation to the case where there is one categorical scale and one numerical scale. (Actually, I restricted to linear scale, but there is no reason not to also allow logarithmic etc.)
A possible extension of that approach that I see is to allow two categorical scales. In that case, the standard positions would be on the intersections of a 2d lattice, and dodging to avoid overplotting could be applied in both dimensions without changing category information, leading to clusters around the lattice intersections. I'm not sure how useful that would be, because there would be no information loss in just counting the number of data points per combination of categories; but I don't see anything wrong with it either.
The GoG plot
I don't think the plot from GoG is a dodged scatter plot, though it looks similar, because the vertical axis is labeled 'count' (though the tick labels are not counts?). This means that this plot is actually a variant of a histogram, with the difference that stacks of dots are used instead of the usual bars. The advantage could be that the discrete nature of counts is visually apparent, I like the plot for that.
In this interpretation, there is no dodging involved. The displayed data is a list of tuples (mpg, count), e.g. [(10, 2), (13, 1), (14, 1), (15, 5), _], as apparent from the axis labels, and each data point is visualized with zero or more circles.
Of course the same effect could be achieved with a dodged scatter plot, by feeding in only the mpg values in multiples, e.g. [10, 10, 13, 14, 15, 15, 15, 15, 15, _]. But that would be a case of numerical combined with categorical, where there is only one category. And there would be no 'counts' axis, but only offsets within this single category.",2021/3/31 19:44,27,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"Tunable parameters
I assume that we're modeling on the Scatter glyph, which can be created from the plotting interface via the scatter method. I think we need two parameters in addition:
dodge_direction: direction towards which dodging occurs. Possible values are 'centered' (like in my implementation), 'increase' to only use dodging values >= 0, 'decrease' to only use values <= 0. Defaults to 'centered'.
This is what 'increase' looks like:

To make it more user-friendly, 'left' & 'right' and 'top' & 'bottom' could be aliases depending on whether the categorical axis is horizontal or vertical.
If we allow two categorical axes, I'm not sure anything other than 'centered' makes sense. A possibility I see is that this parameter could be either 'centered' or an angle (0 _ 2 pi). Then 'left', 'right', 'top', 'bottom' would be aliases for pi, 0, 1/2 pi, 3/2 pi.
marker_sep: space left between adjacent markers, in screen units. Defaults to 1 pixel.
For a circle marker, where the size property is the diameter in pixels, the algorithm would position markers such that the distance between the centers of any two markers is >= size + marker_sep.
If as @jbednar proposed other markers are allowed, then it needs to be clear that for them size is the diameter of the smallest circle enclosing the marker, or size needs to be adjusted depending on the marker type.
If we allow markers of different sizes, then the minimum distance becomes size1 / 2 + size2 / 2 + marker_sep.",2021/3/31 19:48,28,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,"Algorithm
My implementation positions (dodges) markers in the sequence they appear in the data. It is based on a search over predefined possible dodge values (in pixels):

Here fr_scaling is the distance between adjacent categories in pixels. I hard-coded marker_sep to be 1, i.e the -2 here would be -2 * marker_sep in general. The array xcs has the form [0, 1, -1, 2, -2, _].
For each new marker to be positioned, it calculates the squared distance between already positioned markers and the possible offsets for the new one, and then the minimum across already positioned points:

Due to some surrounding logic, at this point the dimension along which dodging is applied is x and the numerical data dimension is y. xo and yo are the coordinates of the already positioned points in pixels. xcs are the possible x-coordinates for the new marker, and y[i] is its given y-coordinate.
The admissible x-coordinates are those for which d2 is >= threshold = (size + marker_sep) ** 2:

The optimal choice is the first entry in the resulting array:

The same logic could be used for the 'left' and 'right' options of dodge_direction by simply preparing the xcs array differently.
A problem occurs if there are so many markers that at some point there are no admissible offsets anymore, i.e. xc is empty. Here is my approach:

Limitations
1) My implementation uses a pre-defined array of possible offsets xcs. I did it this way because it allows to implement the positioning straightforwardly using NumPy array operations, which also makes it relatively fast (~ 60 ms for 1000 points). The drawback is that it can't easily use sub-pixel positions, and that it is not clear how this would be generalized for the two-dimensional case (two categorical axes). I'm also not sure how this would map onto JavaScript.
2)   My implementation positions markers one-by-one in the sequence of the data. This can lead to strange artifacts. For example, if the data are sorted and many differences between subsequent values are lower than the marker size, this leads to 'tendrils':

Better would certainly be a global optimization approach, but that would be very costly. I have some ideas on how to improve this using cheaper tricks, if you are interested. On the other hand, the tendrils as an indication of being-sorted could also be seen as a feature.",2021/3/31 19:52,29,,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/11382,2021/3/26 17:26,Going to move this to a GitHub discussion for further discussion and a new issue can be opened if/when we reach agreement on a concrete implementation/proposal,2021/6/24 17:59,30,1,(IV) Further Discussion
https://github.com/bokeh/bokeh/discussions/12032,2022/3/3 7:45,"I've never heard of or used ""Create new view for output"" what is it meant to do?
Also, the linked issue does not appear related?",2022/3/3 12:58,1,,(VI) Unrelated Repository
https://github.com/bokeh/bokeh/discussions/12032,2022/3/3 7:45,Related issue and discussion holoviz/pyviz_comms#3,2022/3/3 15:00,2,,(VI) Unrelated Repository
https://github.com/bokeh/bokeh/discussions/12032,2022/3/3 7:45,"@philippjfr i see. Very off the cuff speculation:
First, I assume there is some hook in the process that we can make use of to customize or control the behavior? If not, options will be very limited in general. But assuming there is:


Standalone content w/o notebook comms is almost certainly doable. It will be simpler if there is a hook that could call Python code, since then it could just do whatever show currently does to be able to make show(p) workable in two successive cells. But even a JS hook could probably make things work.


Bokeh server content also seems workable, at least in the sense that the new output area should be able to just open a new session easily enough. But the two views would not be linked. Maybe you could re-open the existing session if there is a python hook to call server_session appropriately. Again, it really depends how much control we can insert.


I don't know how to make push_notebook work. There is only one notebook handle, it's only returned to Python, and it can only be used to update one output cell. push_notebook was always a bit of hack to begin with, that only existed before the Bokeh server was viable. So if Bokeh server sessions van be made to work, I think that is the priority and we just declare push_notebook explicitly incompatible with this JLab feature.


But I should be clear: having the different outputs be ""linked"" is probably not feasible. Even in the standalone case, e.g. if you pan one plot, I would not be expect the other plot to pan along as well. They would be independent copies. (That's actually the number 1 reason I can't picture push_notebook being workable). The only circumstance where complete synchronization might be feasible is in the  Bokeh server app case, if existing sessions can be embedded (instead of creating a new session).",2022/3/3 17:56,3,,(VI) Unrelated Repository
https://github.com/bokeh/bokeh/discussions/12032,2022/3/3 7:45,"Given that there is so much uncertainty and unknowns around both the requirements and what is possible from the JLab side, I am converting this to a development discussion",2022/3/3 18:07,4,1,(VI) Unrelated Repository
https://github.com/bulletphysics/bullet3/discussions/3886,2021/6/7 10:45,It depends on the details of the setup. Turning this into a discussion.,2021/6/7 15:13,1,1,(I) Non Actionable Topic
https://github.com/bulletphysics/bullet3/discussions/4122,2021/12/20 16:04,"You might want to use the discussions: https://github.com/bulletphysics/bullet3/discussions
In the meantime can you exactly reproduce your installation steps plus the command you use?",2021/12/27 6:32,1,1,(I) Non Actionable Topic
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"This is a recurrent misconception. There are two prefetch at play here. One is for the Main worker process and the second one is for the worker threads (or spawned processes)
We can take a step back to remember that a Celery worker has a main process which is the one that takes messages from the broker. In this case your output is showing the tasks that each worker is pulling from the broker.
Then, each worker main process spawns multiple processes and each task that's pulled from the broker is then sent to these spawned processes.
Using -O fair makes the main worker send tasks only to spawned processes that are free and not send as many tasks as the spawned processes can take (effectively making the spawned processes do another layer of prefetch). When I say ""send tasks to the spawned processes"" I really mean that the main process writes these messages to the pipe assigned to the spawned process. So, without -O fair the main process writes as much data to the pipe as possible
All of this means that if you want to make sure workers are not too greedy you have to play with the prefetch multiplier
So, if you want to make sure every worker only prefetches as many tasks as the worker's spawned processes can take care of then use -O fair and set worker_prefetch_multiplier to 1",2020/11/26 17:30,1,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"Thanks so much for the info!
Sadly, we've already set -O fair and worker_prefetch_multiplier to 1.  What I am saying is this Using -O fair makes the main worker send tasks only to spawned processes that are free is not the case.  Our workers are pulling tasks from the broker and reserving them even when all the worker's processes are full.
In my original message under the celery inspect active and celery inspect reserved we can look at this worker celery@query-runner-celery-datalake-interval-refresh-784bb858c8-qskvj
Active

Reserved

This worker pulled a task from the broker even when it already had 4 processes running (I have 4 worker processes per worker node).  This is exactly what we don't want to happen, it makes that reserved task wait a long time before it starts executing when there are plenty of other empty workers waiting for tasks.
What I would like to happen is for the worker to wait until one of it's processes is finished, then go look at the broker and grab a task.   I want to completely disable prefetching.",2020/11/30 16:19,2,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"Oh I think I get what you're saying.  -O fair only affect the scheduling of worker main processes to worker threads, while prefetch applies to scheduling from broker to worker main process.
In that case, we really need a way to disable prefetching altogether.  Afaik, this is not possible right?  Why not?  Is there any way I could monkeypatch or config to make prefetching only happen when the worker has a free thread?",2020/11/30 16:24,3,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"It should be possible but I don't think anyone has requested us to implement it.
What we need is a boolean configuration value that will completely disable worker prefetch.
I'm currently scheduling this for the future milestone. If you want to dive in and write a patch, feel free to submit a PR.",2021/2/24 14:17,4,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"We have also been struggling with this (workers prefetching tasks from the queue even though all pool processes / subprocesses are currently busy).
We note from the documentation that it is recommended to set task_acks_late = True, however we are not able to do this.
So we have been trying to expose the mechanism by which a worker decides whether it can accept more messages.
The code below is a loader (invoked via celery -A APP --loader path.to.SingleTaskLoader worker) which patches kombu.transport.virtual.base.QoS.can_consume to call a delegate function, which can be supplied later.
This example fetches a single message from the queue, executes it, and only then fetches the next message.
Additional logic could be packed in the delegate can_consume() towards the bottom.
It seems to work with celery version 4.4.7, prefork pool. But use at your own risk. Hope it helps somebody. Advice appreciated.
",2021/4/9 15:05,5,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"That is an interesting monkeypatch you got there.
The proper way to do this right now is to inherit from the Channel and replace the QoS implementation.
If you can come up with a patch that does this properly, we'll include this as an option.
EDIT:
Or maybe this is a suitable implementation. @celery/core-developers Thoughts?",2021/4/11 11:52,6,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"@samdoolin @george-miller If I'm getting you correctly, you like the worker to consumes messages only when it's free for processing.
I tried reproducing this with a single worker with acks_late=true & worker_prefetch_multiplier=1 and it seems to be fine (consuming only when it can)
What am I missing?",2021/4/11 12:27,7,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"@galCohen88 thank you for your reply. Our problem is that we cannot set acks_late=True (as I said above).
The implementation of kombu.transport.virtual.base.QoS.can_consume is such that the command line argument --prefetch-multiplier limits the number of tasks that a worker will reserve in addition to tasks that have been ack-ed.
Hence with acks_late=True everything looks good, but with acks_late=False it is not possible to prevent a worker from reserving additional tasks when the worker does not have any available pool processes / subprocesses to execute them.",2021/4/11 14:42,8,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"@thedrow thank you for your reply. I agree that it's a bit of a grungy monkey patch.
I could inherit from the concrete kombu channel of the transport type that I'm going to use (e.g. kombu.transport.redis.Channel), and then override the concrete QoS implementation of that channel. But I couldn't find a legitimate way to patch into the base QoS class (kombu.transport.virtual.base.QoS), which would then work irrespective of transport.",2021/4/11 14:57,9,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,@samdoolin Can you give me some context? Why acks_late can not be set to true? Is it mission critical / specific for your usecase?,2021/4/11 16:48,10,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"
Can you give me some context? Why acks_late can not be set to true? Is it mission critical / specific for your usecase?

Unfortunately our tasks are not idempotent, and so per the documentation we should not use acks_late=True.",2021/4/11 18:19,11,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"
The proper way to do this right now is to inherit from the Channel and replace the QoS implementation.

The concrete Transport class is resolved by the method kombu.connection.Connection.get_transport_cls(), and then the class hierarchy is Transport.Channel.QoS. So if I want this to work for all transports (perhaps unreasonably), I end up writing something very fragile like...

Perhaps I should instead override the setting kombu.transport.__init__.TRANSPORT_ALIASES for the one transport that I care about.",2021/4/12 13:17,12,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,Maybe that's the right solution.,2021/4/12 13:42,13,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"Hi @samdoolin, I am facing a similar issue with tasks which cannot be used with acks_late. Could you maybe elaborate on how to use your solution on a Redis broker. I have tried copying the SingleTaskLoader  class and adding --loader path.to.SingleTaskLoader  as an argument in the celery worker (with and without --prefetch-multiplier), without any result. Still tasks are in a RECEIVED state, causing it to take a while before being picked up.
More information on my issue can be found in this StackOverflow issue: https://stackoverflow.com/questions/69987419/celery-prefetched-tasks-stuck-behind-other-tasks-on-ecs-cluster",2021/11/16 12:12,14,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"Hi @Thijsvandepoll
We have been running with a Redis broker, with code for the loader similar to my post back in April (updated below).
We start up the worker with celery worker ... --prefetch-multiplier -1 --loader path.to.SingleTaskLoader. Without the loader, I think that --prefetch-multiplier -1 should force the worker to never consume tasks from the queue. With the loader, the worker is permitted to consume tasks from the queue only by the delegate can_consume(), which is configured by the Set_QoS_Delegate bootstep.
For other backends I think that kombu.transport.virtual.base.QoS.can_consume_max_estimate() might be an additional complication.
Still feels like an unpalatable hack, but it has been working for us.
",2021/11/16 14:06,15,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"Which version of celery are you using? And do you use any concurrency?  I am currently running 5.1.1 together with Flask. For some reason the --loader is not picked up. I tried pass it directly to the Celery class:

Then it indeed starts reaching the can_consume monkeypatch, as expected. However, interestingly it does consume a single task at a time. Even though the concurrency is set to 4 (-c 4). I guess you also use concurrent jobs right? It seems to be only applied on a single ForkPoolWorker for some reason. Expected is that it should work on any of them right? Any ideas?",2021/11/16 14:57,16,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"
interestingly it does consume a single task at a time. Even though the concurrency is set to 4 (-c 4)

The delegate can_consume() in my example above is intended to do that.
You should be able to inject more logic, so I guess something like this might work...
",2021/11/16 15:54,17,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"Ah I was under the impression that you also intended to have more than a single concurrent task at the same time! Anyways this indeed works as expected! The --loader argument does not work as expected, but passing it to the constructor does the job. Thanks a lot! I will test if it indeed works together with autoscaling in ECS, but so far it works great.",2021/11/16 16:07,18,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"This is a problem for me too :( We have our own more reliable retry mechanism (based on DB outbox pattern) and so really don't want the retry behaviour associated with acks_late=True.
@samdoolin

For other backends I think that kombu.transport.virtual.base.QoS.can_consume_max_estimate() might be an additional complication.

We use rabbitmq - are you saying we'd need to monkey patch can_consume_max_estimate too?",2021/11/17 12:23,19,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"@Diggsey the docstring for can_consume_max_estimate() only mentions SQS, and kombu/transport/SQS.py seems to be the only usage, so I guess not.",2021/11/17 13:08,20,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,Was thinking transfaring this issue to discussion before any concrete consensus is reached,2021/11/17 15:21,21,1,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"Hello there, I have found that the prefetch related limit takes into account the reserved tasks, but not the scheduled.
Let's suppose we have a main worker with concurrency 2, prefetch limit 1, and acks_late. The Main Process gets 1 task from RabbitMQ but it's scheduled in the future, this task goes to the scheduled queue (celery inspect scheduled). The next 2 tasks are ready to run and delivered to the worker processes (active). In this scenario, the first task is blocked until the others finish, no matter if there are other main workers ready to consume jobs. If the active tasks run for days, the scheduled tasks will be blocked for days.
@samdoolin I think the worker_state.reserved_requests does not include these scheduled tasks. Have you encountered/tested this scenario using your patch?
Edit: The scheduled tasks are moved to the reserved queue once the ETA is met. If the main process needs to consume 100 scheduled tasks until it encounters 2 tasks ready to run, there will be 100 scheduled tasks blocked in this process. After the ETA time is met, you can see the 100 tasks in the celery inspect reserved (even having the prefetch multiplier 1 and using acks_late)",2021/11/18 10:22,22,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"@samdoolin I tried your solution with RabbitMQ, but I'm unable to specify --prefetch-multiplier -1, as it complains that -1 is outside the allowed range.
Is the change to can_consume alone sufficient?",2021/11/18 18:30,23,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"After a bit of research, I have discovered that the Consumer Prefetch is a RabbitMQ feature. Both librabbitmq and py-amqp (Kombu transports) set the prefetch limit through the RabbitMQ API, never calling a can_consume method. Only the non-AMQP brokers use the virtual QoS.
In my opinion, the virtual implementation must preserve the original behaviour, and therefore we can't develop a ""disable prefetch"" feature as discussed before. However, from an AMQP perspective, the right approach is to acknowledge a task only after it has been processed (i.e. use acks_late) so the native RabbitMQ Consumer Prefetch works as expected.
ETA/Countdown tasks
The ETA/Countdown tasks implementation complicates everything a bit. When the MainProcess reserves a task with an ETA in the future, it stores the task in memory and fetches the next one. It repeats this process until it finds a task without an ETA or one of the ETAs is reached. This means that a MainProcess can reserve an infinite number of tasks if all the tasks have an ETA in the future. In case that the prefetch limit is set, the MainProcess increases its value by one every time it needs to reserve an additional task bypassing the original value of the --prefetch-multiplier flag. With a small experiment, you can see how this value changes in the RabbitMQ web interface (channels tab).
This is a serious issue no matter if you're using the prefetch limit or not. Here is a case example:

A worker reserves thousands of tasks scheduled in the next minute (because none of them met the ETA yet). When the minute has passed, it starts processing them, but it turns out that every task will take 1 hour (they're long tasks). At this point, the system looks running slow, so we manually start new workers, but the new workers do not process any task because the first worker has all the tasks reserved.

Exploring ETA solutions
IMHO, the MainProcess must not reserve tasks that can't process. Moreover, it must never bypass the prefetch limits to avoid the issues described above. So here are my two first thoughts.
A native RabbitMQ solution is the RabbitMQ Delayed Message Plugin (github, blog 1, blog 2). It has some limitations (see Github) but delegates the ETA responsibility to the RabbitMQ bypassing the current Celery implementation. I think it could be an optional feature compatible with the current Celery API (although it would require more configuration)
A general solution for all brokers would require changing the current algorithm. Instead of storing the ETA tasks in memory, they could be requeued using the Celery 'retry' feature when the ETA isn't yet met. However, they can't be requeued using the native RabbitMQ reject/nack because the tasks will be fetched and requeued in an infinite loop (see docs). With this algorithm, a task scheduled first may be processed later, but the current algorithm has the same problem.",2021/11/30 21:18,24,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7106,2020/11/24 21:15,"Yeah that's in line with what I found whilst investigating the issue.
It looks like the AMQP protocol (and therefore RabbitMQ) is simply unsuitable for ""at most once delivery"" if you also need low latency. Which is insane given that this seems to be one of the primary usecases of RabbitMQ.
It seems to stem from the fact that AMQP is a ""push"" based protocol rather than a ""pull"" based one, but doesn't implement any kind of back-pressure system.
And if you need ""at least once delivery"", it's a pain to configure correctly and the tooling for managing persistent data within RabbitMQ is just inadequate.
So, it seems the ideal use-case for RabbitMQ is ""less or more than once delivery""... Can't say I'll be using it again any time soon.",2021/11/30 22:46,25,,(IV) Further Discussion
https://github.com/celery/celery/discussions/7161,2021/11/19 3:21,"Hey @premanandaembed __,
Thank you for opening an issue. We will get back to you as soon as we can.
Also, check out our Open Collective and consider backing us - every little helps!
We also offer priority support for our sponsors.
If you require immediate assistance please consider sponsoring us.",2021/11/19 3:21,1,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7161,2021/11/19 3:21,it is urgent to me please help me.,2021/11/23 3:11,2,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7161,2021/11/19 3:21,"
it is urgent to me please help me.

I see your using Windows...
I recommend using WSL. Although Celery docs say its windows compatible, for me and others its just not stable. I get very inconsistent results, and with Redis running on Windows I actually only recieved the tasks a handful of times. It wasnt worth me trying to keep using a Windows environment. WSL will get you going in the meantime, and its not hard to install at all, Im sure someone else can comment who has had luck with Windows and Celery but I havent seen anything yet.",2021/11/28 5:16,3,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7161,2021/11/19 3:21,closing for not following any templates. beside you can start with discussion firts,2021/12/12 12:48,4,1,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,"Hey @Distortedlogic __,
Thank you for opening an issue. We will get back to you as soon as we can.
Also, check out our Open Collective and consider backing us - every little helps!
We also offer priority support for our sponsors.
If you require immediate assistance please consider sponsoring us.",2022/1/11 14:25,1,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,"Sorry. This is not enough for us for this issue to be actionable.
We need you to fill all the required information in the template and provide us with a minimally reproducible test case.",2022/1/17 9:07,2,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,"Sorry, I just do not have time atm. I just created a work around and rolling with it. For a touch of info while I am here with a little time, I have a celerybeat schedule that fires a task every hour. When I startup celery it will fire this job even tho it is not on the hour. Just drop in ur own task function to reproduce.


",2022/1/17 12:19,3,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,"Which version are you using ?
I'm running celery version 5.2 with the same config as you (I put the same config with the same backend/broker), but I tried to make it run every minutes, then at a specific minutes, and I couldn't reproduce by stopping and starting celery beat.
However of course, If i do the opposite, start beat and send a task every minutes, If  then start celery worker, multiple tasks will be received and started by this worker. This is because the scheduler  send the tasks to the broker, not to the worker. So maybe there is some tasks pending in your  queue ?",2022/1/19 1:13,4,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,"Thanks for the response!
celery == 5.2.1

So maybe there is some tasks pending in your queue ?

great suggestion, offhand I believe u r right and that I shoulda thought bout the broker. I will confirm once I get home but def think this is it.",2022/1/19 11:03,5,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,"@Smixi  Unfortunately this did not resolve. I cleared all my docker images, volumes etc. Then on the fresh build and start, Beat still fired my tasks at startup :(",2022/1/22 12:32,6,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,transferring this to a discussion,2022/4/4 8:14,7,1,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,I am having the same issue. whenever I deploy and the beat service reloads the periodic tasks are executed event when its not time,2022/4/4 20:35,8,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,"Hello everyone, same issue there. When i try to deploy docker containers with Celery, Celery beat and Redis. Celery beat tries to run tasks on the startup which should not be right. Purge not solving the problem.",2022/7/7 10:52,9,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,Same issue here. Does not fire everytime I restart.,2022/8/9 19:16,10,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7408,2022/1/11 14:25,"
I am using CeleryBeat for cron jobs. Sometimes when I start celery the tasks will fire on startup even tho its not the time I scheduled for them to fire. I cannot find any info on this issue anywhere, other than for RedBeat, which im not using. sibson/redbeat#79

I have this problem too, also on azure web app and google cloud run platform.
Do you find a solution for this?",2022/11/25 23:30,11,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7485,2022/4/16 16:43,"Hey @udbhav __,
Thank you for opening an issue. We will get back to you as soon as we can.
Also, check out our Open Collective and consider backing us - every little helps!
We also offer priority support for our sponsors.
If you require immediate assistance please consider sponsoring us.",2022/4/16 16:43,1,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7485,2022/4/16 16:43,probably its an issue with your code?,2022/4/16 17:35,2,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7485,2022/4/16 16:43,"After investigating it more, I think it might actually be something with the combination of asgiref and gevent, I added a new test to my repo where channels is not called, and an event loop is never created. My guess is that asgiref creates a temporary event loop when calling async_to_sync and that event loop isn't always closed when gevent is managing multiple greenlets in the celery process. Will flag it over on the channels ticket, thanks.",2022/4/16 19:34,3,,(II) Invalid Issues
https://github.com/celery/celery/discussions/7485,2022/4/16 16:43,converting this to discussion for now,2022/4/25 10:26,4,1,(II) Invalid Issues
https://github.com/ColorlibHQ/AdminLTE/discussions/3909,2021/8/24 17:54,"
Don't use the Bug Report for Questions, for this we have GitHub's discussions or our discord.
You can update AdminLTE 2.x to use Bootstrap 5 but this will cost much work to get it working without bugs
",2021/8/24 18:13,1,1,(I) Non Actionable Topic
https://github.com/cookiecutter/cookiecutter-django/discussions/3444,2020/9/11 8:43,"I think this is a subjective topic and not one that is very clear in the context of an application that is continually deployed. When do you bump the major/minor/patch version? Who cares about this version? Users visiting my site won't usually see the version anywhere...
What we do at work is create a tag based on the date, which looks like this v20211202.1. We bump the final digit each time a PR is merged, and it goes back to 1 every day. Looks like bump2version can be configured to support it, but it doesn't seem like the main use case.
For a library, it's obviously different, and on some projects which are distributed to PyPI, I use python-semantic-release which provides an all-in-one solution: first updating changelog & version, then tagging and finally releasing.
I don't think the consensus is clear enough to make it part of the template, but I guess it's worth keeping as discussion.",2021/12/2 19:15,1,1,(VII) Information Storage
https://github.com/coqui-ai/TTS/discussions/1161,2022/1/28 10:28,"We don't have an immediate plan to train a Hebrew model.
If you know a public dataset, you can point to it here and someone from the __ community might like to train a model.
(I move this to the discussions as it is out of the Feature Request definition. )",2022/1/28 14:55,1,1,(I) Non Actionable Topic
https://github.com/coqui-ai/TTS/discussions/377,2021/3/13 10:05,It is not a bug but mostly a training discussion so I move it .,2021/3/13 13:45,1,1,(III) Not a Bug
https://github.com/coqui-ai/TTS/discussions/377,2021/3/13 10:05,"Thanks @erogol, I've just attached the training plots as well, the loss seems to just increase from the beginning?",2021/3/13 13:51,2,,(III) Not a Bug
https://github.com/coqui-ai/TTS/discussions/377,2021/3/13 10:05,"Here are some more figures:
",2021/3/13 14:42,3,,(III) Not a Bug
https://github.com/coqui-ai/TTS/discussions/377,2021/3/13 10:05,I can't tell what is wrong just looking at it. I am not at that level yet :). So just debug things yourself and make sure things are fair and square.,2021/3/16 11:33,4,,(III) Not a Bug
https://github.com/coqui-ai/TTS/discussions/807,2021/9/13 12:20,"No, we don't need DTW (but DTW can be an improvement). The alignment network is similar to the attention module used in Tacotron. It learns how the alignment between ground truth spectrograms and the text. Therefore in training, there is no misalignment. However, if we train the model only using the duration predictor then I'd guess we'd need the DTW loss.
Hope it makes sense ?",2021/9/14 7:45,1,,(I) Non Actionable Topic
https://github.com/coqui-ai/TTS/discussions/807,2021/9/13 12:20,Moving this to discussions as it fits there better.,2021/9/14 7:45,2,1,(I) Non Actionable Topic
https://github.com/django/channels/discussions/1568,2020/11/8 19:16,"I think it'll depend on your consumer...


(I think this is better in discussions. It's not really an issue per se.)",2020/11/9 17:06,1,1,(III) Not a Bug
https://github.com/django/channels/discussions/1568,2020/11/8 19:16,"
(I think this is better in discussions. It's not really an issue per se.)

Don't seem to be able to convert just now.",2020/11/9 17:08,2,,(III) Not a Bug
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"
Let's say between our _do_keepalive() loops the connection is lost, and a publisher elsewhere sends a message. RedisSingleShardConnection will silently reconnect having missed the message.

Yes, that is true.
I see it as a tradeoff. Indeed, it is the difference between a ""Pub/Sub"" system vs a ""Message Queue"" system.
The benefit of pubsub is that it's fast and light-weight (i.e. can scale to having HUGE groups and MASSIVE throughput; that's the appeal of Redis PubSub generally). The overhead of pubsub is not much more than just the networking stack itself!
The drawback of pubsub is that it does not queue messages in-memory (well, not beyond the OS-level network stack) thus if the OS-level network connection is lost then you lose messages (as @qeternity points out above). A ""Message Queue"" solves this. Maybe we should have another ChannelLayer based on RabbitMQ? Then folks can choose the one that makes the most sense for their application.

While the reconnect logic is admirable, I'm not sure that it's appropriate in pubsub because it allows for silent missed messages.

My opinion here is that HAVING reconnect logic is better than NOT having it. It at least restores normal operation of your app server within 1 second of restarting your Redis server [1]. It cannot guarantee message delivery, but nothing can guarantee that when using Redis Pub/Sub. For those who want such guarantee, they should instead use a RabbitMQ channel layer (or the original impl if #1683 is fixed).
[1] You previously asked about my production setup. I'm using AWS ElastiCache for Redis for my Redis server. I have my app servers on the same subnet and have never seen them lose the network connection to Redis ""randomly"". The only times I've seen the connection lost is when doing a planned Redis upgrade. Not saying it can't happen... but I've never noticed it happening (based on the logs I keep). So it's not a big concern for me. Dropped messages do screw up my front-end code (which is why I spent so much time looking at #1683) but so far the PubSub impl has been excellent for us in production. I would actually consider a RabitMQ alternative for my app (we don't actually need the crazy throughput PubSub offers) but I'm more familiar with Redis so that's where I focused my solution, TBH.",2021/6/25 16:35,1,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"Redis Streams seem to handle the _ reconnect to the same key_ desire, but that would be another implementation.
Thinking we just need to document the trade-offs maybe_ _",2021/6/26 5:54,2,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"@acu192 Many thanks for your thoughts, and even more so for the pubsub implementation. I agree with everything that you say, but the issue is not so much queueing messages but rather the silent nature of the reconnect. In fact, I would not want to queue messages because I also don't want to serve stale data. In our world, if there were to be a hiccup, then we want to know about it so that we can act appropriately. We have actually experimented with the rabbitmq based layer written by the Workbench team, but we are heavy celery users and lightweight redis connections actually work better for us (hence django/channels_redis#258). I think we're in agreement, I would just like to see a mechanism by which the channel layer bubbles up reconnection events so that consumers can act accordingly if needed.
@carltongibson Not suggesting another implementation using redis streams, just thinking out loud. I also think that would be terrible and as I said above, I actually would not want to serve stale data in the event of a network partition. We should definitely document tradeoffs but I'm not sure there needs to be much of a tradeoff. We either make the reconnect logic optional, or better, we notify consumers of disconnect/reconnect events.",2021/6/26 6:39,3,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"@acu192 Btw - you may have something similar in-house but to stress our infra for consistency at scale, we built this little tool - https://github.com/zumalabs/sockbasher
In dire need of some external docs but might be useful in its current form for you.",2021/6/26 7:57,4,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"@carltongibson @acu192  so after reviewing this morning for a bit, the shards should emit disconnect/reconnect events to that channel layer. Then, I think there are two approaches that we can take:

Maintain backwards compatibility by raising an exception, which is what occurs in the current layer if redis goes away.
Send a message to all consumers on disconnect/reconnect to allow them to implement specific condition handling.

I am in favor of 2 as I think it's the better approach, with the caveat that it won't be fully backwards compatible.",2021/6/26 9:09,5,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"
I agree with everything that you say, but the issue is not so much queueing messages but rather the silent nature of the reconnect.

Ah, I see now. Yes that makes sense.
So, now I assume your idea is: If a Channel Consumer (server-side) knew that a message was dropped, it could close its corresponding websocket. The front-end would notice the closed websocket and reconnect itself. I could go for that on my site, I think it would work well. Let's call this ""Idea A"".

I would just like to see a mechanism by which the channel layer bubbles up reconnection events so that consumers can act accordingly if needed.

To achieve Idea A, I don't think your idea of bubbling-up the Redis-reconnection even is enough. It would help (and for that reason maybe we do it), but we will still need more. Two reasons I can see (bare with me... I'm kinda thinking out loud here):


A Redis-reconnect event will (likely) drop messages destined for more than one consumer, but, only one of those consumers will ""notice"" the dead Redis connection. The others may come in a few milliseconds later and find that the connection is just fine (because it already reconnected). So, to make this work we need a way to notify every consumer when any consumer finds a dead Redis connection. Are we then going to use Redis to notify every consumer? But even so, that's still not enough because...


There is an unlikely (but very much possible) situation on the producer's end. If a producer's connection dies, you probably lost more than just the most-recent message. You may have lost messages sent 2 millseconds ago that were in the network stack memory before the OS decided the TCP connection to Redis was dead.


With all this in mind... let's say Redis dies, a consumer realizes it, tries to notify every consumer of this event... but Redis is still down so it can't notify everyone... or worse Redis is down but the ""send"" function seems to work for a few milliseconds (so the consumer who is doing the notifying is tricked)... also there might be a lot of consumers all noticing this at the same time and all suddenly deciding to notify everyone else! My gut says this will be a mess.

we built this little tool

I like the name... ""sockbasher"" _",2021/6/26 16:02,6,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"A simpler way to achieve Idea A is for each producer to include sequence numbers in its messages. Then the consumers check for gaps in the sequence numbers.
But that has to be implemented at the producer-layer and consumer-layer. I don't think the logic works at the channel-layer... but need to think more.",2021/6/26 16:07,7,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"@acu192 have a look at the last few PRs I've opened. One of them implements the disconnect events. You handle all of this at the layer level directly to the consumers, bypassing any message broker.",2021/6/26 16:28,8,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"I've only been able to follow this discussion from a high level, but given our implementation drives 1000's of point-of-sale devices, a missed message is a nightmare to code for.
I think my current given set of options are:

Leave the existing Redis channel in place and suffer (what appears to be) Daphne lock ups when we have a massive disconnect/reconnect event
Move to redis pub/sub, have clean disconnects/reconnects but the potential to lose messages
Explore RabbitMQ (which we left because we had all kinds of problems in distributed mode)

Just confirming I am correct on these counts.
Thanks.",2021/6/28 13:28,9,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"@LiteWait
""All Redis data resides in-memory, in contrast to databases that store data on disk or SSDs."" ref
Therefore you are playing with fire if you need guaranteed message delivery. If your Redis server dies/reboots/whatever (or the network connection to Redis has a hiccup) it is very likely messages will be lost, no matter which channel layer you use (core or pubsub). It's a shortcoming of Redis (and a tradeoff, it's also what makes Redis so fast).",2021/6/28 15:13,10,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"The issue is slightly more complicated than that. Yes absolutely, Redis does not provide strong durability guarantees. But it is commonplace to run with at least RDB or better AOF which provides some persistence. Additionally, running replicas as we do (hence my focus on Sentinel) lowers the window for data loss tremendously. And ultimately, as long as we can engineer around lost messages (i.e. the disconnect logic and forcing a client re-sync) these issues can be hugely minimized.
@LiteWait RabbitMQ and the Workbench layer is a much better approach if you can use it. We make extensive use of Celery and they have taken an approach that is relatively incompatible with the Celery pre-fork model. This is why our focus is on highly available Redis to AND detection of any network partitions.",2021/6/28 15:24,11,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"@acu192 @qeternity thanks. What worries me isn't Elasticache/Redis dropping out, its from disconnects between Daphne and the JS client and lost messages. Granted you can't do anything about the Internet but as long as I don't have to worry about messages that haven't actually hit the wire yet going missing that would be fault tolerant enough for our application.",2021/6/28 15:52,12,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"@LiteWait

What worries me isn't Elasticache/Redis dropping out

Same with my app actually, I'm not worried about this because it's very rare and it will not kill our business in the event it does happen. We store all the things we actually care to keep in a real database. Redis is just a nice way to sync state but it is not the authority of the state.
With that in mind you should try the new pubsub impl. It should not [1] drop messages unless the Redis server (or the connection to it) dies. I've been happy with it for this reason, as the core impl was dropping messages even while Redis was healthy.
[1] ""should not"" meaning I don't know of any reason it would... and I've been testing it and running it in production for only a few months, so take that for what it's worth. We're considering it beta for now.

disconnects between Daphne and the JS client

I've been using Uvicorn (instead of Daphne) and found it's generally better (faster, more reliable, whatever). Maybe give that a try if you think Daphne is causing issues for you.",2021/6/28 16:07,13,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"@LiteWait @acu192 Absolutely you should not be using Daphne in prod (we also use Uvicorn). In terms of dropping messages between webserver and client, websockets run over tcp so you should have the same guarantees there as you do with tcp. That said, you should expect network issues everywhere. If you are using websockets as a source of truth, I think that's a mistake as you'd need to implement some sort of 2PC on top. Distributed systems are difficult, which is why we treat redis/channels as a nice-to-have real-time sync, which we expect to break and we fall back to sync'ing via api which is backed by our postgres cluster and postgres' decades of battle testing to overcome these exact issues.",2021/6/28 16:16,14,,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"
If you are using websockets as a source of truth, I think that's a mistake ...

This seems the key point. If the layer drops the connection to Redis, you're going to loose messages (independently of which ASGI server you happen to be using.) Short of a much more robust system on top of ASGI you either need to accept that the occasional message will go missing, or else periodically fallback to a more reliable method (HTTP polling the source of truth, as the first approach.)
I'm thinking this is a documentation issue?  There's a few hits for ""at most once"" in the Channels docs already. Perhaps pulling those together into a single discussion would be worthwhile. _
(See discussion on django/channels_redis#259)",2021/7/1 10:52,15,1,(V) Already Fixed
https://github.com/django/channels/discussions/1718,2021/6/25 13:52,"I'm going to move this over to discussions on the Channels repo. If we pin down something addressable, happy to move it back.",2021/7/1 15:06,16,,(V) Already Fixed
https://github.com/django/channels/discussions/1738,2021/8/12 17:01,"Hi @ckcollab -- sorry no not really. You'll need to experiment and see what works.
I think the comments on the linked issue are probably the best starting point.
If you make progress. It'll be interesting to see what you come up with.
I'm going to concert this to a discussion as I'm not sure it's actionable in its current state.",2021/8/12 17:37,1,1,(I) Non Actionable Topic
https://github.com/django/channels/discussions/1738,2021/8/12 17:01,"It was not in a channel context, but for the records, I was able to do some mocking by code like this (my use case was to avoid some requests calls triggered by the UI under tests):
",2022/10/13 15:26,2,,(I) Non Actionable Topic
https://github.com/encode/django-rest-framework/discussions/7820,2021/3/9 14:30,"Guiding you kindly towards discussions... :)

I'm not able to replicate this...
urls.py

client.py:

shell:
",2021/3/10 9:33,1,1,(III) Not a Bug
https://github.com/encode/django-rest-framework/discussions/7824,2021/3/10 12:03,"This issue does not meet our contribution guidelines.

I'll convert this to a Q&A discussion, although you probably need to be more descriptive in order for someone else to be able to help you with this. It looks like the service isn't running locally.",2021/3/10 12:12,1,1,(I) Non Actionable Topic
https://github.com/encode/django-rest-framework/discussions/7939,2021/4/8 13:58,Please check out the original discussion (#7914).,2021/4/21 23:28,1,1,(I) Non Actionable Topic
https://github.com/encode/django-rest-framework/discussions/8134,2021/8/16 19:30,"Hey @tomchristie
I'm not a pro with Django and Django Rest.
So please, confirm that issue reproduced with clear django-restframework project, otherwise i will looking more what's wrong on my side.",2021/8/17 17:26,1,,(VI) Unrelated Repository
https://github.com/encode/django-rest-framework/discussions/8134,2021/8/16 19:30,@tomchristie I found that issue is reproduced only if the image model inherit parent NOT abstract model. In other cases is all fine. Will try to talk with imagekit team. Can be closed as discussion.,2021/8/19 14:55,2,1,(VI) Unrelated Repository
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"Hey hey,
Interesting stuff, but what would be some possible use cases for this?
I'm thinking a use case of a Starlette server side app where we proxy requests from ASGI through HTTPX and responses back again. Is this discussion related to making this kind of things easier?",2020/7/27 11:45,1,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"So heres an example of something you might use this for...
",2020/7/30 15:47,2,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,Could this be used to build the httpx-equivalent of responses / aioresponses?,2020/8/2 11:17,3,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,@jcugat There's already RESPX by @lundberg which I think was intended to be such an equivalent (although I believe the API has parted ways a bit). :-) But yes this MockTransport example totally rings a bell and might be a way to make RESPX even simpler in design and implementation_,2020/8/2 11:27,4,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"True @florimondmanca , the API has parted somewhat, it's hard to keep up with the speed httpx evolves and improves, awesome work!
About the transports...latest version of respx is now implemented using the httpx transport API, and one can choose to use the respx sync/async mock transports directly if wanted. I have a small example in the docs.
I really like the idea of making the request, response and URL from httpx ""easier"" to use for an external lib @tomchristie . Currently I needed to have some wrappers in respx to fully support http_core ""alone"".",2020/8/2 11:43,5,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"What's left to be done on this? I had a lot of awkward stuff going on in my custom transport that was alleviated by using a httpx.Request internally, very welcome change. I'm planning to do the same with Responses. Am I right in thinking this is already good to go?",2020/9/28 6:19,6,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"Yup you're good to go here. There are some bits & pieces that we might still pursue later.
For example... for accessing JSON or Form encoded data on a request instance, we might want to add .json() and .form() methods.
Aside to self: If we were going all the way with supporting server-side requests, then we'd also want to think about supporting ext containing http_version/client_addr/mount_path in order to give us parity with WSGI/ASGI interfaces.",2020/9/28 8:01,7,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"Is there anything left related to building requests/responses from ""raw"" data?
Like_

I think I saw this discussed on another issue or PR, but can't remember where. I think @johtso's experiments with caching hint that there might be a use case there_? (Maybe we already support it, btw, and I'm lacking behind?)
https://github.com/johtso/httpx-caching/blob/54251ea1297c6e50deec1e6570d3da62e847cc18/httpx_caching/_models.py#L19-L24",2020/9/28 12:14,8,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"Hah, please ignore that horrid code, was written in a fit of frustration at having to repeat the list of raw request value types umpteen times.
We're all groovy for easily using httpx.Request and httpx.Response in transports.
The only things are as you mention, having to explicitly unpack the raw Request values when making the Request and having to pluck out the relevant values from the Response when returning it.
Also, this may just be me misunderstanding the definition of ext, but you can't give an ext to a Request. Would be useful to be able to do that when my code is passing the Request around internally and wants to include things in the final ext.
I really like the idea of the raw kwarg, without that I'm going to have to have even more duplication of code that describes how Transport.request gets its arguments.",2020/9/28 12:34,9,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"@florimondmanca
I'm not sure that we want to go quite that far. (Maybe, but?)
Right now we've got...

Which is probably the right balance of:

Allowing easy integration between requests/responses and the transport API.
Without being too clever and obscuring stuff.
",2020/9/28 14:55,10,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"@johtso

Also, this may just be me misunderstanding the definition of ext, but you can't give an ext to a Request. Would be useful to be able to do that when my code is passing the Request around internally and wants to include things in the final ext.

We don't really want to be in the business of passing around arbitrary stuff in the extensions. We ought to define what extensions are supported, and guide users towards not treating it as an arbitrary value store. What are you considering using it for?
I can see a use case for us adding Request(..., ext=...), but it's not something that we need as things stand.
Supposing we decided to add a support minimal API for server support to httpcore in addition to the existing client support. Unlike ASGI and WSGI which are used solely for the server-side interface_, our Transport API is actually perfectly well suitable to be used as the interface either for client -> outgoing transport handling requests or for server handling incoming requests -> app.
In that case, there's a couple of extra bits of information we'd need to pass in addition to the method, url, and headers in order to have parity with the information that ASGI and WSGI pass...

ext['http_version'] - The HTTP version with which the incoming request was handled.
ext['mount_path'] - Any path prefix that the server is mounted on.
ext['client_addr'] - The client IP address. Note that the url here would actually be treated as (scheme, remote_host, remote_port, target), and the Host header is what we'd actually want to use when constructing the URL() instance, given the raw URL.

Corresponding with those we'd also want to add the following properties to request instances...

http_version
client_addr
remote_addr
mount_path

_: Actually that's not completely true. We could feasibly be using WSGI + ASGI instead of our transport API, for the dividing line between httpx and the transports themselves, but it'd be a bit unwieldy, particularly given that the sync + async cases have different styles.",2020/9/28 14:56,11,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"@tomchristie I wanted to include 2 things in Response.ext.
An indication whether or not the response is from the cache.
The actual request that was sent by the caching transport.
Would it be so bad to allow attaching arbitrary stuff to Requests?
One use case is per request options you want to send to a custom transport.
Another use case is if for example you have some kind of multi step web scraping logic that needs context, and your handling of responses is separated from where the requests are sent. The only place that context can be is on the Response, and you couldn't just subclass Request and Response to avoid putting it in ext because the transport would need to pass it along. And even if you did we don't allow setting custom Request and Response classes on a Client I don't think..
Just brainstorming!",2020/9/28 15:31,12,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"Ah interesting...

An indication whether or not the response is from the cache.
The actual request that was sent by the caching transport.

We might want to fold that all into a test_info key, which we could also use for template_name/template_context.
(Eg. stay in line with a tentative ASGI proposal along those lines... django/asgiref#135)

One use case is per request options you want to send to a custom transport.

Yes maybe. I think we'd need to talk specifics?
I can see for example that ssl_context would allow us to support different per-request SSL configurations, which we might end up doing. But we ought to try to be as conservative as possible here.
Short version of this: Just like with any other API. Adding stuff is trivial. Removing something once added is nearly impossible. So in general we should tend towards trying to keep ext constrained for a limited set of well defined use-cases, with a clear motivation for each.",2020/9/28 16:15,13,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"

One use case is per request options you want to send to a custom transport.

Yes maybe. I think we'd need to talk specifics?

Was more of a general idea really. Potentially there could be an option to make the caching transport ignore a request, but that can also be indicated by adding a no-store header without it being an abuse of headers.

Short version of this: Just like with any other API. Adding stuff is trivial. Removing something once added is nearly impossible. So in general we should tend towards trying to keep ext constrained for a limited set of well defined use-cases, with a clear motivation for each.

I do feel like there's two separate issues.


Avoiding 3rd party libraries and httpx internals having a free for all in ext, something that can be managed by case-by-case consideration and maybe a bit of namespacing.


Not allowing users to attach something to a request that will make it through to the response, but not affect the data sent over the wire (e.g. abusing the headers).

",2020/9/28 17:54,14,,(V) Already Fixed
https://github.com/encode/httpx/discussions/1527,2020/7/27 10:18,"Going to push this into an open ended discussion now. I think we've largely resolved the points bought up here, though there's a few bits that might(?) come up later as specific issues.",2021/3/23 12:33,15,1,(V) Already Fixed
https://github.com/encode/httpx/discussions/2132,2022/3/17 13:52,"Hi @chagui, are you using mypy --strict? If so, this might be related to #2092.
How does def fun(proxies: httpx.Proxy = None) do ?",2022/3/17 15:45,1,,(II) Invalid Issues
https://github.com/encode/httpx/discussions/2132,2022/3/17 13:52,"I'm tentatively moving this to Discussions for now, since we try to keep Issues for well-defined work items.",2022/3/17 15:47,2,1,(II) Invalid Issues
https://github.com/gitpython-developers/GitPython/discussions/1425,2022/4/7 11:24,"Thanks for sharing your thoughts!
I turned this into a discussion in case there should be some brainstorming about how such API should look like with potential contributors.
Please note that GitDB, to me, is end of life, so the GitCmdDB would be the only implementation left to support.
This also means that passing --all is valid and working, even though it certainly isn't the most desirable/idiomatic way to achieve this.",2022/4/8 1:13,1,1,(IV) Further Discussion
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,cat robot$project+test.json | jq .secret,2021/3/3 4:34,1,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,docker login harbor.domainname.local --username $(jq -r .name robot$project+test.json) --password $(jq -r .secret  robot$project+test.json),2021/3/3 4:59,2,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,"Yes, I know about jq. But this tool is not installed by default on most distros. I'm distributing the robot files to third parties and it would be great if it would just work out-of-the-box instead of me explaining to them how to install and use jq correctly.
Also, your code snippets are wrong: you didn't escape $ so everything following it will be interpreted as an env var name and expanded to an empty string since those env vars are most likely undefined. That nicely supports my argument above.
Using $ in the user and filename is another design choice (orthogonal to this one) that impacts UX for no reason that is apparent to me. But I'm hesitant to open an issue about that one since fixing it impacts backwards compatibility for existing user names.",2021/3/3 10:00,3,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,"
Yes, I know about jq. But this tool is not installed by default on most distros. I'm distributing the robot files to third parties and it would be great if it would just work out-of-the-box instead of me explaining to them how to install and use jq correctly.
Also, your code snippets are wrong: you didn't escape $ so everything following it will be interpreted as an env var name and expanded to an empty string since those env vars are most likely undefined. That nicely supports my argument above.
Using $ in the user and filename is another design choice (orthogonal to this one) that impacts UX for no reason that is apparent to me. But I'm hesitant to open an issue about that one since fixing it impacts backwards compatibility for existing user names.
it works, $() - https://stackoverflow.com/questions/27472540/difference-between-and-in-bash
I think if people can use docker, they can also use jq
",2021/3/3 10:30,4,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,"Thx for re-confirming my argument __
",2021/3/3 10:36,5,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,,2021/3/3 10:39,6,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,"you are right, there itself was escaped in the process of pressing tab, I forgot ``",2021/3/3 10:41,7,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,docker login harbor.domainname.local --username $(jq -r .name robot\$project+test.json) --password $(jq -r .secret robot\$project+test.json) ,2021/3/3 10:42,8,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,"I guess from the discussion we just had now everyone can understand how guiding of third-parties can look like. Yes, we eventually manage to make it work. But the more such cases I have the more time-consuming it is.
If there's no good reason for the current file format then I'd change it to enable a better out-of-the-box experience. I'm surprised that this is controversial since it should be in the interest of the project to improve UX.",2021/3/3 10:56,9,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,"I would disagree that changing the file format would improve UX. Exporting the robot data in json you have the username and the token in one file (better for distributing), the credentials are better to parse (like the docker login example above) and with jq they are easy to read. nowadays jq is part of the standard toolchain.
Whether it is better to export the information as plain text or as json, depends then nevertheless strongly on the use case and the personal opinion. i prefer the json format.",2021/3/3 22:37,10,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14390,2021/3/2 19:09,Convert it discussion as per there are a lot of discuss,2021/3/8 8:46,11,1,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14487,2021/3/17 18:20,I am able to push docker images to public repo I need to push to harbor any suggestions pls?,2021/3/17 18:20,1,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14487,2021/3/17 18:20,"just push to your harbor instance:
",2021/3/18 12:58,2,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14487,2021/3/17 18:20,"Something like this should work:

If you're using a self-signed TLS certificate or one derived from an internal CA, e.g. as you might find in a large corporate environment, then you'll need to configure docker with the CA. But that's part of configuring docker, so not really relevant here.",2021/3/18 13:01,3,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14487,2021/3/17 18:20,"I have Jenkins running in my local and Harbor in organization server Can you please breif how to add Certificates and make connections between jenkins and harbor,Finally push the image to harbor?",2021/3/18 17:47,4,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14487,2021/3/17 18:20,Configure docker with CA is bit strucked Can you please help me on this?,2021/3/18 18:15,5,,(I) Non Actionable Topic
https://github.com/goharbor/harbor/discussions/14487,2021/3/17 18:20,I'm tranforming this to a discussion as it's not an issue that requires eng work.,2021/3/22 8:19,6,1,(I) Non Actionable Topic
https://github.com/google/flax/discussions/2086,2021/6/23 23:36,"Been exploring this structure to understand this machanic better:

Once initialized like this:

There are a couple of scenarios I tried out:

To answer the question, mutable doesn't filter the variable_upates but rather yields an error if a Module wants to mutate a collection but said collection is not in the mutable list or mutable is not True.
Documentation for apply does state:

If mutable is False, returns output. If any collections are
mutable, returns (output, vars), where vars are is a dict
of the modified collections.

However maybe a simple example might be more useful.",2022/3/21 21:24,1,,(I) Non Actionable Topic
https://github.com/google/flax/discussions/2086,2021/6/23 23:36,"@billmark I moved your issue to our Github Discussions page, since @cgarciae gave an answer, so this can immediately serve as the documentation!",2022/4/27 20:44,2,1,(I) Non Actionable Topic
https://github.com/google/flax/discussions/2714,2021/11/29 9:43,"Maybe, a DenseActivation layer which automatically rescaled the initialiser depending on the nonlinearity, if it is known?
That wouldn't work all the time, and maybe is too smart, but it would be very beginner friendly.",2021/11/29 9:57,1,,(VII) Information Storage
https://github.com/google/flax/discussions/2714,2021/11/29 9:43,"

Machine-Learning practitioners are aware that initialisation functions must be properly rescaled depending on the variance of the output of the nonlinearity.


I don't think this is still an up-to-date statement. There was a time when network training was very sensitive to the initializers because there were no skip connections or normalization.
Nowadays both of these techniques are almost universally applied and have made tuning the initializers largely irrelevant. It's been quite a while since I have seen a network that did have proper initialization correcting for the activation function.
That said, computing the gain might still be interesting, but it isn't essential like it used to be.",2021/11/29 12:56,2,,(VII) Information Storage
https://github.com/google/flax/discussions/2714,2021/11/29 9:43,"@PhilipVinc I transfered your issue to a discussion, since it seems useful for users to be aware of this, but we probably will not implement any utility functions / custom classes for rescaling initializers soon. However, if this comes up more often we might reconsider, thanks for raising it!",2022/12/12 13:11,3,1,(VII) Information Storage
https://github.com/google/flax/discussions/452,2020/9/8 9:01,"A quick workaround is to set train=False during module creation. The edge case arises because you are combining lazy init + shared batch_norm + train=True during init.
I'll have to think a bit more about how to fix this nicely.",2020/9/14 7:33,1,,(V) Already Fixed
https://github.com/google/flax/discussions/452,2020/9/8 9:01,"Thanks for your quick answer.
From your answer, I understood that lazy init is part of the issue. I therefore switched to direct init  (module.init(prng_key,x) instead of module.init_by_shape(prng_key,[(shape,dtype)])
(full updated and working colab here: https://colab.research.google.com/drive/1Cm1MIHkKBmZ-xBq21fEG6byzd1IbrGSz?usp=sharing)
It now works, but I wonder : is there any reason why I would prefer init_by_shape rather than init ? The ability to learn the batch norm is of higher importance in my case than saving the cost of an init which will be done anyhow ! I'd rather keep train=True for learning purposes, or maybe there is something I am not getting right.
For the record : the new create model in the new colab:

Thanks",2020/9/14 8:03,2,,(V) Already Fixed
https://github.com/google/flax/discussions/452,2020/9/8 9:01,"You don't need train=True during init. The problem is that with train=True you are trying to gather batch statistics (they don't exist because the init is lazy). Of course you can set train=True again during the actual train steps.
Basically train=True -> gather batch statsistics, and train=False -> use running average of batch statistics",2020/9/14 8:45,3,,(V) Already Fixed
https://github.com/google/flax/discussions/452,2020/9/8 9:01,"Thanks, your answers made everything super clear to me.
My mind was biased by Keras since in Keras, if you set the trainable = False at batchnorm creation it is not possible to come back by setting it to true since it runs in inference mode forever (at least from what I understand of this post https://keras.io/guides/transfer_learning/#do-a-round-of-finetuning-of-the-entire-model ).
Flax behavior, where the operator is the same in both mode is an excellent news. For the record, based on your proposition:
https://colab.research.google.com/drive/12Bgq0XSy-Y8G2a3HhHLaF6xKuRRQa5Z9?usp=sharing
",2020/9/14 10:31,4,,(V) Already Fixed
https://github.com/google/flax/discussions/452,2020/9/8 9:01,"Yes this works correctly although I would prefer to write it as follows for simplicity:
",2020/9/14 11:08,5,,(V) Already Fixed
https://github.com/google/flax/discussions/452,2020/9/8 9:01,"IIUC, this is no longer an issue in Linen because we no longer have init_by_shape. I'll convert this to a discussion for posterity, but if there are any related issues in Linen please feel free to open a new issue.",2020/12/12 18:31,6,1,(V) Already Fixed
https://github.com/google/flax/discussions/494,2020/9/25 6:25,"Hi @juliuskunze -- nice to ""see"" you here :) I turned this into a discussion -- if we end up with consensus on a change let's file an issue then.
During our Linen design discussions, at first we did make @nn.compact default (and always on __call__ -- note that the implementation of @nn.compact requires that only one method is annotated -- this is to avoid footguns about which submodules are shared and how things are named -- note in comparison to Haiku where ""everything is compact"" where parameter names are prefixed by method name and you need to (sometimes) use hk.transparant if you call one method from another. We hoped to avoid this complexity altogether).
So in our intermediate design, we had a distinction between Module (where __call__ is wrapped with nn.compact) and MultiModule (where it isn't). We did an internal alpha user research group and multiple people found this confusing. I suppose we could consider annotating setup with nn.noncompact but I'm pretty sure people would find that similarly confusing)
Another way to think about it, while I agree that the vast majority of modules benefit from using nn.compact that it's a bit of an educational device -- first, people learn that Modules look like they have seen before in PyTorch. And then we say, well, to co-locate concerns and make your code more concise you can use nn.compact.
Your point about wrapping existing functions is a valid one, and I've heard of people who want that kind of behavior to interoperate with existing numerical computation library. For that we could add a small wrapper e.g. module(lambda parent, x: parent.param('foo', ...)). @levskaya has been thinking about this. (and in the past @shoyer and @adarob have also asked)",2020/9/28 13:29,1,1,(IV) Further Discussion
https://github.com/google/flax/discussions/607,2020/11/9 2:26,"Hi @nikitakit, good to have you here :)
optimizer.replicated() is deprecated, have you taken a look at the patterns we have in our multi-host examples (such as ImageNet?)",2020/11/12 12:49,1,,(I) Non Actionable Topic
https://github.com/google/flax/discussions/607,2020/11/9 2:26,"(I'll convert this to a discussion, if we decide to add any new features we can file an issue afterwards.)",2020/11/12 12:52,2,1,(I) Non Actionable Topic
https://github.com/google/flax/discussions/607,2020/11/9 2:26,"You could sync the paramaters across devices and make the first device the leading one:
",2020/11/12 13:20,3,,(I) Non Actionable Topic
https://github.com/great-expectations/great_expectations/discussions/5167,2021/6/21 9:29,up this,2021/6/25 11:23,1,,(IV) Further Discussion
https://github.com/great-expectations/great_expectations/discussions/5167,2021/6/21 9:29,"Hi @imutyshev - thank you for raising this issue! It appears that this has to do with the way Windows process paths with backslashes as compared to the way Unix/Mac systems processes paths with forward slashes. It is difficult for us to prioritize Windows issues internally, but we always welcome contributions. Is this something in which you might be interested? I'm happy to offer PR review as well as any additional guidance you may need.",2021/8/10 19:34,2,,(IV) Further Discussion
https://github.com/great-expectations/great_expectations/discussions/5167,2021/6/21 9:29,"This would be an excellent topic for a community discussion. it's interesting, and something the community would be more empowered to address!",2022/5/20 16:41,3,1,(IV) Further Discussion
https://github.com/home-assistant/frontend/discussions/11278,2020/7/16 21:24,This is pretty much necessary for anyone with more than 2 people living in a house so I really hope somebody implements it.,2020/12/20 4:33,1,,(I) Non Actionable Topic
https://github.com/home-assistant/frontend/discussions/11278,2020/7/16 21:24,"In my installation, I have several users with, for each, an associated dashboard.
When a user logs out, he loses this association.
It should be possible, via the admin account, to associate a default dashboard to each user without going to each device by logging in with each account",2020/12/26 6:50,2,,(I) Non Actionable Topic
https://github.com/home-assistant/frontend/discussions/11278,2020/7/16 21:24,"I would even suggest that this could be implemented in two stages:

Allow to set a dashboard for a user under Configuration > Users, this dashboard will be used as their default dashboard. You should set all these dashboards to not show in the sidebar so the default dashboard would be virtual... This would be quite straight forward and could be implemented using the current authorisation possibilities
Allow to configure permissions at the dashboard level which would need a more capable and advanced rights system in which there are N to M possibilities for connecting users and resources, and even a third dimension to define the access level to the dashboard. This would also need to have access levels to al sensors, switches, etc. to allow a user to change the dashboard without access to the wrong resources.
",2021/2/20 14:01,3,,(I) Non Actionable Topic
https://github.com/home-assistant/frontend/discussions/11278,2020/7/16 21:24,"I'm setting up a system for my parents. I don't want them to see the default dashboard at all. They should only see a custom dashboard I've created.
I don't see the logic in clearing the default dashboard after a log out from the system. Isn't the point of something being default that it's the one that's shown to the user right after logging in?
So yes, I'd be very happy to see this being implemented. Additionally I'd like to see ""Change the order and hide items from the sidebar"" to persist between logins.",2021/10/23 12:07,4,,(I) Non Actionable Topic
https://github.com/home-assistant/frontend/discussions/11278,2020/7/16 21:24,"Until this feature is partially or fully implemented, I__ looking for a workaround.
For that reason, I started another discussion #11316 - please chip in if you have any comments to add.
Thank you!",2022/1/12 21:41,5,,(I) Non Actionable Topic
https://github.com/home-assistant/frontend/discussions/11278,2020/7/16 21:24,"I would also like to see the option for users to define which default dashboard they want to choose (among the ones available to them).  However, I have not seen a way for non-admin users to define their default dashboards. These non-admin users do not have access to the Dashboards configuration screen (which is expected). For admins, this screen allows opening a specific dashboard and setting it as ""default for this device"". However, non-admin users cannot chose their default Lovelace card in this way, as they don't have access to that configuration.
I have also not seen a way for an admin user to define which Dashboard should be the default for other non-admin users' devices (or profiles, as this discussion seeks).
The only way I have seen to go around this is for an admin to set the non-admin user temporarily to ""Admin"", then on that user's device, access the Dashboards configuration panel, open the desired dashboard, and choose ""Select as Default for this device"". Then, from the admin's account, revert that user back to non-admin.
If my suggestion, being slightly different than what this discussion is mainly aimed at, should be brought up in a separate discussion thread, please let me know and I will move it there!",2022/1/21 12:12,6,1,(I) Non Actionable Topic
https://github.com/home-assistant/frontend/discussions/11278,2020/7/16 21:24,"I agree and I'm eager to be able to set default for Me and other users, regardless of device, it should be tied to the user, not device. And if you have a wallpanel that should have a different default, just create a new user for each wallpanel ( that is a good thing for security ).
Also, even more important than setting the default, the ability to organize the order of dashboards from the backend.
When I started in HomeAssistant, I had many views inside a single dashboard, that dashboard raw file reached 17k lines. So I splitted all the views in many dashboards, now I have 32 dashboards, and it's a pain to deal with the order of them, each device has it's own. I want to organize them from the backend and have consistency.",2022/1/21 15:05,7,,(I) Non Actionable Topic
https://github.com/home-assistant/frontend/discussions/11278,2020/7/16 21:24,"Feels like being connected to #7361 and #11778
Goal would be a streamlined dashboard concept including authentication, user defaults and converting existing content to cards.",2022/3/12 22:21,8,,(I) Non Actionable Topic
https://github.com/home-assistant/frontend/discussions/8345,2021/2/7 16:02,"Are you talking about the HA addon or the HA integration? Initial gut feeling: Sounds more like a backend topic, as the integration should provide an option in the config flow to allow the user to specify the IP.",2021/2/7 16:06,1,,(VI) Unrelated Repository
https://github.com/home-assistant/frontend/discussions/8345,2021/2/7 16:02,"run.sh is about customizing a Docker image. You can do so, by building on top of our images, by using those as a base image.
This is not a frontend related feature request and thus locking this discussion topic.",2021/2/8 10:11,2,1,(VI) Unrelated Repository
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"@pmrowla's reply (#5593 (comment)):
The issue with creating a new dvc.yaml file is the same as creating any new code file for experiments. In order for any new (completely untracked) files to be included in the queued experiment, you must at least stage them with git add (but they do not need to be committed) (it looks like this is not currently documented, see: #5029). For workspace runs since the files are present in the workspace itself, this issue doesn't show up.
The alternative would be for us to explicitly include all of the untracked files in our experiment git commits. Doing so will likely result in us git-committing objects which do not belong in git (like large binary files, or things like venv dirs or pycache dirs that the user forgot to git ignore), or files which the user was explicitly not tracking in DVC or git for a reason (like authentication credentials).
@dberenbaum's reply (#5593 (comment)):
Files not tracked by Git will only be reflected in workspace experiments and not in temp experiments. Workspace experiments will use whatever is in the workspace, but temp experiments copy files to the temp dir by checking them out with Git. In this case, I think it's unclear what the expected behavior is.",2021/4/12 18:26,1,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"
the alternative would be for us to explicitly include all of the untracked files in our experiment git commits. Doing so will likely result in us git-committing objects which do not belong in git

Yes, I see @pmrowla. We could also just copy all the workspace contents to the tmp dir without ever committing untracked files, but then the resulting exp commit could be missing files needed to reproduce its results. Tricky! I guess queued/temp runs are fundamentally different from workspace runs in some ways.
Should def. document this...

temp experiments copy files to the temp dir by checking them out with Git. In this case, I think it's unclear what the expected behavior is

I don't think it's just a git checkout rn @dberenbaum, since git-staged files from the workspace are also copied over. Anyway: yeah, it's wasn't obvious at first, but now I see that we shouldn't silently include untracked files to tmp exp dirs.
Maybe adding an option like exp run --queue/temp --all-files to explicitly include untracked files (somewhat similar to git commit --all) would be best? See #5029 (comment).",2021/4/12 20:13,2,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,A related edge case in this discussion - do we capture the --local DVC config when we run experiments in background?,2021/4/12 23:34,3,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"
Before running an experiment, you'll probably want to make modifications such as data and code updates, or hyperparameter tuning."" Is this the intended behavior?

btw, not sure what is the problem with this phrase specifically?
it looks this is a duplicate/related to this one #5800 - same problem pretty much",2021/4/13 0:26,4,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"
A related edge case in this discussion - do we capture the --local DVC config when we run experiments in background?

We do not, the issue here is similar to where we do not respect the local config when doing certain commands which use our erepo clone functionality.

it looks this is a duplicate/related to this one #5800 - same problem pretty much

And yes, this is the same issue.

We can copy untracked files (and things like the local config) directly into the temp directory, but that adds the question of how to ""stage"" this copy for multiple queued runs. Some options here would be:

Include these untracked files in our queued stash commit.

This was actually the behavior used in a real early experiments feature iteration: #4452
It turned out to be not such a good idea since we potentially end up shoving large binary files into a git commit, and git is bad at storing large binary files
This also does potentially shove things like not-gitignored auth credentials into a git commit object (even though it should be garbage collected eventually, this is still bad practice for DVC)


Copy the files into the temp directory at queue time, and have them sit there until the experiment is actually executed

This is also not ideal since we will potentially end up storing multiple copies of large binary files on disk



One other potential thing that I came up with at the time of that early PR was #4452 (comment)

As a potential future improvement, we could maybe consider doing something like generating a directory containing all of the untracked files generated by an experiment run (along with a manifest or something containing the original relpaths for the untracked files), and then storing that as an experiment-specific dvc-tracked directory.

This would fix the ""storing multiple copies of large binary files"" issue, but the downside would be that these untracked files end up cached by DVC until the next time the user runs gc (we could consider using some separate experiments specific cache dir as well I suppose)

Finally, none of these solutions would directly address the issue in #5800, where the user has gitignored files that also need to be present in the workspace in order to run their pipeline. In this case, it's credentials, but in theory, stuff like (properly gitignored) venvs also needs to be copied over as well.

This issue is also related to cloud execution: iterative/enhancement-proposals#3
Running experiments on remote machines or in CI will also require making sure that these files exist on the remote machine (i.e. the same issue as making sure they exist in the local temp directory)",2021/4/13 1:30,5,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"
do we capture the --local DVC config?
We do not

What could that affect? Remotes shouldn't be a problem. Wrong cache link types is the only significant problem I can think of rn. (Do we need a separate issue for this?)

not sure what is the problem with this phrase specifically?

True @shcheklein, prob. not the specific text we'll need to update. Code changes (including to dvc.yaml) do get picked up by queued exps. I updated this issues' title for clarity.

it looks this is a duplicate/related to this one #5800
yes, this is the same issue.

I don't think it's the same problem (maybe same solution thugh?). I'm suggesting a new option to copy untracked files into tmp dirs (which can currently be done, mostly, by git-staging them, see #5029), but that EXCLUDES git/dvc-ignored ones to my mind. In #5800 they're requesting certain git-ignored files to be copied over as well.",2021/4/13 2:25,6,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"
generating a directory containing all of the untracked files generated by an experiment run (along with a manifest...), and then storing that... using some separate experiments specific cache dir

I like the direction of this thinking @pmrowla. How about just copying untracked files to tmp exp dirs (IMO preferably with a new --all-files option so that we totally decouple this from Git) BUT WITHOUT git-committing them to the exp. Then, instead of deleting the tmp dir, keep it around. Or at least save those untracked files somewhere in the DVC cache like in your idea. Do this in such a way (manifest file?) that exp apply can restore them to the workspace as untracked files (which should help address #5800), so they never get to Git __

Running experiments on remote machines or in CI will also require making sure that these files exist on the remote machine

_. More of a reason to figure this out (at some point).",2021/4/13 4:22,7,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"Excluding the gitignore issue in #5800, one option is to leave as is and document. If we're only focused on the narrow issue here of new code or other files that should be Git-tracked, users ought to be able to work around this limitation pretty easily with git add, even if it feels a bit clunky and inconsistent. We can always revisit if needed. Thoughts?
EDIT: I missed that this has already been proposed in #5029 . Do we need a docs issue created? Can we convert this ticket into a discussion until there are some other actions decided?",2021/4/13 17:07,8,1,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"Yep, documentation for #5029 was missed. If we're not addressing this issue soon, I hope that can be done for now.

users ought to be able to work around this limitation pretty easily with git add
We can always revisit...
Can we convert this ticket into a discussion?

Sure. And this issue is basically about revisiting the current behavior at this point. Summarizing my reasons:

It's a bit strange to use a Git command as a proxy UI to a DVC feature.
You would potentially git add/remove between queueing experiments just to signal something to DVC.
Git-staged files end up committed in the exp (which can be a security concern, esp. with git add --force).
You can't include empty dirs via git add (not a problem if #5802 is addressed).

Proposed actions:

Adding an option to exp run is better than relying on git add, as a first step.
On top of that, a new caching mechanism so that untracked files included to tmp dirs don't get git-committed, but can still be restored by exp apply, should resolve most of the concerns here.
",2021/4/14 0:46,9,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"

It's a bit strange to use a Git command as a proxy UI to a DVC feature.
You would potentially git add/remove between queueing experiments just to signal something to DVC.


You already have to git add and git commit things for other DVC features to work properly.


Adding an option to exp run is better than relying on git add, as a first step.


I still disagree with this, IMO the user should have to be specific about what files they want to stage, and we should not provide an option to blanket add all untracked files.",2021/4/14 0:54,10,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"
You already have to git add and git commit things for other DVC features to work properly.

This is the only situation where DVC bases anything off staged-only files AFAIK. Git-committing yes, as we use Git revisions (for versioning); Git-adding by itself, not that I know of.

IMO the user should have to be specific about what files they want to stage

Sure. The UI can be different e.g. some sort of exp.config file (a manifest like you suggested), or new field in dvc.yaml to list which untracked files to include in the exp. I wouldn't call it ""staging"" or ""adding"" though, since ideally we never want those in Git. I don't think we should use Git at all here.",2021/4/14 1:00,11,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"I guess I don't really see the difference between ""requiring users to run 2 git commands first in order for some DVC functionality to work"" and ""requiring users to run 1 git command first in order for some DVC functionality to work"".
e:
basically

is the now established workflow for using dvc diff.

This is the only situation where DVC bases anything off staged-only files AFIK. Git-committing yes, as we use Git revisions (for versioning); Git-adding by itself, not that I know of.

Sure, this is true. And experiments is a new feature and is the only situation where DVC is trying to execute a pipeline outside of the user's workspace. Since experiments is brand new, there is no established workflow for experiments at all. If we want to tell users that the workflow for using experiments is

we can, because experiments is new and we can set the ""standard practice"" workflow to whatever we want it to be.
If we wanted to, we could even document that using git add for newly untracked files should be considered best practice all of time (even though it is technically not required for workspace runs).",2021/4/14 1:04,12,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"
difference between ""requiring users to run 2 git commands ... and ""requiring users to run 1 git command

I think it's not about Git commands. We integrate with Git versions (""revisions"") regardless of what Git interface was used to create them. I don't think we should integrate with the Git index (""staging area"").

experiments is new and we can set the ""standard practice"" workflow
document that using git add for newly untracked files should be considered best practice

We could. But a workflow like git add + dvc exp run seems too coupled IMO. Almost like we're trying to replace Git.
I could be wrong. Let's make this a discussion and invite others to weight in? For now BTW, #5800 (comment) seems like it's becoming the same issue indeed (a request along the line of @pmrowla's exp manifest/cache idea and to my new option suggestion, I think).
UPDATE: Migrated to a discussion! ___",2021/4/14 20:42,13,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5816,2021/4/12 18:19,"Looks like we have a user endorsing this change idea (#5800 (comment)).

We have some other issues open related to fine-grained control of file-level permissions and remotes, so maybe this is a use case for those features.

@dberenbaum yeah I guess a special part of the cache for sensitive info that should never leave the local env could have several uses. DVC Vault",2021/4/16 4:06,14,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5836,2021/4/16 1:04,"Users may understand that --queue runs in ""tmp dirs"" though so that's an argument for keeping --tmp. E.g. #5800",2021/4/16 1:06,1,,(II) Invalid Issues
https://github.com/iterative/dvc/discussions/5836,2021/4/16 1:04,"Unless --tmp runs are made the default again, rel. #5750 (comment), in which case the option would no longer exist this issue can be closed.",2021/4/16 1:11,2,,(II) Invalid Issues
https://github.com/iterative/dvc/discussions/5836,2021/4/16 1:04,"Nice synopsis, @jorgeorpinel! It seems like there's too much uncertainty with how these commands will change going forward to make a decision on this right now. Maybe we can just keep the discussion here and revisit later?",2021/4/16 12:19,3,1,(II) Invalid Issues
https://github.com/iterative/dvc/discussions/5836,2021/4/16 1:04,An alternative solution is to have --temp be the default and use --workspace or --debug when wanting to run in the workspace. See #5750 (comment).,2021/4/19 14:55,4,,(II) Invalid Issues
https://github.com/iterative/dvc/discussions/5836,2021/4/16 1:04,Should note here as well that --temp used to be the default. It was changed based on feedback from @dmpetrov since running in the workspace seemed much more intuitive than running outside of the workspace.,2021/4/20 0:46,5,,(II) Invalid Issues
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,(I think this can be moved to the discussions?),2021/4/27 5:20,1,1,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"(Probably. I assume this will be a discussion, so I'm fine with that. But TBH discussions feel like archived issues sometimes.)",2021/4/27 5:56,2,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"The main limitation with forcing users to use CLI Git for these kinds of operations is that they cannot use experiment names with CLI Git. Additionally, the 1:1 substitutes for certain exp commands would be low-level plumbing commands which users are probably unfamiliar with (and generally should not be using unless they are very familiar with how git internals work).
As an example, exp remove exp-123 would need to be replaced by

which is probably not something we want to tell users to use (for several reasons).

Also, exp apply cannot be replaced by pure Git CLI commands, as it does additional DVC specific operations. exp apply is needed when users want to modify an intermediate checkpoint and then do a branching exp run from that modified point.",2021/4/27 6:03,3,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"Hmmmm OK.
Here's my thought process: from our many iterations on Experiments docs I've realized it can get pretty tricky to explain experiments without the context of how they're implemented, which is why I think maybe it would be easier to expose it anyway, whether we keep some of these utilities or not.
So maybe just consider removing exp branch (only one that can be easy subs with git, it seems) and explain exps differently in docs. Thoughts?",2021/4/27 22:23,4,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"The issue with being unable to use exp names still applies with git branch, but yes, you can do git branch <new-branch-name> <exp-SHA> to get the same behavior",2021/4/28 0:56,5,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"On the issue of exp names vs. Git porcelain, one option is to show/list command SHAs by default instead (if we eventually decide to expose the underlying Git implementation).",2021/4/28 2:17,6,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"I do think the opposite, there are already too much Git-related messages, e.g., I'd simply commit the changes (subject to an option) instead of telling the user ""You can now git add .gitignore dvc.lock etc.dvc"". For the experiments, in particular, emphasizing to play with the related Git objects will result in more headaches for them, I think.",2021/4/28 9:35,7,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"Interesting discussion!

I do think the opposite, there are already too much Git-related messages, e.g., I'd simply commit the changes (subject to an option) instead of telling the user ""You can now git add .gitignore dvc.lock etc.dvc"".

This is a common suggestion since it feels like dvc requires an extra step that many users will always be doing by default. I think forcing users to git add gently forces them to understand what dvc is doing. If dvc committed all those files automatically, many users would have no idea that .gitignore gets modified or that dvc.lock or .dvc files even exist or how they got there.
@shcheklein has linked to this article before, which I find really useful in thinking about designing UI: https://hisham.hm/2020/12/18/user-power-not-power-users-htop-and-its-design-philosophy/. It's a bit long, but here's a quote that summarizes the point:

I wanted to make a tool that has the power but doesn't hide it from users, and instead invites users into becoming more powerful. A tool that reaches out its hand and guides them along the way, helping users to educate themselves about what's happening on the internals of their system, and in this way have more control of it.

Whether this is correct or not, there is some rationale for why the current UI exists, and a similar rationale could apply to gently exposing the experiments implementation.",2021/4/28 14:49,8,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"
there are already too much Git-related messages

Idk if that's a problem. There's no way around the fact that DVC versioning and other features (experiments) need Git.
The problem with just asking users to ""commit changes to Git"" is that some people may not know what to do exactly. Failing to copy/paste git commit .gitignore data.dvc is much less likely.

A tool that reaches out its hand and guides them along the way

Great principle. I think that as far as DVC builds up on Git, we may need to educate about that as needed.
p.s. OK, moving this to discussions...",2021/4/29 6:15,9,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/5896,2021/4/27 5:16,"I just realized we are already pretty open about the underlying Git implementation of experiments in the options of exp show. Specifically --all-branches/tag/commits and --sha. Why do so selectively, only in that command and its docs?",2021/5/28 5:04,10,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6475,2021/8/23 12:23,"This is currently intended behavior, it's not really a bug. Doing exp run --reset is treated similarly to doing dvc repro --force, and will reproduce your entire pipeline from the initial checkpoint state.
It looks like the discussions regarding how to handle ending checkpoint experiments, and whether or not to change the current ""infinite run"" approach are still ongoing (#6104), are we sure that this change is going to be the desired behavior?
@dberenbaum",2021/8/23 12:39,1,1,(III) Not a Bug
https://github.com/iterative/dvc/discussions/6475,2021/8/23 12:23,"I see. I guess that at least we should clarify this --force similarity in the docs, as it seems to be missing from https://dvc.org/doc/command-reference/exp/run#options",2021/8/23 12:55,2,,(III) Not a Bug
https://github.com/iterative/dvc/discussions/6475,2021/8/23 12:23,"In a related issue, when looking at the upcoming blog post from @flippedcoder on transfer learning in iterative/dvc.org#2656, I noticed that the queued experiments didn't seem to start over (see the last table in the post). I would have expected each queued experiment to start from step 0 and not have a parenthetical modified checkpoint revision.",2021/8/23 20:23,3,,(III) Not a Bug
https://github.com/iterative/dvc/discussions/6475,2021/8/23 12:23,"reset here is to reset the epochs to 0, so it is reasonable to restart the training.",2021/8/27 9:58,4,,(III) Not a Bug
https://github.com/iterative/dvc/discussions/6730,2021/10/3 2:53,Converting to discussion #6730,2021/10/3 10:25,1,1,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,"DVC has long had an open issue to add block-level versioning rather than file-level: #829. Although I can't commit to any timeline on this yet, it remains a high priority and getting closer to reality since we have recently addressed prerequisites for it. Unfortunately, it's still too far away to recommend you wait for this to be implemented, but it's close enough that I don't think it's likely that we want to adopt some other solution to specifically handle hdf5 with external links.
Are you able or willing to put the external links into the same directory as main.h5? If so, you could track the whole directory with DVC. This approach tends to work for users with other partitioned data formats like parquet.
Happy to discuss further.",2021/9/22 18:44,1,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,"Please have a look at these changes
annmary-roy#1
The intention is to support dataset versioning for HDF5 in DVC with minimal changes/overheads on DVC code.
And provide better manageability of these split files in DVC.
The versioning at a dataset boundary can be achieved when we track the whole folder, but  when these hdf5 files are individually tracked, the user would have to add each split file separately.",2021/9/23 15:03,2,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,"Nice! So you have this working and are interested in seeing if it's something we would want to merge?
It looks like a good way of doing things for your use case, and it would be great to promote this to anyone else who's using HDF5 with external links.
TBH I'm still not sure about making it part of the main codebase as is since DVC is generally agnostic to file formats for data management. This could open up requests to support other types of metadata files. @efiop what do you think?",2021/9/23 20:46,3,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,"Doesn't look like the linked PR is in any way complete and is more of a draft of how it could work with dvc add. __
@annmary-roy It looks like the main problem you have with dvc is file-level granularity, right? If so, I would maybe wait for #829 to see how well it will work for hdf5 files by treating them just like binaries.
Just to clarify, if we are talking about dvc support for hdf5, we should treat it not as a storage format, but more like a mount point in the filesystem, that virtually extends your workspace. In that case, actual underlying data storage should happen in dvc with no hd5f specific difference, but saving (aka dvc add) and linking (aka checkout) will have to know how to work with hdf5. In that sense, this reminds me of our experimental external outputs feature, that allows one to use clouds like s3/ssh/etc to extend their workspace.  Unfortunately, that scenario is due for reconsideration, but local hdf5 could potentially be supported as well, we'll just need to carefully think through the scenario and internal changes for dvc. E.g. from our local repo perspective, local hdf5 file is similar to what git submodules are for git tree.
It is also clear that hdf5 support is not only relevant for data management (dvc add), but also for our pipelines, where one could want to depend on some particular path within the hdf5 file, instead of depending on it as a whole. This case seems to also be solvable by the ""mountpoint"" approach I've mentioned before.
Also, looks like fsspec (the fs backend that we use for our cloud and git operations) could support hdf5 (though there are some details to figure out fsspec/filesystem_spec#5 (comment)) as it already seems to support zarr ( i see some hdf5 mentions in https://filesystem-spec.readthedocs.io/en/latest/_modules/fsspec/implementations/reference.html?highlight=hdf5 , but I'm not sure if it actually supports it or not). So fsspec fs implementation for hdf5 is likely going to be a pre-requisite to any other integration activities. For example, in your PR you are manually iterating through all files in hdf5 file, but it is much more convienient and powerful to view it as a fs and be able to use things like fs.glob.",2021/9/23 21:35,4,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,"
Doesn't look like the linked PR is in any way complete and is more of a draft of how it could work with dvc add. __
@annmary-roy It looks like the main problem you have with dvc is file-level granularity, right? If so, I would maybe wait for #829 to see how well it will work for hdf5 files by treating them just like binaries.

Could you share a reference to any work-in-progress code or design for #829?",2021/9/27 14:34,5,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,"@suparna-bhattacharya There is none, but we plan on working on it in the future. It will likely be very similar to git packs.",2021/9/28 12:33,6,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,"For time being we are planning to go ahead with the folder based approach, where we can put all the splitfiles into a single directory and do an explicit dvc add on the folder and main.h5 .  fsspec implementation for hdf5 is something which we may consider later.",2021/10/5 11:08,7,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,Converting to discussion since this is not actionable yet,2021/10/8 18:19,8,1,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/6776,2021/9/21 4:33,"@efiop @dberenbaum @suparna-bhattacharya
We did the folder based approach to version hdf5 files with dvc. To enable more efficient versioning of large hdf5 files containing multiple datasets, we split the hdf5 files at a dataset boundary into separate files (grouped inside a single folder) and mount the separate files inside the main file as hdf5 external links. This enables versioning of HDF5 files at a dataset boundary. The main file and other  dataset files can be managed as a single unit with the read and write operations happening transparently using the main file handle. The changes in the underlying datasets are tracked at the individual file boundary. Whenever a dataset changes only the split file containing that dataset and main file needs to be duplicated. The files hosting the other datasets which are part of the main file in unchanged hence not duplicated. This increases the storage and bandwidth efficiency, when versioning large HDF5 files.
Please find more details here
https://github.com/hpc-io/vol-dset-split
We are planning to write a detailed blog post on this work.",2022/1/13 8:17,9,,(I) Non Actionable Topic
https://github.com/iterative/dvc/discussions/7876,2021/9/23 20:15,"I didn't mention there but I think this feature might be some p5-too-little-to-consider There are many workarounds like

which guarantees unique names more than $RANDOM
TBH I believe it doesn't deserve an option, but can be considered as a part of a broader feature to add formatting to names, like

that will also fix the problem we have in #6071",2021/9/27 10:25,1,,(VII) Information Storage
https://github.com/iterative/dvc/discussions/7876,2021/9/23 20:15,"Similarly, there are workarounds for #6071, right? Like:

There could be all kinds of shortcuts to make this easier. Maybe it's better to focus on a doc or blog post showing off how these kinds of tricks could make for a more robust way to organize experiments?",2021/9/27 19:17,2,,(VII) Information Storage
https://github.com/iterative/dvc/discussions/7876,2021/9/23 20:15,"dvc exp run --format ""exp-param1={param1}-metric1={metric1}-{sha}"" is a general purpose idea that can be used for #6071 and this. This option can automatically rename the experiment based on runtime parameters and metrics, e.g.,

I think the team can direct their efforts for this general purpose option instead of fixing experiment naming case by case.",2021/9/30 14:30,3,,(VII) Information Storage
https://github.com/iterative/dvc/discussions/7876,2021/9/23 20:15,"@dberenbaum
As in #6674, currently duplicated exp names are forbidden before creating. Do we still need this?",2021/10/27 8:30,4,,(VII) Information Storage
https://github.com/iterative/dvc/discussions/7876,2021/9/23 20:15,"Thanks @karajan1001, that makes it less problematic, although the original feature request from the user in #6071 was to alleviate the burden on users to be responsible for making unique names. It's really just one implementation of #6071. I don't see it as a likely feature to implement anytime soon, so we could set as low priority or move it to a discussion.",2021/10/27 12:31,5,1,(VII) Information Storage
https://github.com/JohnSnowLabs/spark-nlp/discussions/2617,2021/3/26 12:30,"Hi, I am a newbie to spark myself but I faced a similar problem. My idea is that you might have to convert lemmas and transform it from the annotation format of Spark NLP to a __uman-readable_ format (finisher).",2021/3/26 21:15,1,,(IV) Further Discussion
https://github.com/JohnSnowLabs/spark-nlp/discussions/2617,2021/3/26 12:30,"Thank you @SameekshaS for the response.
My problem is related to the creation of the ""lemmas"" column because I cannot apply the pipeline composed by the lemmatizer.
So far I have found a workaround (as suggested by you) by transforming the ""mapped"" dataframe with a finisher and then rebuilding a new pipeline starting from the DocumentAssembler up to the lemmatizer. This obviously solves the problem but introduces a large computation overhead.
Analyzing the issue a bit I think I've understood the problem but I don't know exactly how to solve it. The point is that I should change the column type in the dataframe schema metadata .... but I have to figure out how to do it.",2021/3/29 13:51,2,,(IV) Further Discussion
https://github.com/JohnSnowLabs/spark-nlp/discussions/2617,2021/3/26 12:30,"I've solved with this:

after the ""mapAnnotationsCol"" function.
I think this annoying behavior could be corrected by simply inheriting the column metadata value from the annotationType metadata of each annotations in the column.
Anybody knows if there is an open backlog to propose changes?
Thank you all",2021/3/29 18:38,3,,(IV) Further Discussion
https://github.com/JohnSnowLabs/spark-nlp/discussions/2617,2021/3/26 12:30,I converted this to a discussion so it can continue without being close as it is interesting. Hope it's OK.,2021/3/29 19:55,4,1,(IV) Further Discussion
https://github.com/microsoft/AirSim/discussions/3206,2020/12/11 8:30,"Hi @wangwwno1, Have you tried to change the simulation's ClockSpeed?
I would assume that slowing down the simulation would allow for more API calls per 'simulated second' (for a fixed real API call response rate).
I tried running your test code with ClockSpeed = 1 and ClockSpeed = 0.1 and there was a noticeable difference in the readings which means this might work for you.",2020/12/11 22:39,1,,(VII) Information Storage
https://github.com/microsoft/AirSim/discussions/3206,2020/12/11 8:30,"
Hi @wangwwno1, Have you tried to change the simulation's ClockSpeed?
I would assume that slowing down the simulation would allow for more API calls per 'simulated second' (for a fixed real API call response rate).
I tried running your test code with ClockSpeed = 1 and ClockSpeed = 0.1 and there was a noticeable difference in the readings which means this might work for you.

I rewrite my code as:

And test it with ClockSpeed = 0.03333333333 (roughly as 1/30), the output is satisfied:

Update: The best result is ClockSpeed = 0.3333(1/3 with 4 digit precision) and 0.003ms in simContinueForTime:

For a simulation interval of 2.5ms, the best setting is ClockSpeed=0.8333 with an error of 0.0064%",2020/12/14 5:05,2,,(VII) Information Storage
https://github.com/microsoft/AirSim/discussions/3206,2020/12/11 8:30,"So the conclusion is:

The time in function simContinueForTime is WallClockTime, or the elapsed time in our world
What I desired is simContinueForSimClockTime or the time elapsed in Simulation
Passing argument secs shorter than 2ms to simContinueForTime might make the code out of sync with simulation: the user may have an illusion that the simulation is still running(which is not)

For example, with the ClockSpeed = 0.01(the minimum interval would be 0.3ms) and time interval in simContinueForTime to 1ms would result in:

The code is running but no advancement in timestamp, which means the simulation is ""freezed"". Would be very confusing as there is no warning or exception.
Since the original question is solved, I will bring this discussion to another issue and close this one.
Thanks for your suggestion, @ahmed-elsaharti ! __",2020/12/14 5:21,3,1,(VII) Information Storage
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"Hi @tharindurmt , as I mentioned in #2980, I could get the point cloud from the depth images recorded through the ROS wrapper. However, the point cloud also has radial distortions:
",2021/1/19 4:07,1,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"@GimpelZhang  Thanks for confirming that it doesn't work with ROS as well. So, this could be an issue with the DepthPerspective images then.
Were you able to rectify this issue?",2021/1/19 5:23,2,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"
@GimpelZhang Thanks for confirming that it doesn't work with ROS as well. So, this could be an issue with the DepthPerspective images then.
Were you able to rectify this issue?

No, I didn't notice this problem until you raised this issue.",2021/1/19 5:28,3,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"Hi @tharindurmt, my first guess here would be that your script might not be taking the lens' radial distortion into account (forgive me if that is not the case, I quickly skimmed through the code), that could be the reason behind this.",2021/1/19 6:47,4,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"@ahmed-elsaharti Yes, the radial distortion coefficients are not used here. However, I'm under the assumption that the standard PIPCamera does not exhibit any radial distortions. I will double check on this though. Thanks for the suggestion.",2021/1/19 7:10,5,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"@tharindurmt Since the camera allows you to set the FOV, it would be safe to assume that radial distortion would occur.
Also, I did a quick couple of tests using your scripts. On setting FOV to 20 degrees, the distortion disappears from the point cloud.

as opposed to the 90 degrees FOV:

I would suggest using openCVs undistort() function prior to processing the image. That should theoretically fix the issue.",2021/1/19 7:16,6,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,@ahmed-elsaharti Thank you very much for conducting that experiment and feel silly for not thinking about it myself. I'll definitely try it out tomorrow and see how it goes.,2021/1/19 12:40,7,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"@ahmed-elsaharti I ran a test with 20 degrees FoV and still seems to be some radial distortions (please see attached screenshot).

Can you please confirm whether you used DepthPerspective or DepthPlanner image type? When I use DepthPlanner, I can get the following result at 20 degrees FoV.

However, to get to my original goal (that is to approximate the lidar sensor using a depth camera which is described in 3272), it seems that I will have to obtain a high resolution depth image and then sample from it to create the velodyne point pattern.  The following is when I set the FoV to 90 degrees and use DepthPlanner.

What do you think of this proposal?",2021/1/20 9:53,8,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"Hi @tharindurmt, I used your code as it is (as far as I remember) so it was DepthPerspective.
Not entirely sure about the usage of a planar view here. To get as close to a realistic system as possible I would stick with a perspective camera but would calibrate the camera and undistort the image.
This is me assuming that you're simulating a real-life scenario. Especially since your method of converting a depth image to a point cloud is pretty valid for real life use. Which is why I would recommend keeping this as 'non-ideal' as possible. But this mainly depends on your use case.",2021/1/20 19:04,9,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"@ahmed-elsaharti Thanks for confirming that.
I agree that the perspective image should be used for better realism (which is why I was using it from the beginning). I conducted some further experiments and I think there are two strategies to model the lidar using DepthPerspective images (as opposed to my original proposal).

Obtain multiple high resolution DepthPerspective images with very narrow FoVs (something like 5-10 degrees) and then sample from them to create the corresponding velodyne pattern. So, at each sensor call, multiple such images will be obtained to cover the lidar sweep needed.
Build a custom projection matrix at each sensor call and obtain a single high resolution DepthPerspective image to cover the entire lidar seep needed and then sample from it to create the corresponding velodyne pattern.

Will try both and report back.",2021/1/20 23:51,10,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"Hi @tharindurmt , it seems like the depth image obtained by camera is the distance from the scene to the camera center, you need to convert it to the distance to image plane. Just  use DepthConversion before you generatepointcloud function. The original answer and more discussion can be found in this link.

Scene

Before conversion

After conversion
",2021/4/13 10:31,11,1,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"@virtualRooom  Thaks for this and sorry, I just saw this.
I will have a look and see how this goes.
Since this didn't work out, we divereged from this a bit.",2021/5/11 6:46,12,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"@virtualRooom  I just tested this and the artifacts similar to what I have described initially. Please see attached. There is a clear radial distortion which is similar to what I have explained above.
Code:

Settings file:

The view that was used to generate the pointcloud and the resultant poincloud are as bellow.

",2021/5/11 7:24,13,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"Hi @tharindurmt, try to replace pcl = generatepointcloud(img2d) with pcl = generatepointcloud(img2d_converted).",2021/5/11 7:33,14,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"@virtualRooom  Stupid me! Totally missed that and thanks for pointing out that.
The result is similar to what I get from DepthPlanner - see above. I think a further sampling operation is needed to re-create the Velodyne pattern from this. Not sure how accurate it's going to be in that instance though.",2021/5/11 7:46,15,,(I) Non Actionable Topic
https://github.com/microsoft/AirSim/discussions/3955,2021/1/19 2:15,"For those still struggling with this issue..
ros depth_image_proc has a nodelet: depth_image_proc/point_cloud_xyz_radial. using it instead of depth_image_proc/point_cloud_xyz fixed the problem for me",2021/8/10 13:12,16,,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/3740,2021/6/8 9:48,"@FinderSerg - thanks for raising this new feature request. I'm moving this to discussion so that the issue is also visible to others to vote.
Guys - please use thumb up to vote for adding this feature.",2021/6/9 3:08,1,1,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/3752,2021/4/21 10:13,"The pruner.compress() only uses a mask (a tensor contains only 1 and 0) to multiply the weight (and bias if any) before calling the module on the input, but all the modules are kept the same, in terms of their shape, weight, and bias, so the model is at the same size. While speedup_model() actually changes the modules, for example, from Con2d(in_channels=16, out_channels=32) to Conv2d(in_channels=13, out_channels=27), so the size of the model is physically changed. Speedup_model() does not remove the batch normalization layer, it only changes it if the shape of its input or output is changed.
If you find it hard to converge after speedup_model(), I guess it might be due to the over pruning. Try pruning less, see if it can converge.",2021/4/22 11:57,1,,(IV) Further Discussion
https://github.com/microsoft/nni/discussions/3752,2021/4/21 10:13,"@davbowman
what i mean removing batch normalzation is that it will remove the ""dependent"" batch normalization layer.  for example, conv(64)->bn(64), if the removed planes are 32, it would be conv(32)->bn(32). am i right?
so is it ok to finetune directly from speedup_model()?",2021/4/22 12:23,2,,(IV) Further Discussion
https://github.com/microsoft/nni/discussions/3752,2021/4/21 10:13,"
@davbowman
what i mean removing batch normalzation is that it will remove the ""dependent"" batch normalization layer. for example, conv(64)->bn(64), if the removed planes are 32, it would be conv(32)->bn(32). am i right?
so is it ok to finetune directly from speedup_model()?

Sorry, my misunderstanding. Yes, you are right.
I think it is better to finetune from speedup_model().",2021/4/22 12:27,3,,(IV) Further Discussion
https://github.com/microsoft/nni/discussions/3752,2021/4/21 10:13,"my problem is that with the same sparsity. finetune from pruner.compress() or finetune from model_speedup(), the former is fine but the other is hard to conveage.
the problem exists even though reducing the sparsity.",2021/4/22 12:29,4,,(IV) Further Discussion
https://github.com/microsoft/nni/discussions/3752,2021/4/21 10:13,so my guess is that the un-removed bias of the batch normalization would recover some meaningful activation value.  if finetune from model_speedup() is good then why examples of nni don't do that?,2021/4/22 12:31,5,,(IV) Further Discussion
https://github.com/microsoft/nni/discussions/3752,2021/4/21 10:13,"thanks @twmht for raising this up, and sorry for the late responds, we were too busy on the recent release and missed this issue somehow, hope you are still stay tuned.
I just converted the issue to a discussion, as we don't have a good resolution for this issue yet, and would like to open this for idea and discussions.",2021/6/9 8:00,6,1,(IV) Further Discussion
https://github.com/microsoft/nni/discussions/3771,2019/6/25 5:59,@SparkSnail can we close this issue now?,2019/11/28 13:11,1,1,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/3771,2019/6/25 5:59,"I don't remember the scenario to use this feature, perhaps we could have a short discussion about this feature, and close it if it's not necessary to be implemented.",2019/11/29 3:35,2,,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/3771,2019/6/25 5:59,"
I don't remember the scenario to use this feature, perhaps we could have a short discussion about this feature, and close it if it's not necessary to be implemented.

any conclusion from the discussion?",2019/12/23 2:53,3,,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/4212,2021/9/16 20:12,"You can use random strategy in multi-trial NAS, which will sample one architecture in each trial.",2021/9/17 8:40,1,,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/4212,2021/9/16 20:12,Could you show me some code examples of how to do that? I just want to sample some architectures or cells from DARTS actually...,2021/9/17 14:03,2,,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/4212,2021/9/16 20:12,"I tried to use the Random strategy but the documentation says that ""Do not dry run to get the full search space. Used when the search space has variational size or candidates"" which is not the case for the defined search spaces such as DARTS. Could you please give me an example of how to simply sample at random the cells or entire architectures from DARTS space? I'm still waiting...",2021/9/20 18:54,3,,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/4212,2021/9/16 20:12,"I don't think DARTS search space has a variational number of candidates.
You can refer to multi-trial/mnist example on how to use Random strategy.",2021/9/22 2:58,4,,(I) Non Actionable Topic
https://github.com/microsoft/nni/discussions/4212,2021/9/16 20:12,"This more like a discussion about the usage, moving the issue to discussion to get more people to join the discussion.
@geantrindade - haven't seen your reply for few days, get any luck on trying the Random Strategy as @ultmaster mentioned lately?",2021/9/26 7:23,5,1,(I) Non Actionable Topic
https://github.com/mu-editor/mu/discussions/1336,2018/7/29 23:20,"I have no particular view of the usefulness of this but, technically, it could be supported by using the built-in trace module",2018/7/30 7:46,1,,(I) Non Actionable Topic
https://github.com/mu-editor/mu/discussions/1336,2018/7/29 23:20,"I will convert this to a discussion in our new discussion board here at Github, as that's where we discuss feature requests from now on.",2021/2/25 13:17,2,1,(I) Non Actionable Topic
https://github.com/mu-editor/mu/discussions/1344,2021/3/1 15:32,"Is that a ""desktop"" Python module? Is it meant for a Raspberry Pi? Or how does it interface with the robot?
If the code is in github it can probably be pip installed via URL, and that could probably be done via the Third Party packages window. Although I haven't tried that yet.",2021/3/2 14:42,1,,(IV) Further Discussion
https://github.com/mu-editor/mu/discussions/1344,2021/3/1 15:32,"Perhaps off-topic, but I'll add it anyway: if the library is a single file, I would just ask my pupils/students to copy it to the directory of their project. I think that kids should also learn that there's nothing ""magical"" about creating libraries and showing them that it's just another Python file is a nice illustration of that.
As this is not really an issue, I will convert this to a discussion in our discussion forum until we've figured out if there's something that needs to change in Mu.",2021/3/2 19:06,2,1,(IV) Further Discussion
https://github.com/mu-editor/mu/discussions/1344,2021/3/1 15:32,"I would have to check, but I believe the mu_code folder is added to the site-packages paths, so adding the package folder there should make it importable. Just need to make sure the folder structure is correct (it is not subnested in more directories than it needs).
If these are individual modules they can be added next to the user python code (like @dybber suggested) or they can probably be also added the mu_code folder.",2021/3/3 10:46,3,,(IV) Further Discussion
https://github.com/mu-editor/mu/discussions/1344,2021/3/1 15:32,"The BirdBrain library is not meant for the raspberry pi. They have two ""robot"" kind of devices, one called a Hummingbird (which you attach sensors and output devices to) and the Finch 2, which is a self-contained robot. On your system is a ""BlueBird Connector"" which appears like a web server on a non-standard port to your app running on your system. The ""BlueBird Connector"" then takes those http requests and uses BLE to communicate to the device. Very nice for writing your own drivers. Young students also do very well with this.
The BirdBrain Python library is just a single file with some class definitions, so there is no sort of packaging with the code.
On my system (Mac with Big Sur), the sys.path contains:
['/Users/fmorton/tmp', '/Users/fmorton/.pyenv/versions/3.7.6/lib/python37.zip', '/Users/fmorton/.pyenv/versions/3.7.6/lib/python3.7', '/Users/fmorton/.pyenv/versions/3.7.6/lib/python3.7/lib-dynload', '/Users/fmorton/Library/Application Support/mu/mu_venv/lib/python3.7/site-packages']
I do not see mu_code in the path. For this case, it would be nice to have mu_code/site-packages, but this is probably an outlier. It is fine for me to put it in the mu_env site-packages. I just wanted to make sure there was not some well-known place to put files like this that I did not know about.
I did try the third-party packages without success, but this library is not really a package. It is just a file with some class definitions.
I will also consider the @dybber advise about just adding the library file with the project. One challenge with that is I plan to make changes to the library and I want to avoid old versions floating around. I don't actually copy it into the site-packages directory, but instead put a symbolic link pointing to the github repository on the systems that we put in a standardized place, so everyone is always using the latest library.
BTW, the mu editor is absolutely fantastic with the birdbrain devices. You just use the Python 3 mode and everything works with almost no effort. Well done!",2021/3/3 15:56,4,,(IV) Further Discussion
https://github.com/networkx/networkx/discussions/4656,2021/3/4 18:13,"Hey there !
Disclaimer :  I am not a contributor to networkx, but I can try to answer at least your technical questions.

The entries need not be in (0, 1) ! For example, if nstart = [1, 0, 0, 0], then you can convince yourself that nstart_sum = 1 and therefore x = [1, 0, 0, 0]. The only thing that is ensured is that nstart_sum != 0, so that the division in this line does not throw an error.
The line you highlighted checks for convergence, i.e. ""is my new vector close to my previous one"" ? If the answer is true, then you've (approximately) found your eigenvector (since when the iterations get close, it means you're approaching the limit).
My guess is that networkx does not explicitly depend on numpy nor scipy, and thus there is a version of this function for when one of those packages is not installed. As a rule of thumb, if you have numpy on your system : use the numpy version !
",2021/3/5 15:14,1,,(I) Non Actionable Topic
https://github.com/networkx/networkx/discussions/4656,2021/3/4 18:13,"Totally makes sense, point for point. Thanks for taking the time to read/reply!",2021/3/5 17:31,2,,(I) Non Actionable Topic
https://github.com/networkx/networkx/discussions/4656,2021/3/4 18:13,"BTW this discussion is more suited towards GitHub discussions (moving this issue to discussions).

My guess is that networkx does not explicitly depend on numpy nor scipy, and thus there is a version of this function for when one of those packages is not installed. As a rule of thumb, if you have numpy on your system : use the numpy version !

Yes numpy/scipy weren't hard dependencies of networkx but now they are and we will be updating the code(#4371) to use numpy version by default whenever someone uses eigenvector_centrality, currently if you want to access the numpy version you need to use eigenvector_centrality_numpy. The python implementation will stay in our codebase for reference (if someone just wants to read it or use it without numpy/scipy).",2021/3/5 18:19,3,1,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"Hi @anant-k-singh! Note: I've moved your issue to a discussion.
It looks like you're trying to build a custom transform for a Jupyter display type. You can find docs on how to do this in the nteract/outputs repo at https://github.com/nteract/outputs/tree/master/docs.

This python function would create a request for nteract/jupyter-extension and that request gets mapped to my custom package, which is a React component

The Python function should return a display_data object with a unique media type that can be mapped to a React component on the client. The documentation linked above has more info on this.

This React component would also return data (.csv file) to the notebook cell, or to the notebook in general

Can you clarify what you mean here. I can interpret your question in one of two ways:

You want your React component to send data back to the kernel
You want the component to embed some data into the serialized notebook file

Both are feasible but I can provide guidance depending on what you mean.
To get started, I'd recommend reading the docs mentioned above. If you have any follow-up questions, please post them here and I'll address.",2020/10/28 18:24,1,1,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"Thanks @captainsafia ! Now I have better understanding of transforms
I should be better able to explain the doubt/question.
I want to embed a visual/graph in notebook's output cell using a custom transform (i.e. React component)
In next cell, I want to call an API to extract data (as string) from that prev visual and assign it to a python variable
For this to happen, I would have to refer the instance of React component which rendered the visual in prev cell, so that I can call extract data API on that embedded visual.
Hypothetical usage would look something like this:
In [1]: EmbedVisual(...someParams)
Out [1]: (Visual is embedded with custom transform's output)
In [2]: csvData = ExtractData(...sameParams)   # Calls extract data API on the same embedded visual and returns the data as plain string
In [3]: csvData
Out [3]: 'Visual's csv data as plain string'
Note: Here EmbedVisual and ExtractData are python functions which basically request for lets say media types application/my-visual and application/my-data-format respectively
To make this work, I can of course embed the visual again in ExtractData but instead of returning the embedded visual, I can call the API on visual and return the required data.
It would be great to avoid embedding the whole visual again and again everytime I want to access it's APIs
My questions are:

Is it possible to access a React component's instance later from some other notebook cell? Refer sample usage above.
How can the custom transform/React component return a plain string and not an HTML element?
",2020/10/29 18:08,2,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"
Is it possible to access a React component's instance later from some other notebook cell? Refer sample usage above.

You could do this on the client pretty easily. However, based on the code that you showed above the ExtractData method is running in a Python kernel and not on the client.
Based on your example, it seems like the best approach would be to have your EmbedVisual and ExtractData methods reference the same internal objects on the Python side. So, when a user calls EmbedVisual, we take data_X and return a serialized visual representation. Then the user calls ExtractData(data_X) and the kernel returns the data associated with the data_X reference.
If you wanted all this to happen entirely in the client, you could achieve it with two transforms. An EmbedVisual transform that renders the visual and an ExtractData transform that calls a static method to get the data from an EmbedVisual component.

How can the custom transform/React component return a plain string and not an HTML element?

Yes, a transform can render whatever you want it to on the page. Of course, it would have to be an HTML element since it is rendered on the page.
Perhaps you're referring to the fact that all transforms are associated with display_data objects that have a serialized JSON string representation that is stored in the notebook?",2020/10/30 17:53,3,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"Regarding the second question:

How can the custom transform/React component return a plain string and not an HTML element?

As you can see in the In [2] of sample usage above, ExtractData() function does not render anything, instead it returns string value.
When ExtractData() function is called, I want to:

Import some js libraries
Include run some js logic (Find the visual rendered by EmbedVisual() and extract data from it using imported libs)
Return the extracted data as string and not as HTML element

Is custom transform fit for this or do you have other recommendation?",2020/11/3 10:00,4,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"
Is custom transform fit for this or do you have other recommendation?

Ah, I see! Yeah, what you're describing is a plain-text display object. You can achieve this by having your ExtractData method return a text display object (ref).",2020/11/3 18:59,5,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"I'm sorry, this has become a long thread
Please correct me if I am wrong, the display object is used to render HTML (JSX element) in the output cell
In my scenario:

I do not want to render anything in the output cell (Check usage of ExtractData() function)
I want to execute some JS logic in ExtractData() function, which would return a string (We do NOT want to render this string, instead store it in a python variable in the notebook)

My understanding is, that JSX element returned by custom transform's react component would be rendered in the output cell even if it is just a string.
How do I return a string after executing some JS logic without rendering it in output cell?",2020/11/4 15:47,6,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"
How do I return a string after executing some JS logic without rendering it in output cell?

In that case, you could render nothing (aka return null from the render method) from the React component.

I want to execute some JS logic in ExtractData() function, which would return a string (We do NOT want to render this string, instead store it in a python variable in the notebook)

This is a particular challenging part of the design because two runtimes have to communicate the state of a variable to each other, the JavaScript runtime running in the user's browser and the Python kernel running on the users computer (or perhaps in the cloud somewhere). If possible, I'd recommend investigating if there is some way to get around having to execute the JS logic to extract the variable on the client. It'll make the implementation a lot cleaner and more portable across different Jupyter front-ends.",2020/11/5 1:10,7,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"I went through some of the nteract docs:
https://docs.nteract.io/
https://components.nteract.io/
https://github.com/nteract/docs
https://github.com/nteract/outputs/tree/master/docs
https://github.com/nteract/vdom/tree/master/docs
Please correct me if I am wrong with my understandings:

Custom transform can only use React components which by nature can only return JSX element
Custom transforms are one way communication, i.e. they take data + media type in python kernel and render the output in client's browser, i.e. Python kernel -> React component
Custom transforms cannot return data back to python kernel
nteract, by design, does not allow data to be transferred from client's browser to python kernel running the nteract notebook e.g. via JSON or something else
In the sense that we can have data transfer from python kernel to client's browser (custom transforms) but not the other way round
",2020/11/5 14:01,8,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"
In the sense that we can have data transfer from python kernel to client's browser (custom transforms) but not the other way round

Generally, true. The notable exception is ipywidgets which does allow syncing data from a client to a kernel, but that's sort of its own thing.
The other points are correct.",2020/11/5 22:37,9,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,"I am not able to render ipywidgets in my nteract notebook (though there are no errors)
Does nteract support ipywidgets?
Details: #5348",2020/11/9 12:12,10,,(I) Non Actionable Topic
https://github.com/nteract/nteract/discussions/5339,2020/10/28 7:30,It does! I posted a comment on the regression that is causing this.,2020/11/11 1:08,11,,(I) Non Actionable Topic
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,"The val_wer and saving checkpoints are based one just the first manifest in the list. I don't think we take any average or something. If you want to do something like this, you can sample from different validation manifest and create a new one to be used as the first one.",2022/4/18 18:14,1,,(III) Not a Bug
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,"Note that we particularly avoid average of manifests since it is usually meaningless. There's no baseline to compare against such averaged datasets.
As Vahid suggested, you can still do it by concatenating all the Dev sets you want into one and then using it as the first but I would recommend not doing that. While wer would appear on average to drop, it does not mean that your average accuracy on unseen test data would also be good on average - some datasets might dominate due to amount of data and the average would still drop, sacrificing the accuracy of the smaller datasets.
It is simply better to separate all manifests, and focus the model to improve on the one you most care about (and usually it is the more challenging datasets in the mix - somewhat like dev other for LS instead of using dev clean as the first dataset)",2022/4/18 18:51,2,,(III) Not a Bug
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,"@titu1994 thank you for the explanation of the reasoning behind it. I have, in the meantime, found  
 where the approach is explained.
My question was spurred by the fact that by reading only the config yaml when I added multiple validation sets (in addition to my principal validation set, I added a very small cherry picked one, but placed it as the first one in the list). I was somewhat expecting an average to be reported and used for checkpointing, surprised by the values I later found out they were based just on the cherry picked set.
If you don't mind, I would suggest adding val_dl_idx: 0 with a comment that this is the index of the validation set for reporting and checkpointing to all config yamls. I think this alone would help a lot, already. For example, I wasn't aware such a parameter even existed before reading trough the code.",2022/4/18 20:06,3,,(III) Not a Bug
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,"val_dl_idx is not mentioned for a good reason - it is really problematic the moment you finish training and save a pretrained Nemo file.
Next time you load it up, and you try to fine-tune the model, say your validation setup no longer has that many manifests (just 1 for example). Now the val_dl_idx remains some non 0 number but doesn't index an appropriate manifest, so after training when you try to save a file, it will crash after a training epoch.",2022/4/18 20:10,4,,(III) Not a Bug
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,"Honestly, this system is outdated and doesnt support the original goal - uniform multi dataloader support across domains. Turns out there's no need for multi data support in NLP, NMT during evaluation since they simply compute loss over text slices, not needing manifests. TTS never needs such validation metrics over multiple datasets.
So it is actually wrong that such support was added to ModelPT, but for example other domains in the future still may need it so it's stuck around.",2022/4/18 20:13,5,,(III) Not a Bug
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,"I understand it is outdated. As I understood, the approach in test_ds is the same, am I right? For clarity and ease of use purposes, wouldn't a simple warning work in the case of fine-tuning? I mean in case validation_ds is changed on load time, check if val_dl_ids differs from 0 and issue a warning in case it does, error if it's out of range?
I imagined also that NMT could take advantage of a multi-validation set, e.g. sacreBLEU scores for written, spoken, scientific or other sub-domain text, ... am I mistaken?",2022/4/18 20:28,6,,(III) Not a Bug
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,"NMT needed it's own setup for multi validation and it's more domain dependent setup.
Instead of a warning, you'll just hard crash at the moment and there's no easy way to check at the time of setup of the dataloaders themselves.",2022/4/18 20:30,7,,(III) Not a Bug
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,"I see. Thank you for the explanation. I think you can convert the issue to a discussion now, so it can come handy to others.",2022/4/18 20:45,8,1,(III) Not a Bug
https://github.com/NVIDIA/NeMo/discussions/4025,2022/4/14 9:06,Good point will do.,2022/4/18 21:03,9,,(III) Not a Bug
https://github.com/payloadcms/payload/discussions/130,2021/2/17 17:20,"Hey @mo7orh3ad _ this is actually just an error with the docs. Uploads are not meant to support multiple, but this could potentially be a future enhancement if you would find value out of it.
If you want to support multiple uploads, you could nest an upload field within an array field and that would also do the trick. That's what we do when we need data like sliders of images, galleries, etc.
Thoughts?",2021/2/18 3:38,1,,(I) Non Actionable Topic
https://github.com/payloadcms/payload/discussions/130,2021/2/17 17:20,"Hi @jmikrut, thanks for getting back - using an array field would have been my next approach. I agree, it would be great if this enhancement would find its way into a future release.",2021/2/18 8:16,2,,(I) Non Actionable Topic
https://github.com/payloadcms/payload/discussions/130,2021/2/17 17:20,Docs have been updated. We'll keep this one open for now until we publish our roadmap__t which time we will be moving all enhancement issues to the Roadmap instead of tracking them here.,2021/2/20 23:27,3,,(I) Non Actionable Topic
https://github.com/payloadcms/payload/discussions/130,2021/2/17 17:20,Converted from an issue to a Feature Request discussion thread.,2021/4/7 2:48,4,1,(I) Non Actionable Topic
https://github.com/payloadcms/payload/discussions/130,2021/2/17 17:20,"Would also like to see this, it's a very common use-case to be able to select multiple images and reorder them (galleries, sliders, etc.), using an array field with contained upload field is very clunky.
Craft's Asset field does this well:
",2022/9/9 2:21,5,,(I) Non Actionable Topic
https://github.com/payloadcms/payload/discussions/454,2022/2/15 15:11,"Hey @oranoran _ this should actually be documented but it is actually functioning as intended, however the intended functionality could be up for debate.
Right now, we export withCondition as a HOC from payload/components/forms. You can wrap your custom component with this HOC and then boom - conditions work.
The question is - should this be done for you? The reason that we did not auto-wrap custom components in withCondition is because sometimes you might want to control conditions yourself as part of the custom component - for example, this discussion.
I'll move this over to a discussion but I'd like to keep the convo going!",2022/2/15 15:18,1,1,(IV) Further Discussion
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"Hey @jeanbmar. Thank you for the report. We're going to look at reproducing this.
I think some rough examples of queries you are using and (rough) collections that you have defined would be helpful in this.
Here are some initial questions I have for you that would help with troubleshooting:

How many relationships are getting hit in the query chain?
Are any Custom IDs being used? (I know we put that in for you)
Are any of the fields localized? (localized: true)
Are any of the relationships indexed? (index: true)
",2022/3/14 2:44,1,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"Hey @denolfe, thank you for your quick response.
Here is a real example we use, it's nothing too crazy:

This query returns only 60 docs. It took 66s to get the result.
The collections involved are brawlers, stats, skills, starPowers, gadgets, pets, skins, resources, campaigns, media (upload).
All collections use custom string ids except media.
Maybe 20 fields are localized. This query only returns default language content.
We didn't configure any index on relationships.",2022/3/14 9:13,2,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"Hey @jeanbmar _ I can shed some light on this for you quick.
This is absolutely something going wrong at some point in either custom hooks or a bug within Payload's code as that type of performance is absolutely unacceptable.
We've got sites in production with Payload right now that load up 60+ relationships, but the responses are returned in around 1 sec, which is a far cry from what you're experiencing. We'll get to the bottom of this.
Regarding the differences between how we populate and how Mongoose's internal populate works, you're right in that we make queries one-by-one, while Mongoose batches together queries based on collection.
Funny thing is that for the first ~2 years of Payload's development, we did use Mongoose's autopopulate plugin, but we had all sorts of issues with it and opted to remove it. We probably will not look back to using Mongoose's populate, because we are trying to set up as few low-level ties to Mongoose as possible in case we want to support other databases in the future.
We may be able to optimize the way that our own populate runs by doing something similar to what Mongoose has done. That said, when we ditched autopopulate, our performance impact was negligible enough to not even bat an eye. I think that's because our populations are all run in parallel _ meaning they all get populated right alongside of one another anyway. Still more database transactions, but they run in parallel.  So, batching them together is definitely going to provide some optimization, but it's not going to fix the problem you're seeing.
That problem definitely lies somewhere else.
Few questions for you:

Can you check to see if the _id field of each of your collections that use a custom ID is indexed, and unique, in MongoDB? This may be the problem. Note: this does not have anything to do with relationship fields themselves being indexed, because you're not filtering on those fields. This only has to do with the collection _id fields themselves being properly indexed.
Does this happen on both local development, and production environment(s)?
If you update to the newest beta of Payload, does this problem persist?
Are you using any custom Mongoose connection options?
",2022/3/15 16:00,3,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"Hey @jmikrut, thank you for helping on this.
Good call on the custom options.
We use the Mongo option maxPoolSize: 1 to prevent Vercel from exceeding MongoDB maximum allowed connections during our website build.
Removing this option brought back the query execution time to 12s. It's still too much for this relatively small amount of data but it's better.
It makes sense when I think about it. Our relationships must generate hundreds or thousands of queries, and MongoDB client must queue a lot of operations when there's a single connection available.
About the other questions,
I've checked the _id fields for all the collections and they seem to be indexed properly.
The index is not marked as unique but it seems to be something specific with the _id field in MongoDB (https://jira.mongodb.org/browse/SERVER-5335).
I've tried to insert a duplicate id in the collection and it is rejected properly.
I confirm the issue happens both in development and production environments, with both local and remote Payload instances.
I've made a test with Payload 0.14.31-beta.0 and response times are identical.",2022/3/16 10:54,4,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"I've enabled mongoose debug option.
There are 1184 individual queries performed for the example I gave. I think the network latency alone on so many queries will prevent a proper response time.
",2022/3/16 11:00,5,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,I just noticed I forgot to put the limit: 60 in the original example. The response times and logs I've given until now are for 10 root docs.,2022/3/16 11:55,6,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"Hey @jeanbmar _ ok this is all gold.
For now, I think it would be a very good idea to increase your maxPoolSize, even past the default of 100 if you'll be running complex queries in production.
But outside of that, I can see two further optimizations that we could make here:

Reduce the amount of queries that are run, specifically the countDocuments query if we are only doing a findByID or similar
Batch together populate queries like we discussed above (although because this is GraphQL specifically, this may be difficult to do. Will have to do some thinking regarding how this will work, because GraphQL relationship field resolvers each handle their own resolutions, obviously. We'd need to find a way to batch them all together and that might be tough.

What happens if you increase your maxPoolSize to 1000?",2022/3/16 14:01,7,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"I just made an attempt with maxPoolSize: 1000 but I saw no change in the response times. Actually I don't see any change above maxPoolSize: 8. It's probably because the pool size matters for parallel processing such as here: 

but doesn't matter for sequential queries such as here:


The pool size is probably close to optimal when it reaches the number of relationships. As long as there's only one user doing queries at least.
I agree with batching being difficult, in particular with GraphQL.
MongoDB queries are probably a major challenge for CMS software generally speaking, and even big players like Strapi haven't been able to achieve a clean implementation despite working with Mongoose maintainers.
The response time isn't killing our project because we are able to build every page beforehand fortunately, but it would be complicated if we had hundreds of users online triggering queries even smaller than this one.",2022/3/16 14:58,8,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"
but doesn't matter for sequential queries such as here:

These are actually also executed in parallel. The promises are all added to an array of promises where are all awaited in parallel. Can't see it in that file but all relationships are added to a top-level relationshipPromises array.
I'm surprised that increasing maxPoolSize had no effect! Interesting.
Nonetheless, we have identified 2 things that we are going to build right now which will ideally dramatically improve performance for GraphQL relationships:

Remove countDocuments queries where they're not needed (i.e. GraphQL relationship populations). This will reduce the queries made by half.
Memoize find operations on a request by request basis, like we've done already for the findByID operation. This will make it so that for example, if you've got 100 documents, and 50 of them has a relationship to a certain related doc, that related doc will only be retrieved from the database one time, stored, and then returned for all other subsequent requests afterward. This will also cut down on the actual number of queries made.

Give us a bit here and we'll get these two items published. If you could give them a shot, that would be great! We've also got a test repo setup that we'll be doing some testing with.
Will report back shortly!",2022/3/16 15:23,9,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"Hey @jeanbmar _ optimization 1, reducing countDocuments, is now released in 0.15.0.
Optimization #2 will be coming in the following days. Will report back here when it's in place.
For now, I'd like to suggest moving this post to Discussions, as it only deals with optimizations at this point. What do you think? If you agree, I'll move it!",2022/3/16 19:24,10,1,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,Perfect let's do this. Thank you.,2022/3/16 19:27,11,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"Hey there, sorry I never got back with the results.
The optimization has halved the queries and execution time so that's definitely positive, thank you.
This makes things okish for building a website on top of Payload but it is still not suitable for real-time usage like a service that would leverage Payload GraphQL API.
I believe you will have to address this situation when Payload becomes very popular (and you guys are doing a great job overall so I'm sure it will), because other people will deal with this issue and it can kill some projects.
A complex CMS setup will configure a lot of relationships, and running thousands of atomic queries like Payload is doing now will trigger different throttling and security systems on networks, db, etc.
You should consider an ensemblistic approach in my opinion.",2022/5/25 7:20,12,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"
ORMs such as MikroOrm let you temporaly catch (i.e during a request) the queries and objects loaded in memory to reduce the memory bloat.
This is standard practice under heavy load environments.

https://www.reddit.com/r/node/comments/tpv7iy/does_prisma_work_in_production/i2gj9ty/",2022/6/5 15:23,13,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/483,2022/3/13 15:31,"@jeanbmar just a heads-up, we did a lot of reading about this and realized that this is a common problem that GraphQL APIs face called the N+1 problem.
But, it has a beautiful solution! And we just implemented it!
#819
This should dramatically improve the performance of even the most complex queries. Once it's merged, will you give it a shot?",2022/7/25 19:20,14,,(VII) Information Storage
https://github.com/payloadcms/payload/discussions/537,2022/4/20 14:42,"Hey @AlessioGr _ I've just done some verification that these arguments do indeed populate properly, and I think I know what's going on here.
We run update access control in a few places:

Before you actually update a document (I've validated that the arguments are populated properly here).
Via the /api/access endpoint which is used for the admin panel to show which users can update which documents

In the second case, because we are just simply executing the update access control, without a related document, to determine if a user can update any document within a collection, there is no data or id argument. But, when you go to actually update a specific document, the access control will be executed with arguments defined properly.
Will you take a look to see if this produces your desired results?
PS - I am going to move this to a Discussion because we do indeed populate those arguments while you update document(s).",2022/4/20 15:21,1,1,(I) Non Actionable Topic
https://github.com/payloadcms/payload/discussions/726,2022/4/11 5:24,Thanks for reporting this @quornik. We're considering what the configuration settings will be for timezone and what effects it will have for the admin UI and APIs to solve the issue. We'll update again when we have a plan or fix ready.,2022/4/12 3:45,1,,(III) Not a Bug
https://github.com/payloadcms/payload/discussions/726,2022/4/11 5:24,"Thanks @DanRibbens.
You should also consider if I haven't rushed to quickly to call it a bug. Maybe it works properly and what it actually needs is just slightly updated documentation to the field. A warning of some kind that one should always localize the response.
If you think about it, the best practice is to adjust the timezone always on frontend, after the dateObject has been received and befere it is displayed, because the user's context should determine the way date is interpreted & displayed.
The scenario in which I had a problem with the way it works now, was this particular situation that I did not localize the response date before displaying. In every other situation it works fine.",2022/4/12 4:51,2,,(III) Not a Bug
https://github.com/payloadcms/payload/discussions/726,2022/4/11 5:24,"
If you think about it, the best practice is to adjust the timezone always on frontend, after the dateObject has been received and befere it is displayed, because the user's context should determine the way date is interpreted & displayed.

Exactly, @quornik. This isn't really a bug per se, rather, it's a potentially useful admin UX enhancement.
We're thinking through how to make this happen as Dan said and will report back here once we have more!",2022/4/12 13:42,3,,(III) Not a Bug
https://github.com/payloadcms/payload/discussions/726,2022/4/11 5:24,Since we deterimined there isn't anything in Payload to fix I'm moving this to a discussion for others to see.,2022/7/7 13:55,4,1,(III) Not a Bug
https://github.com/payloadcms/payload/discussions/726,2022/4/11 5:24,"Here's a use case that doesn't fit the current setup too well - I'm adding events using payload, however the events I'm adding take place in a different timezone to where I am currently.
It seems payload converts my local time to UTC and stores that - it means I have to do some calculating to input the time I want as it would be in my current timezone.
I think a possible solution would be to have the ability to select the timezone of the time you are entering, though I'm not sure if react-datepicker supports this.",2022/12/30 6:49,5,,(III) Not a Bug
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Someone started a port a few months or maybe even years ago. I think it was named black mesa, but I can't find it anymore right now.
There is also the excellent https://juliadynamics.github.io/Agents.jl/stable/, so I don't see the imminent need for another abm library on Julia.
That said I don't think there will be any resistance if you want to port it!",2021/1/19 13:52,1,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Their benchmark claims that Agents.jl is significantly faster than Mesa, with fewer LOC. Sounds very promising. People could be migrating to Julia for ABM instead of Python.
I'm still investigating why they have fewer LOC, while at the same time porting a subset of Mesa just enough to be able to run the forest fire example.",2021/1/28 2:44,2,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Ugh! The benchmark is not fair.
The Mesa code for forest fire includes data collection (see https://github.com/JuliaDynamics/Agents.jl/blob/4bf56d853cdaa47db8416ab92271d2dedfc499ff/benchmark/compare/Mesa/ForestFire/model.py#L63).
When I ran the benchmark as is, I got:

When I commented out all the data collection lines, I got

The Agents.jl implementation of forest fire contains no data collection at all: https://github.com/JuliaDynamics/Agents.jl/blob/4bf56d853cdaa47db8416ab92271d2dedfc499ff/src/models/forestfire.jl.
This is what I got for running on my laptop:

Which is definitely hard to beat.
Regarding with LOC, another reason why the Mesa version is longer is because of the boilerplate line like self.pos = pos, the code for data collection, and the wrapper for step() to include data collection. If all of these are addressed, I think the LOC is comparable to Agents.jl.",2021/1/28 9:48,3,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Also, since the Mesa benchmark code is copied/derived from Mesa's exmples/ code, there should be an attribution somewhere in Agents.jl repo.",2021/1/28 10:31,4,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,JuliaDynamics/Agents.jl#388,2021/1/28 10:43,5,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Great, it is already merged! The comparison is definitely motivating for optimizing mesa. Not necessarily the performance, which will be impossible, but also the LOCs (although, as you have shown, not the best metric).",2021/1/28 13:17,6,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Further unnecessary code in ForestFire:


Just on a first look!",2021/1/28 13:31,7,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"
Not necessarily the performance, which will be impossible, ...

Python may not be able to outrun Julia, but Mesa is even slower and more verbose than NetLogo in the outdated benchmark.",2021/1/28 14:58,8,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,#981,2021/1/28 15:10,9,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"
self.density = density

This line is necessary.",2021/1/28 15:12,10,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"

self.density = density

This line is necessary.

Why/how? Maybe I just don't see it, but it would be very helpful if you could explain your comments a bit more",2021/1/29 7:53,11,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Ah, never mind. Yeah, density is only used for the TreeCell placement. I will remove the 3 unused params initialization from the example.",2021/1/29 8:12,12,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,I removed them in #981,2021/1/29 8:20,13,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"From https://juliadynamics.github.io/Agents.jl/stable/#Comparison-with-existing-software

Agents.jl was originally inspired by the Mesa framework for Python, but has since then departed in design, leading to a dramatically simpler and cleaner API, ...
",2021/1/29 9:00,14,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Maybe @Libbum can explain a few of the design choices? There is stuff like the dict of agents lives in ABM struct in Agents.jl, while the dict lives in a scheduler in Mesa, but this is not crucial; I am still reading the code of Agents.jl. The slight naming differences of the functions are also not crucial.
Maybe the API simplifications can be incorporated into Mesa?",2021/1/30 12:04,15,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"Hi all!
So I've just taken over as lead dev on Agents.jl as of v4.0. Have been assisting since about v2.4.
My understanding is that v1 was mostly a Mesa clone and once it became a part of JuliaDynamics, started to diverge in some ways. Explicitly how though I'd really need to take some time on that to get you a decent answer, since I've never used Mesa personally. (I know how everything has changed, just unaware whether those changes are related to the Mesa design or not)
The benchmark files you're actively working on now and comparisons you see in our table in the documentation were completed by the original author who has now left the project.
That being said, I'm very happy to work with you on improving what we can together, for both packages and ABM frameworks in general.
One thing I know is a problem in terms of speed for Mesa is your continuous space. The way we (and MASON) manage this is we actually operate on a grid for neighbour searches etc, so we don't have to do a bunch of vector calculations to everyone in the space, just those in grid cells covered by some radius.",2021/1/30 18:32,16,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"I am catching up, but I wanted to say that I love everything about this thread and everything that came from it including the dialog that has started with @Libbum.",2021/2/9 5:16,17,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,Closing discussion linked to optimize continuous space module #993,2021/2/17 13:01,18,1,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,Why is the issue closed? Continuous space is not the only point in this design discussion.,2021/2/17 13:32,19,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,"@rht Sorry, I closed after the dev meeting discussion. The larger goal was to transition from a comparison to julia to actionable issues to take as Mesa transitions to GitHub discussions.
Please, feel free to add more action items to the issues list from this discussion. (Can we just transition this to discussions??)",2021/2/17 22:30,20,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,Did you mean GitHub Discussions? It isn't enabled yet. #987,2021/2/18 1:31,21,,(I) Non Actionable Topic
https://github.com/projectmesa/mesa/discussions/995,2021/1/19 5:49,@tpike3 can you leave this issue open until the Dicussions is enabled?,2021/2/18 11:04,22,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,I have the same question. @brian-brazil is this how we are expected to build a Histogram in a custom collector?,2021/8/31 23:41,1,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"Experimenting with this a bit, I was able to write a custom collector that utilizes the regular Histogram and not the HistogramMetricFamily with the following steps. All of this takes place in the Collect method of your custom collector.

Start with an empty list of metrics like metrics=[].
Create a Histogram with registry=None so that it is not registered to the default REGISTRY.
Add observations to the Histogram as you would outside of a custom collector.
Extend the metrics list with the return from histogram.collect().
Return the metrics list from the Collect method.

Example:

In your project's initialization code, ensure that the custom collector is registered.
I'm certain this is not supported, but implementing the Histogram's logic on my own sounds dreadful.",2021/9/1 0:28,2,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"That's how I'd do it, keep in mind that you possibly want to switch the type to GaugeHistogram before sending it on if you're using a custom collector like this.",2021/9/1 7:17,3,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"Nice, glad I am on the right track. What is the benefit of using the GaugeHistogram?",2021/9/1 15:11,4,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"It's a type thing, so may be used by downstream tooling for better hints etc.. If the data is based on counters then it's a Histogram. If it's a snapshot of current state it's a GaugeHistogram.",2021/9/1 16:04,5,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"Interesting, I've never seen a GaugeHistogram being used anywhere else. The use case I have is for buckets that increment (as with classic Histograms) so I think I'm safe to keep the current type, but is there any existing documentation or future plans to include documentation for this type? And maybe for other types like StatefulSet which I've never seen before looking through the source.
Appreciate the help!",2021/9/1 17:52,6,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"Also @cunningr to keep a running count of the metrics that you're proxying (in the case of a classic Histogram), it wouldn't be a bad idea to use an instance variable with locking in the Collect method to ensure that multiple scrapes don't step on each other. In my example, I'm using gevent but you should use whatever library is appropriate to your project.",2021/9/1 18:40,7,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"You should create a new Histogram for each collect, otherwise you're risking race conditions.",2021/9/1 19:34,8,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"Ah yes, my mistake. It's the job of Prometheus to collect the scrapes and store them within the Histogram. :)",2021/9/1 19:41,9,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"Well.. actually, am I correct about that? For example, I have a Counter that is tracking the number of times a specific operation is performed. That counter is instrumenting this exporter that I'm writing, and therefore is not recreated on each scrape.
How do I ensure that this Histogram from the Custom Collector exposes buckets that are potentially incrementing on each /metrics call with new data from the external API?",2021/9/1 19:54,10,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"
_",2021/9/2 6:49,11,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,"In that case, manually putting together the Histogram's samples is what I'd do.",2021/9/2 8:38,12,,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/691,2021/7/16 14:05,We (the Prometheus team) are trying out Github discussions so I moved this as it is not an issue with the library. Thanks all for the great answers.,2021/9/2 10:49,13,1,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/713,2021/10/19 12:27,"Hello, I converted this from an issue to a discussion as it appears to be a usage question.
You should not be including the value of a metric as labels such as in your example. Every time your disk usage changes right now Prometheus will create a new series which will be very expensive and hard to use. Typically you will want multiple metrics each with the value. For example:

Does that help? You can take a look at some of the metrics in the windows exporter to get an idea: https://github.com/prometheus-community/windows_exporter/blob/master/docs/collector.logical_disk.md.",2021/10/29 15:00,1,1,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/721,2021/11/4 6:40,"Hello,
I converted this to a discussion instead of an issue as it appears to be a usage question, not a bug/feature request for client_python.
Prometheus will handle metrics across replicas for you by adding an {instance=""my-host:8080""} label to each series when ingested. You can then tell the series for each machine apart when querying using the instance label. You can read more about that behavior here: https://prometheus.io/docs/concepts/jobs_instances/.
That said, multiprocess mode does not work across instances in most cases. Your concerns are both valid, as pids are used to identify metrics (and could be re-used by different hosts), and PROMETHEUS_MULTIPROC_DIR would need to be shared and writeable by each machine. I would definitely not recommend trying to use multiprocess mode across machines :).",2021/11/8 21:34,1,1,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/739,2021/12/14 21:56,"Hello, I just converted this to a github discussion.
There is not a supported way to avoid resets when an application restarts, and the multiproc directory must be wiped. However, Prometheus is designed to handle resets in counters whenever a process is restarted so it shouldn't cause too much of an issue. What sort of issues/missing information are you seeing?",2021/12/17 20:05,1,1,(I) Non Actionable Topic
https://github.com/prometheus/client_python/discussions/750,2022/1/18 15:10,"Hello, I converted this to a discussion as this behavior is by design, not an issue with the client.
Typically, when you want to create an exporter collecting data from another source you will typically want to use a Custom Collector to create the values at scrape time so the information is correct for that moment. You can also specifically call power_gauge.remove('<some-meterid>) to remove a metric that has disappeared, but keeping the state is usually harder than populating the state correctly during each scrape using a custom collector.
Hope that helps!",2022/1/18 19:02,1,1,(III) Not a Bug
https://github.com/pycontribs/jira/discussions/1052,2021/5/20 10:58,Thanks for moving into discussion,2021/5/22 3:00,1,1,(II) Invalid Issues
https://github.com/pycontribs/jira/discussions/1052,2021/5/20 10:58,"Summarizing the previous thread.
The field in question was a Jira Server MultiUserPicker:
https://developer.atlassian.com/server/jira/platform/jira-rest-api-examples/#multiuserpicker
Which wants the payload in the following form:

So the following code works:
",2021/5/29 14:54,2,,(II) Invalid Issues
https://github.com/pydata/xarray/discussions/6010,2021/11/14 11:50,"I decided to also look at what happens when you plot with contourf. In this case both the plot of the original data and the interpolated data have a white line at the central longitude, but the interpolated data also has white lines at the poles:
Original MPI-ESM

Interpolated MPI-ESM

Here's the code that produced the plots:
",2021/11/14 13:28,1,,(I) Non Actionable Topic
https://github.com/pydata/xarray/discussions/6010,2021/11/14 11:50,"Xarray's built-in interpolation method does not know how to handle periodic coordinates, which is why you see the blank center line.  I would recommend using xESMF, which is an xarray wrapper of some code specifically written to interpolate from one geospatial grid to another, and knows how to handle periodic coordinates (see the periodic flag in the constructor of the Regridder object).  It also provides multiple different interpolating methods which, depending on your application, might be more applicable than xarray's options.",2021/11/14 14:13,2,,(I) Non Actionable Topic
https://github.com/pydata/xarray/discussions/6010,2021/11/14 11:50,"@spencerkclark Thanks for the suggestion! I haven't made any serious tests yet, but my initial tests worked fine =)",2021/11/14 22:56,3,,(I) Non Actionable Topic
https://github.com/pydata/xarray/discussions/6010,2021/11/14 11:50,"If this solves your issue all is fine. However, I am a bit surprised that the pcolormesh creates a white line - can you print the lon coords? For the contourf you may need to add a cyclic point.",2021/11/15 13:20,4,,(I) Non Actionable Topic
https://github.com/pydata/xarray/discussions/6010,2021/11/14 11:50,"@mathause I think the issue is that non-periodic interpolation introduces NaNs into the dataset, which get plotted as a white line.",2021/11/15 13:29,5,,(I) Non Actionable Topic
https://github.com/pydata/xarray/discussions/6010,2021/11/14 11:50,"As an aside, this is something that we might want to try and fix via a PeriodicIndex, once the explicit indexes refactor is complete.",2021/11/16 4:15,6,,(I) Non Actionable Topic
https://github.com/pydata/xarray/discussions/6010,2021/11/14 11:50,@huaracheguarache can I convert this issue to a discussion? I think it would better fit there.,2021/11/16 12:56,7,1,(I) Non Actionable Topic
https://github.com/pydata/xarray/discussions/6010,2021/11/14 11:50,"@mathause Sorry for the late reply! I've been very busy lately, but yes, please move it to a discussion.",2021/11/19 22:10,8,,(I) Non Actionable Topic
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"I often miss the ability to specify a shape different than the observed data for PP sampling. I think this Discourse issue is about the same: https://discourse.pymc.io/t/posterior-predictive-check-limited-to-certain-sample-sizes/7120/3
Is this what you meant with this Issue? If not, could you give a minimal example?",2021/3/31 12:36,1,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,aren't we using dims and coords instead of shape now(or atleast encouraging that)?,2021/4/5 6:46,2,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"For a lot of models like linear regression there are two pm.Data() for the X and Y (this one goes into the observed).
Then when doing post pred sampling we need to update X and Y, but Y can just be dummy placeholder values. I think setting Y in this case is unnecessary as the shape of X just translates through to the output.
I'm not quite sure how to handle the post pred shape in the case where the likelihood shape is not specified externally (via an X). Maybe as a kwarg to sample_posterior_predictive?",2021/4/5 9:05,3,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"maybe in the second occurence of pm.data, we can just do pm.set_data({'data': data_vals})
in between final sampling and initial input there's one data container / placeholder to train part right?

think setting Y in this case is unnecessary as the shape of X just translates through to the output.

I might be missing something but we need some sort of placeholders for X right, which we train during sample_pp. I'm not sure if I make sense, I'm somewhat speculating I need to understand the whole coords dims thing properly again.",2021/4/5 9:23,4,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"@almostmeenal We still need to set X, no way around that, as we still need the features/predictors.
There's analog to scikit-learn. For us, specifying and sampling the model is basically sklearnmodel.fit(X_train, y). Their sklearnmodel.predict(X_test) is for us:

I would like us to get rid of having to set Y above.",2021/4/5 13:24,5,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"
pm.set_data({""X"": X_test, ""Y"": np.empty(X_test.shape[0])})
pm.sample_posterior_predictive()

got it, thanks for clarifying. So is the likely solution then only removing observed data?",2021/4/5 19:36,6,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"So that would work for the case where there's an X to specify the shape. However, what if that's not the case? (When writing this issue I hadn't considered this complexity, but maybe we can think it through here).
An example would be the coin-flip model where I infer a p of a Bernoulli with n coin-flips (specified by the length of the data I pass in). In that case if I just call pm.sample_posterior_predictive() I would get pp samples of length n which is what I want. In that case we couldn't just remove the data. If we want to get more or less length, we would need to use pm.Data for the observed data and update with a place-holder. There we also couldn't replace.
So the key question is if the shape of the observed is specified by its inputs (like when an X is present) or not. And I'm not sure we can automatically infer that.
One solution might be to rely on the dims kwarg if it's present to specify the shape. Although I'm not sure we have a method of updating that. Perhaps this needs to wait for something like https://github.com/pymc-devs/aesara/issues/352.
Anyone got ideas?",2021/4/5 20:08,7,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"If I'm understanding this correctly, all this is already covered in v4.",2021/4/6 0:17,8,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,@brandonwillard Neat! How do I set the target shape in v4?,2021/4/6 4:01,9,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"Here's a small example in v4:

The original shape of the observations is 200:

Change the shape of the observations to 100:


Change the shape to (2, 3, 100) (i.e. sample a size of (2, 3)):


The same can be done for any other shared variables (i.e. not just observed ones) or variables that get their shapes from the shape of a shared variable (e.g. regressions parameters).",2021/4/19 22:22,10,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,This issue needs a bit more discussion to understand what is the behavior we want. I don't think we want to always ignore the shape of the observed variables.,2022/2/19 10:40,11,1,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"I often miss the ability to specify a shape different than the observed data for PP sampling. I think this Discourse issue is about the same: https://discourse.pymc.io/t/posterior-predictive-check-limited-to-certain-sample-sizes/7120/3
Is this what you meant with this Issue? If not, could you give a minimal example?",2021/3/31 12:36,12,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,aren't we using dims and coords instead of shape now(or atleast encouraging that)?,2021/4/5 6:46,13,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"For a lot of models like linear regression there are two pm.Data() for the X and Y (this one goes into the observed).
Then when doing post pred sampling we need to update X and Y, but Y can just be dummy placeholder values. I think setting Y in this case is unnecessary as the shape of X just translates through to the output.
I'm not quite sure how to handle the post pred shape in the case where the likelihood shape is not specified externally (via an X). Maybe as a kwarg to sample_posterior_predictive?",2021/4/5 9:05,14,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"maybe in the second occurence of pm.data, we can just do pm.set_data({'data': data_vals})
in between final sampling and initial input there's one data container / placeholder to train part right?

think setting Y in this case is unnecessary as the shape of X just translates through to the output.

I might be missing something but we need some sort of placeholders for X right, which we train during sample_pp. I'm not sure if I make sense, I'm somewhat speculating I need to understand the whole coords dims thing properly again.",2021/4/5 9:23,15,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"@almostmeenal We still need to set X, no way around that, as we still need the features/predictors.
There's analog to scikit-learn. For us, specifying and sampling the model is basically sklearnmodel.fit(X_train, y). Their sklearnmodel.predict(X_test) is for us:

I would like us to get rid of having to set Y above.",2021/4/5 13:24,16,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"
pm.set_data({""X"": X_test, ""Y"": np.empty(X_test.shape[0])})
pm.sample_posterior_predictive()

got it, thanks for clarifying. So is the likely solution then only removing observed data?",2021/4/5 19:36,17,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"So that would work for the case where there's an X to specify the shape. However, what if that's not the case? (When writing this issue I hadn't considered this complexity, but maybe we can think it through here).
An example would be the coin-flip model where I infer a p of a Bernoulli with n coin-flips (specified by the length of the data I pass in). In that case if I just call pm.sample_posterior_predictive() I would get pp samples of length n which is what I want. In that case we couldn't just remove the data. If we want to get more or less length, we would need to use pm.Data for the observed data and update with a place-holder. There we also couldn't replace.
So the key question is if the shape of the observed is specified by its inputs (like when an X is present) or not. And I'm not sure we can automatically infer that.
One solution might be to rely on the dims kwarg if it's present to specify the shape. Although I'm not sure we have a method of updating that. Perhaps this needs to wait for something like https://github.com/pymc-devs/aesara/issues/352.
Anyone got ideas?",2021/4/5 20:08,18,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"If I'm understanding this correctly, all this is already covered in v4.",2021/4/6 0:17,19,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,@brandonwillard Neat! How do I set the target shape in v4?,2021/4/6 4:01,20,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,"Here's a small example in v4:

The original shape of the observations is 200:

Change the shape of the observations to 100:


Change the shape to (2, 3, 100) (i.e. sample a size of (2, 3)):


The same can be done for any other shared variables (i.e. not just observed ones) or variables that get their shapes from the shape of a shared variable (e.g. regressions parameters).",2021/4/19 22:22,21,,(IV) Further Discussion
https://github.com/pymc-devs/pymc/discussions/5606,2021/3/29 11:18,This issue needs a bit more discussion to understand what is the behavior we want. I don't think we want to always ignore the shape of the observed variables.,2022/2/19 10:40,22,,(IV) Further Discussion
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,"Works fine for me:

Can you maybe share a built docker image where this can be reproduced?",2020/10/12 14:57,1,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,"Thanks for he quick feedback.
When creating a docker image with the following .dockerignore I noticed I couldn't reproduce the problem:

I pinned the issue to the __pycache__ directory:

The error file /Users/benjaminbertrand/Dev/web/flask/test-factoryboy/tests/test_models.py, line 1: source code not available should have warned me. This isn't the path of the file inside the docker container but on my machine...
Removing test_models.cpython-38-pytest-6.1.1.pyc made this error disappear but the fixture was still not found. I had to remove conftest.cpython-38-pytest-6.1.1.pyc.
In this example, those files were generated on my mac, so I understand I should have cleaned that directory.
In my original problem, the tests are always run inside the docker container, but I mount the app directory as a volume. I found some pyc files generated:

Moving the conftest.cpython-38-pytest-6.1.1.pyc solved the issue. I know some requirements had changed when I re-run the tests and I think I updated the Python 3.8 version in between. I'm still not sure why I had to remove that file.",2020/10/13 5:34,2,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,I saved the old pyc file. Is there a way to find the differences?,2020/10/13 5:45,3,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,"After checking I saw that VS Code is creating that file when discovering the tests. So I guess that's what happened.
You can close this issue. Sorry for the noise.",2020/10/13 7:32,4,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,"Hmm, I still wonder how this can happen though! By any chance, can you reproduce when you run pytest --collect-only first, then run pytest normally (with the pyc generated by the first run)?",2020/10/13 7:58,5,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,"If I run pytest --collect-only on my mac and then run pytest in the docker container, yes, I can reproduce the issue.
I created this repo https://github.com/beenje/test-pytest-factoryboy and pushed the docker image beenje/test-pytest-factoryboy with the __pycache__ directory that was created locally.
If you pull this image, you can reproduce the problem. Running pytest inside will fail if you don't remove the pyc file:
",2020/10/13 14:03,6,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,"To summarise, it seems the full path of the files is used inside the pyc files instead of relative path. This isn't a platform issue. It can easily be reproduced using docker by mounting the same directory in 2 different locations:

Same issue with pytest 5 but not with pytest-4.6.11:
",2020/10/15 6:33,7,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,I'm not really familiar with how pytest generates/reads .pyc files as part of its assertion rewriting - maybe someone else can look into this?,2020/10/15 9:37,8,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,"We check for the mtime to see if we need to regenerate the .pyc file; because it is the same file in different locations, pytest won't regenerate the .pyc file when executed from the different mount.
This is not pytest specific, Python stores full paths inside .pyc files; often this doesn't matter, but for pytest the file location is important.
I'm not sure there's anything to be done by pytest here. Perhaps we can edit the title and turn this into a discussion for others to find in the future?",2020/10/17 12:07,9,1,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/7890,2020/10/12 13:32,"I agree. I don't think pytest should try to handle this. It's fine to close this issue. It's more something to be aware.
Feel free to change the title.",2020/10/18 17:22,10,,(VII) Information Storage
https://github.com/pytest-dev/pytest/discussions/8063,2020/11/23 2:21,"I took the freedom to convert this to a discussion, since this is a question rather than an issue in pytest.
I don't think there's a way to do this at the moment - like you say, collection happens before execution of the tests. What problem are you trying to solve exactly?",2020/11/24 10:17,1,1,(I) Non Actionable Topic
https://github.com/pytest-dev/pytest/discussions/8388,2021/3/2 11:41,@alex can you help me in solving it.,2021/3/2 11:42,1,,(III) Not a Bug
https://github.com/pytest-dev/pytest/discussions/8388,2021/3/2 11:41,"This seems like an issue with your tests or environment, and not a bug in pytest:

I'm thus converting this to a discussion.",2021/3/2 15:25,2,1,(III) Not a Bug
https://github.com/pytest-dev/pytest/discussions/8486,2021/3/23 20:17,"This isn't an issue in pytest:

I'm converting this to a discussion.",2021/3/23 20:54,1,1,(VI) Unrelated Repository
https://github.com/pytest-dev/pytest/discussions/8486,2021/3/23 20:17,"This would happen the same way with unittest.
The problem is that C.value is read the first time the prod module is loaded. Reimporting C does not help, because it is already cached.
If you want to use a mocked version you have to remove it from the module cache before importing it, e.g. something like:

You may have to do the same for the other test.",2021/3/23 21:14,2,,(VI) Unrelated Repository
https://github.com/pytest-dev/pytest/discussions/9470,2022/1/1 6:50,"If you launch pytest with -o faulthandler_timeout=5 (adjust seconds as needed), it should give you a trackeback of the hang. Where does that point to? FWIW I doubt this is a bug in pytest.",2022/1/3 10:50,1,,(III) Not a Bug
https://github.com/pytest-dev/pytest/discussions/9470,2022/1/1 6:50,"this is the output
",2022/1/3 13:33,2,,(III) Not a Bug
https://github.com/pytest-dev/pytest/discussions/9470,2022/1/1 6:50,"according to the traceback, it seems like you have a network hangup thats not running to completion and is not wrapped with a timeout/cancelation",2022/1/3 14:15,3,,(III) Not a Bug
https://github.com/pytest-dev/pytest/discussions/9470,2022/1/1 6:50,"Taking the freedom to convert this to a discussion, as it indeed doesn't look like a bug in pytest.",2022/1/3 17:55,4,1,(III) Not a Bug
https://github.com/pytest-dev/pytest/discussions/9470,2022/1/1 6:50,"Hi,
I had the same problem. For me it was creating new process in AppConfig.ready() in Django App. Pytest-django runs django app when running tests, hence it was also running that new process. As I was not closing this additional process in tests, the process was running forever causing pytests to go in circles and not shutting down.
The quick workaround is to add shutting down the process at the end of all tests, i.e. by using pytest_sessionfinish.
I hope it will be useful for someone in the future.",2022/5/16 10:37,5,,(III) Not a Bug
https://github.com/python-gitlab/python-gitlab/discussions/2095,2021/2/4 23:41,"This question would be more for upstream gitlab, but you can maybe try - the default level is Participating. I'm assuming you mean global settings.

Not sure if this overrides custom settings. Otherwise you'd need to loop and set them all to True/False as needed. And also loop over groups/projects you're a member of for non-global settings probably.",2021/2/5 0:22,1,,(VII) Information Storage
https://github.com/python-gitlab/python-gitlab/discussions/2095,2021/2/4 23:41,@nyue I think this was answered but I'll move it to discussions in case anyone wants to add their own ideas :),2022/6/25 10:12,2,1,(VII) Information Storage
https://github.com/rasterio/rasterio/discussions/2406,2015/10/29 16:14,It seems like the windows users are generally going for conda anyways. Let's defer this until we have a real need or a champion to take the lead on it.,2016/6/10 15:52,1,,(VII) Information Storage
https://github.com/rasterio/rasterio/discussions/2406,2015/10/29 16:14,"I would love to see this, it would make Windows CI much easier. Right now, we have an entire section of our GitHub Actions just for installing the few packages without Windows wheels. Ideally a simple pip install would be enough to install all of our dependencies on all platforms.",2022/3/11 3:26,2,,(VII) Information Storage
https://github.com/rasterio/rasterio/discussions/2406,2015/10/29 16:14,I turned this from an issue into a discussion. There's no plan to do anything about this right now.,2022/3/11 3:41,3,1,(VII) Information Storage
https://github.com/rasterio/rasterio/discussions/2406,2015/10/29 16:14,"FWIW, rtree has Windows wheels. The CI code to do this doesn't look too bad. Maybe @hobu can comment on how easy/hard it is to get this working.",2022/3/12 16:21,4,,(VII) Information Storage
https://github.com/rasterio/rasterio/discussions/2406,2015/10/29 16:14,"I think @perrygeo captured the current situation: ""until we have ... a champion to take the lead on it.""
I could be interpreting the situation incorrectly, but I don't think there is opposition to having Windows wheels. The rasterio devs just don't have the time/desire to support and maintain Window wheels. So, if there is someone or a group of people willing to step up to both add a automated method to build Windows wheels and provide active support, then there is a good chance for this to become a reality. Otherwise, it is likely not going to happen.
Side note, unofficial Window wheels: https://www.lfd.uci.edu/~gohlke/pythonlibs/#rasterio",2022/3/12 16:39,5,,(VII) Information Storage
https://github.com/rasterio/rasterio/discussions/2406,2015/10/29 16:14,Related: geopandas/pyogrio#48 (potentially use vcpkg to simplify things),2022/3/18 1:31,6,,(VII) Information Storage
https://github.com/rasterio/rasterio/discussions/2406,2015/10/29 16:14,Shapely also has Windows wheels and might be a better starting point since it shares many of the same developers. Shapely uses cibuildwheel as well: https://github.com/shapely/shapely/blob/main/.github/workflows/release.yml,2022/4/5 19:28,7,,(VII) Information Storage
https://github.com/rasterio/rasterio/discussions/2406,2015/10/29 16:14,rasterio/rasterio-wheels#86,2022/6/27 19:31,8,,(VII) Information Storage
https://github.com/rq/rq/discussions/1774,2021/6/23 18:23,"Since your load balancers are already redirecting traffic away from servers running old code versions, I think it makes sense to also stop RQ workers running on those servers and start again after the new code is properly deployed.
Does this make sense?
Having said that, I'm also open to ideas about implementing some kind of ""filter"" such that certain workers will skip certain jobs, but I'm not sure how we can achieve that in a scalable way.",2021/8/21 12:15,1,,(I) Non Actionable Topic
https://github.com/rq/rq/discussions/1774,2021/6/23 18:23,"Hey @selwin thanks for the reply! I misspoke about our load balancers' capabilities. They don't detect old code versions, they only detect servers undergoing deployment. So in a 3-server deployment A-B-C, server B will be not receive traffic during its ""deployment"", while A and C will receive traffic using different code versions. This is typically fine, but with workers server A could enqueue a Job for a function that C doesn't have yet (since it's behind a code version). If server C's workers pick up that job, it fails.
The only thing we can come up with so far is not a ""filter"", but overrides check_for_suspension on the Worker. Here's a snippet:

Do you see any issues with this implementation? It works well in our isolated tests (although the biggest struggle right now is the code for check_appropriate_code_version, which has nothing to do with RQ)
I could see some scalable way of doing this. This implementation of check_for_suspension is essentially the same as Worker's implementation, with the additional condition of check_appropriate_code_version. Seems to be that all that would be needed is a list of callables that return booleans to determine suspension.
",2021/8/23 15:24,2,,(I) Non Actionable Topic
https://github.com/rq/rq/discussions/1774,2021/6/23 18:23,"Maybe you could use a different Queue for the other version? Old workers with 'old' queues and new workers with 'new' queues for example
Never tried different queues to be honest, but it seems like a solution",2021/10/20 19:12,3,,(I) Non Actionable Topic
https://github.com/rq/rq/discussions/1774,2021/6/23 18:23,"This is an interesting issue indeed. However interesting this may be, I do think this will always be very specific to the use case/environment: what is check_appropriate_code_version, how one determines what is ""appropriate"" or not is mostly related specific to the application. For that reason, I don't really think this is something RQ should handle.
This does seem a perfect example of how useful custom Worker/Job/Queue classes can be to solve complex issues, though.
I'm moving this to a Discussion.",2023/1/28 4:28,4,1,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/19699,2021/3/16 7:43,"
and I try all the method that I find in google
still can not fix it

Sorry to ask but have you tried the suggestions in the error message above? If you have, please report the issues you found. In particular for

Also why do you need 0.21, and what arch are you building on?",2021/3/16 8:18,1,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/19699,2021/3/16 7:43,"I try your suggestion
it didn't raise previous problem anymore but it raise another error
which is

before that there is a lot of red warning and error
sorry for throw another error  again
and the reason I want to use 0.21.3 is because I want to run a project code called Plasclass
it need to use module 'sklearn.linear_model.logistic'
and 0.24.1 will throw a model can't find error
ModuleNotFoundError: No module named 'sklearn.linear_model.logistic'",2021/3/16 9:48,2,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/19699,2021/3/16 7:43,"I think it is unlikely that this is a problem with scikit-learn. I feel this is more a general ""how do I get out of weird issues when installing something with Python""  question or maybe an installation question for  https://github.com/Shamir-Lab/PlasClass (it that is the PlasClass package you are trying to use).
As a general recommendation it is a very bad idea to mix conda and pip for the same package. This can happen without realising, e.g. conda install scikit-learn + pip install PassClass will replace a conda-installed scikit-learn with a pip-installed one and weird things can happen. This is because PassClass insists on very specific version of scikit-learn (also numpy and scipy) in its setup.py.
I would first try to create a separate conda environment and see whether the issue is still there. This is what I would try (wild-guesses because it is hard to help you with very partial information):
First approach: the ""conda for environments + full pip"" way
Probably preferable here because PassClass insists on very specific version of scikit-learn (also numpy and scipy) in its setup.py.

Alternative: the ""conda for most packages + pip on top of it when you absolutely need it"" way

Misc
not sure why from your logs, pip is trying to build from source rather than use a wheel, maybe someone else will have a better idea about this ...",2021/3/17 8:43,3,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/19699,2021/3/16 7:43,"Migrating this to a discussion, which it should probably have been in the first place ...",2021/3/17 8:46,4,1,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20012,2021/3/31 10:49,"I'm not sure who is mainly responsible for spectral clustering within scikit-learn these days, but it seems like @GaelVaroquaux is the original author of that main module? Just trying to get some eyes on this issue.",2021/4/13 20:15,1,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20012,2021/3/31 10:49,"Hi @chrisyeh96, thanks for reaching out. I'm under the impression that this is more a general question, as at this stage it is difficult to understand how the code should be modify to fix the issue. I'm going to move this issue to discussions, where I hope it will bring more attention. Thanks for your patience.",2021/4/30 14:04,2,1,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20244,2021/6/8 22:26,"Please extend your code snippet so that it fully reproduces your issue. Include imports, etc",2021/6/8 22:59,1,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20244,2021/6/8 22:26,"Here's a more minimal example:
",2021/6/9 10:56,2,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20244,2021/6/8 22:26,"At first sight I'm not too shocked about this: joblib will fork new processes, which means that each of these processes has a copy of the memory of the parent process, i.e. they each have their own version of train_loss_history, and only the local copies of it are modified. The parent process's version of train_loss_history is left untouched.
If you change the backend to threads (where memory is shared), you'll get the expected behaviour:

But depending on how your DualSVR is implemented, threading may not be ideal because of the GIL.
There might be some joblib stuff that I'm unaware of that could enable such memory sharing between processes. Perhaps @ogrisel or @jeremiedbb ?",2021/6/9 11:09,3,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20244,2021/6/8 22:26,@NicolasHug I just realized this following this link,2021/6/9 11:17,4,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20244,2021/6/8 22:26,I'm glad these docs are helpful <3,2021/6/9 11:19,5,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20244,2021/6/8 22:26,I am moving this topic to a discussion,2021/6/10 14:04,6,1,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20690,2021/8/6 7:53,"@sohail759 I moved your question in the discussion which is the right place to ask a question.
The issue tracker is to follow bugs and the implementation of new features.
To answer your question, you can check the following example:
https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py
The line of interest is the following:

Basically, you get the handle from the axis and then use it to add the title.",2021/8/6 7:56,1,1,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20690,2021/8/6 7:53,"
_",2021/8/6 11:20,2,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20690,2021/8/6 7:53,"
",2021/8/6 11:57,3,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20690,2021/8/6 7:53,"
_",2021/8/6 12:10,4,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20690,2021/8/6 7:53,"
_",2021/8/6 12:43,5,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20690,2021/8/6 7:53,"
_",2021/8/6 13:21,6,,(I) Non Actionable Topic
https://github.com/scikit-learn/scikit-learn/discussions/20756,2021/8/15 15:43,I convert this topic to a discussion because it should only be an issue of installation. How do you install scikit-learn?,2021/8/16 12:19,1,1,(III) Not a Bug
https://github.com/sktime/sktime/discussions/683,2021/2/14 13:03,"Quick comment: this suggests a major API change!
Reversing y and X was a conscious API choice due to the differences between tabular learning tasks and forecasting.
Note that in forecasting, X is optional while y is not, whereas in sklearn tabular learning, y is optional (in unsupervised instances) while X is not!
The rule ""optional arguments come last"" implies the ordering already.
That is not superseded by ""we could make y optional too"", since it does not change the ""moral story"", i.e., the semantic optionality.",2021/2/14 13:09,1,,(VII) Information Storage
https://github.com/sktime/sktime/discussions/683,2021/2/14 13:03,pinging @mloning for the forecasting module,2021/2/14 13:10,2,,(VII) Information Storage
https://github.com/sktime/sktime/discussions/683,2021/2/14 13:03,"An alternative we discussed internally is whether in forecasting, the symbols y and X should be exchanged while leaving the semantic meaning intact, i.e., y/Y for exogenous variables and X for endogenous ones. We decided against that due to the common association of the symbol y with ""target"" and X with ""side information"" being stronger than the association of X with ""core/non-optional data"".",2021/2/14 13:13,3,,(VII) Information Storage
https://github.com/sktime/sktime/discussions/683,2021/2/14 13:03,"Hi @fkiraly, Thanks for the quick feedback. I understand this was a conscious choice, but making sktime compatible with sklearn and other sklearn compatible API calls would open up more functionality to be built around sktime.",2021/2/14 13:15,4,,(VII) Information Storage
https://github.com/sktime/sktime/discussions/683,2021/2/14 13:03,"scikit-learn's API is for cross-sectional learning, forecasting and other time series tasks are considerably different to cross-sectional learning. I think it makes sense to have a different interface for different tasks. For more general design principles behind sktime, see our paper. The rationale for the forecasting API is also discussed in more details in this enhancement proposal. Feedback is very welcome!
Making estimators fully compatible with the scikit-learn interface also requires changing the predict method by the way, so you could do something like fbprophet does, but I don't think it's very intuitive, and it uses the same symbols, but changes the meaning of the symbols. In scikit-learn, X is cross-sectional data whereas in sktime, it'd be time series data. One also has to be careful to apply functionality from the cross-sectional setting to forecasting, because some assumptions do not hold (e.g. rows do not represent i.i.d. samples).
sktime is still interoperable with scikit-learn through the reduction interfaces (e.g. ReducedRegressionForecaster). Why doesn't that work in the above example? What other functionality do you have in mind that doesn't work with the current interface?",2021/2/14 14:12,5,,(VII) Information Storage
https://github.com/sktime/sktime/discussions/683,2021/2/14 13:03,"Looks like the discussion above is settled?
I think it's unlikely we'll change the signature soon, of course discussion is always welcome.",2022/2/6 19:39,6,1,(VII) Information Storage
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"Hi,
As indicated in the documentation, the proper uri string is postgresql not postgres.
See https://docs.sqlalchemy.org/en/13/dialects/postgresql.html#module-sqlalchemy.dialects.postgresql.psycopg2",2020/12/26 15:36,1,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"""postgres"" has been deprecated for many years, see https://docs.sqlalchemy.org/en/14/changelog/changelog_14.html#change-b2b94629c3428a7a3d1e7d10546de36f for removal note",2020/12/26 15:46,2,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,Moving to discussion,2020/12/26 22:12,3,1,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"great, indeed. but it was running like this for years, without warning :)",2020/12/28 19:27,4,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"Python doesn't show warnings by default these days and you need to have appropriate warnings flags enabled, these flags are typically turned on by test frameworks also:
",2020/12/28 20:07,5,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"FYI, this bit me as well, thanks for opening this as I was able to find this easier than the notes about the actual documentation.",2021/3/17 17:39,6,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"Thank you for posting this, it has been bugging me intermittently for a long time. Admittedly, in the traceback there was a reference to 'deprecations.py' in relation to the dialect statement I was using so this should have been obvious.",2021/4/8 2:40,7,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"this is a bit of a problem, given that postgres:// is incredibly popular. for example, i just deployed to Heroku and immediately got bitten. and Heroku doesn't allow you alter DATABASE_URL, at least for hobby databases",2021/5/29 5:13,8,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"whoever decided to deprecate ""postgres://"": this is a strange decision. it maybe did cost this library 2 loc to maintain. Removing it spawned tons of issues on GitHub and Stackoverflow (for this library and tools depending on it) and forced tons of users to implement ""if startswith then replace""-boilerplate.",2022/1/21 10:18,9,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5799,2020/12/26 10:39,"In case anyone found this discussion but is already using the correct URI scheme:
A similar warning can be raised by accidentally passing a postgres_where kwarg instead of the correct postgresql_where when creating a partial index.",2022/4/11 8:55,10,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"Also for this method not needed add args and kwargs because for bound we can use lambda. For example
",2021/1/5 20:05,1,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"Hi,

We run something asynchronously.

Actually the function inside run_sync is run with a synchronous style api, so without using await etc. That is what gives the name to the function",2021/1/5 20:24,2,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"Hi,

Actually the function inside run_sync is run with a synchronous style api, so without using await etc. That is what gives the name to the function

But we, as users, look at it from the user's point of view. From the user's point of view, we take a callback and launch it not now, but sometime, but now we just get a future like object. And how it's done inside is already implementation details.",2021/1/5 20:48,3,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"if the function were named after its implementation detail it would be called run_in_greenlet().
a more correct name would be run_synchronous_blocking_style_code_in_asyncio() but that's kind of long.   run_async() definitely is misleading because you're already in asyncio, everything is ""async"".   saying ""await"" already means you are running something asynchronously.",2021/1/5 20:59,4,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"
Hi,

Actually the function inside run_sync is run with a synchronous style api, so without using await etc. That is what gives the name to the function

But we, as users, look at it from the user's point of view. From the user's point of view, we take a callback and launch it not now, but sometime, but now we just get a future like object. And how it's done inside is already implementation details.

I kind of see your point. Sill I don't think run_async would not be correct. Maybe a run_sync_function would better convey what the call does",2021/1/5 21:05,5,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"
run_async() definitely is misleading because you're already in asyncio,

run_async has nothing to do with asyncio. It's like a pattern. There is a run method that runs the code synchronously (for example, in a virtual machine) and a run_async method that runs the code asynchronously (most often it is used with a thread pool).

run_synchronous_blocking_style_code_in_asyncio()

Why is it necessary synchronous_blocking_style_code? You can call with lambda wich do something like loop.run_untill_compleate(some_coro). You just call any code in sometimes future and just get an object which provides methods for manipulating this task.",2021/1/5 21:27,6,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"In our projects we use

This code was borrowed from this repository.
.",2021/1/5 21:36,7,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"

run_async() definitely is misleading because you're already in asyncio,

run_async has nothing to do with asyncio. It's like a pattern. There is a run method that runs the code synchronously (for example, in a virtual machine) and a run_async method that runs the code asynchronously (most often it is used with a thread pool).

that's not what run_sync() does here.


run_synchronous_blocking_style_code_in_asyncio()

Why is it necessary synchronous_blocking_style_code? You can call with lambda wich do something like loop.run_untill_compleate(some_coro).

run_sync() does not accept a coroutine, is accepts a plain Python function which calls blocking style API functions that ultimately call awaitables for the actual IO.     it's important to note the difference in terminology between ""blocking style"" and ""blocking"".   SQLAlchemy's asyncio extension does not run ""blocking"" IO calls.   The run_sync() method runs code that is written in ""blocking style"" but ultimately is using asyncio using greenlets.

You just call any code in sometimes future and just get an object which provides methods for manipulating this task.

if you call upon SQLAlchemy's async-adapted database drivers without using await you get an error message.
Just so we're on the same page, make sure you try out an example script like https://github.com/sqlalchemy/sqlalchemy/blob/master/examples/asyncio/basic.py .
For example, your proposal ""loop.run_until_complete(lambda)"" is not compatible because the event loop is already running:

output:


In our projects we use

it looks like you are running a plain ThreadPoolExecutor to run blocking database code.   That is not at all what run_sync() does.   run_sync() doesn't run blocking IO and it does not need to use executors or anything with thread-pools.",2021/1/5 21:44,8,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"Yes, you are right. I have not seen how it is implemented.
In my example, I mean something like

It would have worked if used ThreadPool instead of greenlet.",2021/1/5 22:35,9,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"OK, anyway, our API is not given a cotourine it's adapting a non-coroutine, but one that ultimately uses internal coroutines for IO, into an awaitable, and then invoking it.    With a few obscure exceptions, there is no comparison to how this works to nearly anything else in the Python world right now.    A name that might be more descriptive without being too long might be run_as_awaitable().      The way the function runs is most similar to how one would see code running under gevent or eventlet, except it calls into a pure asyncio IO library.",2021/1/5 23:36,10,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"@zzzeek maybe we should make clear that run_sync does not run inside a thread executor and that doing any other blocking call that is not sqlalchemy, like sleep/requests, will block the asyncio loop",2021/1/6 0:16,11,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,the method is described in great detail here: https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html?highlight=run_sync#adapting-orm-lazy-loads-to-asyncio    so the docstring should at least link to that,2021/1/6 2:44,12,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"
Also for this method not needed add args and kwargs because for bound we can use lambda.

Since the lambda can be used either way, I don't see what's the harm in having args and kwargs.

the method is described in great detail here: https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html?highlight=run_sync#adapting-orm-lazy-loads-to-asyncio so the docstring should at least link to that

Will update the docs.
Also I think we could transform this to a discussion",2021/1/6 9:29,13,1,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"
Will update the docs.

https://gerrit.sqlalchemy.org/c/sqlalchemy/sqlalchemy/+/2453",2021/1/6 10:35,14,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"just FTR we should probably change the name to something, I was not that happy with run_sync() anyway.    run_as_awaitable() seems closest to me for now and at least wont get confused with the ambiguity of the terms ""sync/ async"".",2021/1/6 12:51,15,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"Maybe make run_sync as the internal method? We can check we are in async code or not using

Let's make decorator wich do something like

and decorate all methods which should work as async.",2021/1/6 13:40,16,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"Workable example:

Also, we can choose by connection class using isinstance(conn, AsyncConnection) for check in the decorator.",2021/1/6 14:15,17,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"Universal code for async and sync mode:
",2021/1/6 14:30,18,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"I'm not sure what you what to achieve here. Could you explain?
Also note that sqlite cannot be used with the asyncio extension.",2021/1/6 14:36,19,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,I want to get rid of the run_sync method. If we use async engine context then all blocking style methods should make available interior without user help because a user can forget use run_sync method. One solution is to use context variables that can work as both thread-local and task-local and chose what return by this connection type. If a connection type is async we return something that returns run_sync method another we do old behaviour.,2021/1/6 14:44,20,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/5914,2021/1/5 20:02,"
I want to get rid of the run_sync method. If we use async engine context then all blocking style methods should make available interior without user help because a user can forget use run_sync method.

OK, this seems to be a different issue than what you originally raised, so now you're saying that when one uses the SQLAlchemy async API , that we have in our examples some utility methods that aren't available as async directly so we provide an adapter, you would rather not have the adapter be present, you would rather have all IO related methods throughout all of SQLAlchemy be available with an ""await"" API.
It's not clear if you're proposing that SQLAlchemy make this available by deafult, e.g. metadata.create_all() would work ""out of the box"" inside of an asyncio script with ""await"", or if you are proposing that the end user be given a special decorator that they would apply to methods such as metadata.create_all, that they would then monkeypatch onto metadata, so that within their code they can say ""await metadata.create_all()"" rather than ""await conn.run_sync(metadata.create_all)"".
The first option, if applied, would not have to use context variables or anything else, we would simply provide an AsyncMetaData object that has awaitable ""create_all()"" and ""drop_all()"" methods.
This option is not feasible because each of SQLAlchemy's async APIs are presented as a new module with classes that wrap around existing ones, and there's simply no need for this to always have to extend to DDL-related methods and other assorted methods throughout the library, that places an undue burden for development, testing, documentation, and maintenance.     Additionally, the event system is not adapted to async at all, so if one were using events like before_execute() or before_cursor_execute() , as well as ORM events like before_insert(), after_insert(), etc., the connection passed within those is a sync connection.  So there is no solution to the issue of SQLAlchemy providing a complete ""asyncio for absolutely everything everywhere"" approach, this would be impractical and is unnecessary for the stated goals of Python's asyncio library.  SQLAlchemy's asyncio API provides the asyncio interface for the part that it's relevant, e.g. within the DML and DQL emitting portions of the Core and ORM components.   people don't need to write SQL migration tools and routines using asyncio.
The second option is not feasible, as an addition to SQLAlchemy itself (you are of course free to use any pattern you'd like in your own application) for reasons which include the following:   1. we would never encourage monkeypatching 2. it's unnecessarily complex, as a much simpler approach already exists  3. it associates connections with implicit context variables which is the opposite of SQLAlchemy's design goals.   connections and sessions are always explicitly passed and global ""contexts"" are typically a last resort programming pattern IMO.

should make available interior without user help because a user can forget use run_sync method.

users would not forget to use run_sync because the code raises an error for a variety of reasons.

create_all() and drop_all() require a connection to be passed, because the metatdata is not bound.   so that raises this error:




try passing the async connection to them, as meta.drop_all(conn).  That raises an error, because AsyncConnection is not accepted by these (this error message should be improved):
AttributeError: 'AsyncConnection' object has no attribute '_run_ddl_visitor'


Try passing the sync connection instead, that is, meta.drop_all(conn.sync_connection).   the greenlet adaption will fail:
sqlalchemy.exc.InvalidRequestError: greenlet_spawn has not been called; can't call await_() here.


Not to mention if the method were to silently fail, their code would be failing anyway.   So there is no chance that forgetting to call run_sync() in the appropriate place would not raise an error and the user would have a clear path to fixing their code.    Python's asyncio API is already fraught with a significant number of ways to forget to do things correctly in any case, it is already a confusing API and I am confident that users will have no problem using run_sync() ( or wahtever we call it) for the extremely small number of cases where it's needed.
To sum up I can easily agree that the name ""run_sync()"" is not clear, so we can change the name, but it is only used for very occasional DDL-related calls and is not needed for nearly anything else.   it provides a clear way to explicitly bridge into any arbitrary non-awaitable code and is useful far beyond the small number of example cases we gave.    it's in fact how the whole asyncio->non asyncio->asyncio bridge works.",2021/1/6 15:10,21,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,there's not much we can do without a clear reproducer.   are you working with custom PG types and/or ENUMs ?,2021/6/16 12:50,1,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"Thank you for your response.
I'm only using simple enums like this:

No custom PG types.
This is the Device model:

Ok, allow me please a couple of hours to produce a small project and I will then share it here.
Kindly leave this open until I'm back. Thanks",2021/6/16 13:28,2,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"
`    Column(""type"", Enum(DeviceType), nullable=False),``

This by default will create an enum on postgres, you need to add native_enum=False to make it construct a var-char column in the db side",2021/6/16 13:45,3,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"yes it's very likely the native ENUM causing a problem here, although of course that should be supported as well.
can you also try setting prepared_statement_cache_size to zero as follows:
",2021/6/16 13:56,4,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"@zzzeek Mike, your suggestion throws this error:

@CaselIT I have dropped the database and recreated the schema to start clean. I added your suggestion to both enums within the model.  But the problem remains.

I will have a small demo ready soon. Thanks",2021/6/16 14:37,5,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"
@zzzeek Mike, your suggestion throws this error:


that's the psycopg2 dialect.   This is asyncio, we are using asyncpg.  can you check the URL please.",2021/6/16 15:32,6,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"Hello Gents,
I just prepared the demo after two hours, as I was about to post it, I see Mike's update. :-D
And it fixed the issue.  Thank you so much Mike.
https://github.com/houmie/demo_db
I have the Database declaration twice. Once as async for the real app:
https://github.com/houmie/demo_db/blob/master/app/database/database.py
And then in test_base.py I have it as sync, since I didn't know how to create and drop the schema in an asynchronous way.

I pasted your suggestion in the wrong config it seems.
Mike can you please explain to me what this setting does?  And do you think I have setup everything else correctly in the config below? In particularly the connection pooling? Or do I have to do everything as part of the URL as you suggested?
""postgresql+asyncpg://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}?prepared_statement_cache_size=0& pool_size=20& max_overflow=0& pool_pre_ping=true""

Many Thanks
Houman",2021/6/16 16:37,7,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"we cache asyncpg prepared statement objects as that saves a ton on performance.   but it runs into this problem sometimes which we had hoped were all fixed, so we'd have to figure out what's happening here.",2021/6/16 16:39,8,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,ah.  does your test suite create and drop tables ?    like between tests ?,2021/6/16 16:41,9,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"Correct, it does, create and drop the tables in a synchroneous way.
I use an sync engine in test_base.py instead of async.

Do you think this is the problem?",2021/6/16 16:47,10,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"it's likely related.  im trying to make a simple reproducer here.
the big red dragon box at https://docs.sqlalchemy.org/en/14/dialects/postgresql.html#prepared-statement-cache gets into this.",2021/6/16 16:53,11,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"here we are, the reproducer, there are two errors we can do here:

one error is the one you have:

then if you take out the insert statements and just do the select(), we get:

we are supposed to raise the latter exception in all cases (edit: but we are going off of asyncpg's errors, so that's what we have, nevermind).",2021/6/16 16:56,12,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"Thanks Mike. I hope you have seen my demo, which should be somewhat helpful as reproducer.
https://github.com/houmie/demo_db
By the way thanks for sending that link with the dragon box. That's exactly the issue. So what do we need to do if the cache is stale? Literally disabling the cache as you suggested? mhhh",2021/6/16 16:57,13,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,well you could dispose() your async engine after a DDL operation with the psycopg2 engine.  that would fix it.,2021/6/16 16:58,14,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,IMO this is at most a documentation issue to maybe make the errors here more prominent in the docs.,2021/6/16 16:59,15,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"Apologies Mike, I'm still trying to get my head around what you mean and what I have to do to overcome this.
I have applied the dispose() to the teardown()

But this doesn't solve it. Have I missed anything? Thanks",2021/6/16 17:06,16,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"i meant, you need to dispose the engine that is associated with the asyncpg dialect, which is an awaitable:
",2021/6/16 17:11,17,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"You are a star! Thank you.
I renamed the sync engine to sync_engine and left the async engine as engine. So that I could refer to them both in test_base.py.

Then I used asynchio.run() to run that dispose function from within the teardown().
Now it works. Nice!!",2021/6/16 17:21,18,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"I think this is mostly a discussion issue, so converting this.",2021/6/16 17:25,19,1,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6648,2021/6/16 10:53,"Hi here. Can you guys help me with my issue with GINO, sqlalchmy, enum in postgres ? I think I faced the same problem, but in different environment (in pytest). I prepared a project to reproduce the bug: https://github.com/zerlok/python-bugs/tree/gino/enum-usage#troubles-with-gino-sqlalchemy-postgres-enum and reported about the bug in GINO: python-gino/gino#814. I'm not sure what lib is a direct cause of the problem.
Thanks in advance.",2022/5/30 14:15,20,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/6909,2021/8/18 22:20,"this is because you are using an unusual setting called ""implicit_returning=False"".  SQLAlhcemy has no way to get the newly generated value of an IDENTITY key if you turn off RETURNING.   remove the use of this flag to resolve the issue.",2021/8/18 23:33,1,,(III) Not a Bug
https://github.com/sqlalchemy/sqlalchemy/discussions/6909,2021/8/18 22:20,"That was the problem.
Do you think this is worth some additional documentation? The existing docs tell me what the effect is on the SQL, but not why I might want that. I could see a couple different paths:

Just a warning saying ""you probably don't want this in new code""
""If you do this, then (slight benefit), but here's an incomplete list of things that might break""
",2021/8/19 14:23,2,,(III) Not a Bug
https://github.com/sqlalchemy/sqlalchemy/discussions/6909,2021/8/18 22:20,"I do think it's a bug, since with ""implicit_returning=False"" the session should manually fetch the generated value like it does in the db that don't support returning.
Thoughts @zzzeek?
On the other hand, why do we even have the implicit_returning flag? When one would want to turn that off?",2021/8/19 14:30,3,,(III) Not a Bug
https://github.com/sqlalchemy/sqlalchemy/discussions/6909,2021/8/18 22:20,"
I do think it's a bug, since with ""implicit_returning=False"" the session should manually fetch the generated value like it does in the db that don't support returning.

SQLAlchemy only has three methods of fetching IDs, there is ""run the default ahead of time"", ""use returning"", and ""use cursor.lastrowid"".    The ""run the default ahead of time"" technique was always a little awkward for PKs and it is now virtually never used, i'd have to check what its role is for non-PK defaults but I would favor taking that whole thing out.

On the other hand, why do we even have the implicit_returning flag? When one would want to turn that off?

there's really no reason to ever turn this flag off, it is a remnant of when DBs were first beginning to support RETURNING and SQLAlchemy's support was very new.    I'd want to do a search to see if there are perhaps some MSSQL cases that people still want it (these would be driver-level issues), but for Oracle, the cx_Oracle driver is now a very solid, super-well supported driver and we are actually behind them on our support for RETURNING so I dont see any reason this flag is needed; we have some docs about this flag in the oracle docs so it's likely that when we were on cx_oracle 4 and 5 and it was hard to get support / docs for the driver, there were probably problems.
in short, there's no need for this flag im aware of so it should just be removed w/ a deprecation phase.",2021/8/19 14:42,4,,(III) Not a Bug
https://github.com/sqlalchemy/sqlalchemy/discussions/6909,2021/8/18 22:20,"Since it's a pk in the example above, shouldn't this case fall back on ""use cursor.lastrowid""? Not sure if maybe something is messed up in the pk detection when we use identity",2021/8/19 15:01,5,,(III) Not a Bug
https://github.com/sqlalchemy/sqlalchemy/discussions/6909,2021/8/18 22:20,"
Since it's a pk in the example above, shouldn't this case fall back on ""use cursor.lastrowid""? Not sure if maybe something is messed up in the pk detection when we use identity

I mean we currently don't have a db that does support identity but does not support returning (oracle, pg and mssql all have returning), so maybe we have a bug there",2021/8/19 16:33,6,,(III) Not a Bug
https://github.com/sqlalchemy/sqlalchemy/discussions/6909,2021/8/18 22:20,"cursor.lastrowid is not a thing for PG and Oracle, and only marginally for MSSQL.    the code relies on RETURNING for these DBs and im fine moving towards that being the only way we operate on these DBs for now.",2021/8/19 17:04,7,,(III) Not a Bug
https://github.com/sqlalchemy/sqlalchemy/discussions/6909,2021/8/18 22:20,Ok. I guess we can move to discussion then,2021/8/19 17:09,8,1,(III) Not a Bug
https://github.com/sqlalchemy/sqlalchemy/discussions/7339,2021/11/17 7:30,"Hi,
Since load_history need to access the db, and there is not yet a async api available the usual solution is to use run_sync.
So you can do

more documentation here: https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html#running-synchronous-methods-and-functions-under-asyncio",2021/11/17 13:08,1,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/7339,2021/11/17 7:30,"@CaselIT
Wow, I didn't know I could use run_sync.
That was the answer.
Thank you.",2021/11/17 13:12,2,,(I) Non Actionable Topic
https://github.com/sqlalchemy/sqlalchemy/discussions/7339,2021/11/17 7:30,"let's migrate to a discussion, so maybe it can be of more help to others",2021/11/17 13:19,3,1,(I) Non Actionable Topic
https://github.com/svinota/pyroute2/discussions/877,2022/2/1 13:52,"I get nlsock = IPRoute(), obtain its file descriptor nlfd = nlsock.fileno(), add it to the poll set, and when poll() notifies about a read event on the descriptor, run nlsock.get() (in a loop) to read all messages.",2022/2/1 15:54,1,,(I) Non Actionable Topic
https://github.com/svinota/pyroute2/discussions/877,2022/2/1 13:52,"Hello Eugene,
but isn't that pretty much functionally equivalent to just doing this?

I'm asking because this doesn't work for me. While running this, ip monitor shows some netlink events occuring. But I don't get any output from the script.",2022/2/1 16:14,2,,(I) Non Actionable Topic
https://github.com/svinota/pyroute2/discussions/877,2022/2/1 13:52,"OK, so maybe I should've read the documentation more carefully.
This works for me:

EDIT:
So just to make sure. If I do things like that, I can just use IPRoute normally? So I'm not somehow interferring with netlink messages being processed by the library?",2022/2/1 16:41,3,,(I) Non Actionable Topic
https://github.com/svinota/pyroute2/discussions/877,2022/2/1 13:52,"@tobiasjakobi-compleo

So I'm not somehow interferring with netlink messages being processed by the library?

Long story short: no, you will not interfere
tl;dr


every netlink request is marked with a unique sequence_number in the header; the responses come with the same number, so you never will confuse responses from different requests, even if they come overlapping in time


the library takes care of that, so you don't need to analyze sequence_number manually


all the broadcast messages come with sequence_number 0, and ipr.get() is actually ipr.get(msg_seq=0)


PS: converted the issue to a discussion",2022/2/2 18:43,4,1,(I) Non Actionable Topic
https://github.com/tox-dev/tox/discussions/2267,2021/11/5 17:28,"PWD is not defined. You'd likely want to use {toxinidir} instead that points to the root of the project, where the tox.ini file lives. Also consider using {/} instead of / to make it cross-platform compatible, though you might care about that part here.",2021/11/5 17:33,1,,(I) Non Actionable Topic
https://github.com/tox-dev/tox/discussions/2267,2021/11/5 17:28,"Thanks for the fast reply and moving to Discussions. With PATH = {toxinidir}/bin:{env:PATH} I have the same ERROR: InvocationError for command could not find executable tool:/. echo {env:PATH} shows in both cases the correct one.
Thanks for the pointer to {/}. Not relevant here, but useful in another project.",2021/11/5 17:40,2,1,(I) Non Actionable Topic